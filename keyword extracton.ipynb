{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYhD/LuR0HiZ/AyUM0JO3Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LCW8iEdck4pV"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMAv2PbjoYkz","executionInfo":{"status":"ok","timestamp":1738205828641,"user_tz":-330,"elapsed":38100,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"2c0ca5a3-5bc3-4676-9ff1-69b4198a42ca"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"IpoFL9_Boagb","executionInfo":{"status":"ok","timestamp":1738205828644,"user_tz":-330,"elapsed":8,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/MINI PROJECTS/papers.csv')"],"metadata":{"id":"Hv6Paf_co-z9","executionInfo":{"status":"ok","timestamp":1738205836396,"user_tz":-330,"elapsed":7759,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"-83xvOF1pBvW","executionInfo":{"status":"ok","timestamp":1738205838548,"user_tz":-330,"elapsed":2157,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"b7886ccf-33bf-4a6f-aec3-1c386e4e212d"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     id  year                                              title event_type  \\\n","0     1  1987  Self-Organization of Associative Database and ...        NaN   \n","1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n","2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n","3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n","4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n","\n","                                            pdf_name          abstract  \\\n","0  1-self-organization-of-associative-database-an...  Abstract Missing   \n","1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n","2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n","3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n","4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n","\n","                                          paper_text  \n","0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n","1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n","2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n","3  Bayesian Query Construction for Neural\\nNetwor...  \n","4  Neural Network Ensembles, Cross\\nValidation, a...  "],"text/html":["\n","  <div id=\"df-3ee45dde-7c95-445f-9501-79b07fad1178\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year</th>\n","      <th>title</th>\n","      <th>event_type</th>\n","      <th>pdf_name</th>\n","      <th>abstract</th>\n","      <th>paper_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1987</td>\n","      <td>Self-Organization of Associative Database and ...</td>\n","      <td>NaN</td>\n","      <td>1-self-organization-of-associative-database-an...</td>\n","      <td>Abstract Missing</td>\n","      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10</td>\n","      <td>1987</td>\n","      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n","      <td>NaN</td>\n","      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n","      <td>Abstract Missing</td>\n","      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100</td>\n","      <td>1988</td>\n","      <td>Storing Covariance by the Associative Long-Ter...</td>\n","      <td>NaN</td>\n","      <td>100-storing-covariance-by-the-associative-long...</td>\n","      <td>Abstract Missing</td>\n","      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000</td>\n","      <td>1994</td>\n","      <td>Bayesian Query Construction for Neural Network...</td>\n","      <td>NaN</td>\n","      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n","      <td>Abstract Missing</td>\n","      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1001</td>\n","      <td>1994</td>\n","      <td>Neural Network Ensembles, Cross Validation, an...</td>\n","      <td>NaN</td>\n","      <td>1001-neural-network-ensembles-cross-validation...</td>\n","      <td>Abstract Missing</td>\n","      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ee45dde-7c95-445f-9501-79b07fad1178')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3ee45dde-7c95-445f-9501-79b07fad1178 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3ee45dde-7c95-445f-9501-79b07fad1178');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2ce6a03f-7007-452e-9e97-9c8568ce8fc5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ce6a03f-7007-452e-9e97-9c8568ce8fc5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2ce6a03f-7007-452e-9e97-9c8568ce8fc5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 7241,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2098,\n        \"min\": 1,\n        \"max\": 7284,\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          1466,\n          3336,\n          6755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2017,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings\",\n          \"Near-Maximum Entropy Models for Binary Neural Representations of Natural Images\",\n          \"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf\",\n          \"3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf\",\n          \"6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3923,\n        \"samples\": [\n          \"Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world data sets.\",\n          \"Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.\",\n          \"Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit. We further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7237,\n        \"samples\": [\n          \"Statistical Performance of Convex Tensor\\nDecomposition\\nRyota Tomioka?\\nTaiji Suzuki?\\nDepartment of Mathematical Informatics,\\nThe University of Tokyo\\nTokyo 113-8656, Japan\\ntomioka@mist.i.u-tokyo.ac.jp\\ns-taiji@stat.t.u-tokyo.ac.jp\\n\\nKohei Hayashi?\\nGraduate School of Information Science,\\nNara Institute of Science and Technology\\nNara 630-0192, Japan\\nkohei-h@is.naist.jp\\n\\n?\\n\\n?\\n\\nHisashi Kashima?,?\\nBasic Research Programs PRESTO,\\nSynthesis of Knowledge for Information Oriented Society, JST\\nTokyo 102-8666, Japan\\nkashima@mist.i.u-tokyo.ac.jp\\n?\\n\\nAbstract\\nWe analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their\\nperformance. We show under some conditions that the mean squared error of\\nthe convex method scales linearly with the quantity we call the normalized rank\\nof the true tensor. The current analysis naturally extends the analysis of convex\\nlow-rank matrix estimation to tensors. Furthermore, we show through numerical\\nexperiments that our theory can precisely predict the scaling behaviour in practice.\\n\\n1 Introduction\\nTensors (multi-way arrays) generalize matrices and naturally represent data having more than two\\nmodalities. For example, multi-variate time-series, for instance, electroencephalography (EEG),\\nrecorded from multiple subjects under various conditions naturally form a tensor. Moreover, in\\ncollaborative ?ltering, users? preferences on products, conventionally represented as a matrix, can\\nbe represented as a tensor when the preferences change over time or context.\\nFor the analysis of tensor data, various models and methods for the low-rank decomposition of\\ntensors have been proposed (see Kolda & Bader [12] for a recent survey). These techniques have\\nrecently become increasingly popular in data-mining [1, 14] and computer vision [25, 26]. Besides\\nthey have proven useful in chemometrics [4], psychometrics [24], and signal processing [20, 7, 8].\\nDespite empirical success, the statistical performance of tensor decomposition algorithms has not\\nbeen fully elucidated. The dif?culty lies in the non-convexity of the conventional tensor decomposition algorithms (e.g., alternating least squares [6]). In addition, studies have revealed many\\ndiscrepancies (see [12]) between matrix rank and tensor rank, which make extension of studies on\\nthe performance of low-rank matrix models (e.g., [9]) challenging.\\nRecently, several authors [21, 10, 13, 23] have focused on the notion of tensor mode-k rank (instead\\nof tensor rank), which is related to the Tucker decomposition [24]. They discovered that regularized\\nestimation based on the Schatten 1-norm, which is a popular technique for recovering low-rank\\nmatrices via convex optimization, can also be applied to tensor decomposition. In particular, the\\n1\\n\\n\\fConvex\\nTucker (exact)\\nOptimization tolerance\\n\\n0\\n\\n10\\n\\n?3\\n\\n10\\n\\n0\\n\\n0.2\\n0.4\\n0.6\\n0.8\\nFraction of observed elements\\n\\n1\\n\\nFigure 1: Result of estimation of rank-(7, 8, 9) tensor of dimensions\\n50???? 50 ? 20 from partial\\n???\\n? ? W ? ??? is plotted against the\\nmeasurements; see [23] for the details. The estimation error ???W\\nF\\nfraction of observed elements m = M/N . Error bars over 10 repetitions are also shown. Convex\\nrefers to the convex tensor decomposition based on the minimization problem (7). Tucker (exact)\\nrefers to the conventional (non-convex) Tucker decomposition [24] at the correct rank. Gray dashed\\nline shows the optimization tolerance 10?3 . The question is how we can predict the point where the\\ngeneralization begins (roughly m = 0.35 in this plot).\\n\\nstudy in [23] showed that there is a clear transition at certain number of samples where the error\\ndrops dramatically from no generalization to perfect generalization (see Figure 1).\\nIn this paper, motivated by the above recent work, we mathematically analyze the performance of\\nconvex tensor decomposition. The new convex formulation for tensor decomposition allows us to\\ngeneralize recent results on Schatten 1-norm-regularized estimation of matrices (see [17, 18, 5, 19]).\\nUnder a general setting we show how the estimation error scales with the mode-k ranks of the true\\ntensor. Furthermore, we analyze the speci?c settings of (i) noisy tensor decomposition and (ii)\\nrandom Gaussian design. In the ?rst setting, we assume that all the elements of a low-rank tensor\\nis observed with noise and the goal is to recover the underlying low-rank structure. This is the most\\ncommon setting a tensor decomposition algorithm is used. In the second setting, we assume that\\nthe unknown tensor is a coef?cient of a tensor-input scalar-output regression problem and the input\\ntensors (design) are randomly given from independent Gaussian distributions. Surprisingly, it turns\\nout that the random Gaussian setting can precisely predict the phase-transition-like behaviour in\\nFigure 1. To the best of our knowledge, this is the ?rst paper that rigorously studies the performance\\nof a tensor decomposition algorithm.\\n\\n2\\n\\nNotation\\n\\nIn this section, we introduce the notations we use in this paper. Moreover, we introduce a H?olderlike inequality (3) and the notion of mode-k decomposability (5), which play central roles in our\\nanalysis.\\nQK\\nLet X ? Rn1 ????nK be a K-way tensor. We denote the number of elements in X by N = k=1 nk .\\n?\\nThe inner product between two tensors ?W, X ? is de?ned as ?W, X ? = vec(W)\\np ), where\\n??? ??? vec(X\\nvec is a vectorization. In addition, we de?ne the Frobenius norm of a tensor ???X ???F = ?X , X ?.\\nQ\\nThe mode-k unfolding X (k) is the nk ? n\\n? \\\\k (?\\nn\\\\k := k? ?=k nk? ) matrix obtained by concatenating\\nthe mode-k ?bers (the vectors obtained by ?xing every index of X but the kth index) of X as column\\nvectors. The mode-k rank of a tensor X , denoted by rankk (X ), is the rank of the mode-k unfolding\\nX (k) (as a matrix). Note that when K = 2 and X is actually a matrix, and X (2) = X (1) ? . We say\\na tensor X is rank (r1 , . . . , rK ) when rk = rankk (X ) for k = 1, . . . , K. Note that the mode-k rank\\ncan be computed in a polynomial time, because it boils down to computing a matrix rank, whereas\\ncomputing tensor rank is NP complete [11]. See [12] for more details.\\nSince for each k, the convex envelope of the mode-k rank is given as the Schatten 1-norm [18]\\n(known as the trace norm [22] or the nuclear norm [3]), it is natural to consider the following\\n2\\n\\n\\f??? ???\\noverlapped Schatten 1-norm ???W ???S of a tensor W ? Rn1 ?????nK (see also [21]):\\n1\\n\\n??? ???\\n???W ???\\n\\nS1\\n\\n=\\n\\nK\\n?\\n1 X?\\n?W (k) ? ,\\nS1\\nK\\n\\n(1)\\n\\nk=1\\n\\nwhere W (k) is the mode-k unfolding of W. Here ? ? ?S1 is the Schatten 1-norm for a matrix\\nXr\\n?W ?S1 =\\n?j (W ),\\nj=1\\n\\nwhere ?j (W ) is the jth largest singular-value of W . The dual norm of the Schatten 1-norm is the\\nSchatten ?-norm (known as the spectral norm) as follows:\\n?X?S? = max ?j (X).\\nj=1,...,r\\n\\nSince the two norms ? ? ?S1 and ? ? ?S? are dual to each other, we have the following inequality:\\n|?W , X?| ? ?W ?S1 ?X?S? ,\\n(2)\\nwhere ?W , X? is the inner product of W and X.\\nThe same inequality holds for the overlapped Schatten 1-norm (1) and its dual norm. The dual norm\\nof the overlapped Schatten 1-norm can be characterized by the following lemma.\\n??? ???\\nLemma 1. The dual norm of the overlapped Schatten 1-norm denoted as ???????S ? is de?ned as the\\n1\\nin?mum of the maximum mode-k spectral norm over the tensors whose average equals the given\\ntensor X as follows:\\n??? ???\\n(k)\\n???X ??? ? =\\nmax ?Y (k) ?S? ,\\ninf\\nS1\\n1\\n(1) +Y (2) +???+Y (K) =X\\nk=1,...,K\\nY\\n(\\n)\\nK\\n(k)\\n\\nwhere Y (k) is the mode-k unfolding of Y (k) . Moreover, the following upper bound on the dual norm\\n??? ???\\n??????? ? is valid:\\nS1\\n\\n??? ???\\n???X ???\\n\\nS1?\\n\\n??? ???\\n1 XK\\n?X (k) ?S? .\\n? ???X ???mean :=\\nk=1\\nK\\n\\n??? ???\\nProof. The ?rst part can be shown by solving the dual of the maximization problem ???X ???S ? :=\\n1\\n??? ???\\nsup ?W, X ? s.t. ???W ???S1 ? 1. The second part is obtained by setting Y (k) = PK K1/c ? X /ck ,\\nwhere ck = ?X (k) ?S? , and using Jensen?s inequality.\\n\\nk? =1\\n\\nk\\n\\nAccording to Lemma 1, we have the ??following\\n? ??? ??? H?\\n?o??lder-like\\n??? inequality\\n??? ??? ???\\n|?W, X ?| ? ???W ???S1 ???X ???S ? ? ???W ???S1 ???X ???mean .\\n\\n(3)\\n??? ??? ??? ???\\nNote that the above bound is tighter than the more intuitive relation | ?W, X ? | ? ???W ???S ???X ???S\\n1\\n?\\n??? ???\\n(???X ???S? := max1,...,K ?X (k) ?S? ), which one might come up as an analogy to the matrix case (2).\\n1\\n\\nFinally, let W ? ? Rn1 ?????nK be the low-rank tensor that we wish to recover. We assume that W ?\\nis rank (r1 , . . . , rK ). Thus, for each k we have\\nW ?(k) = U k S k V k\\n(k = 1, . . . , K),\\nwhere U k ? Rnk ?rk and V k ? Rn? \\\\k ?rk are orthogonal, and S k ? Rrk ?rk is diagonal. Let\\n? ? Rn1 ?????nK be an arbitrary tensor. We de?ne the mode-k orthogonal complement ???k of an\\nunfolding ?(k) ? Rnk ??n\\\\k of ? with respect to the true low-rank tensor W ? as follows:\\n??k\\n\\n???k = (I nk ? U k U k ? )?(k) (I n? \\\\k ? V k V k ? ).\\n\\n(4)\\n\\n:= ?(k) ? ???k is\\nthe true tensor W ?(k) .\\n\\nIn addition\\nthe component having overlapped row/column space with the\\nunfolding of\\nNote that the decomposition ?(k) = ??k + ???k is de?ned for\\neach mode; thus we use subscript k instead of (k).\\nUsing the decomposition de?ned above we have the following equality, which we call mode-k decomposability of the Schatten 1-norm:\\n?W ?(k) + ???k ?S1 = ?W ?(k) ?S1 + ????k ?S1 (k = 1, . . . , K).\\n(5)\\nThe above decomposition is de?ned for each mode and thus it is weaker than the notion of decomposability discussed by Negahban et al. [15].\\n3\\n\\n\\f3\\n\\nTheory\\n\\nIn this section, we ?rst present a deterministic result that holds under a certain choice of regularization constant ?M and an assumption called the restricted strong convexity. Then, we focus on\\nspecial cases to justify the choice of regularization constant and the restricted strong convexity assumption. We analyze the setting of (i) noisy tensor decomposition and (ii) random Gaussian design\\nin Section 3.2 and Section 3.3, respectively.\\n3.1\\n\\nMain result\\n\\nOur goal is to estimate an unknown rank (r1 , . . . , rK ) tensor W ? ? Rn1 ????nK from observations\\nyi = ?Xi , W ? ? + ?i (i = 1, . . . , M ).\\n(6)\\nHere the noise ?i follows the independent zero-mean Gaussian distribution with variance ? 2 .\\nWe employ the regularized empirical risk minimization problem proposed in [21, 10, 13, 23] for the\\nestimation of W as follows:\\n??? ???\\n1\\nminimize\\n?y ? X(W)?22 + ?M ???W ???S1 ,\\n(7)\\nn\\n?????n\\n1\\nK\\n2M\\nW?R\\nwhere y = (y1 , . . . , yM )? is the collection of observations; X : Rn1 ?????nK ? RM is a linear\\noperator that maps W to the M dimensional output vector X(W) = (?X1 , W? , . . . , ?XM , W?) ? ?\\nRM . The Schatten 1-norm term penalizes every mode of W to be jointly low-rank (see Equation (1));\\n?M > 0 is the regularization constant. Accordingly, the solution of the minimization problem (7) is\\ntypically a low-rank tensor when ?M is suf?ciently large. In addition, we denote the adjoint operator\\nPM\\nof X as X? : RM ? Rn1 ?????nK ; that is X? (?) = i=1 ?i Xi ? Rn1 ?????nK .\\n? ? W?\\nThe ?rst step in our analysis is to characterize the particularity of the residual tensor ? := W\\nas in the following lemma.\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nLemma 2. Let W\\n/M ,\\nmean\\n? ? W ? , where W ? is the true low-rank tensor. Let ?(k) = ?? + ??? be the\\nand let ? := W\\nk\\nk\\ndecomposition de?ned in Equation (4). Then we have the following inequalities:\\n1. rank(??k ) ? 2rk for each k = 1, . . . , K.\\nPK\\nPK\\n?\\n??\\n2.\\nk=1 ??k ?S1 .\\nk=1 ??k ?S1 ? 3\\nProof. The proof uses the mode-k decomposability (5) and is analogous to that of Lemma 1 in\\n[17].\\nThe second ingredient of our analysis is the restricted strong convexity. Although, ?strong? may\\nsound like a strong assumption, the point is that we require this assumption to hold only for the\\nparticular residual tensor we characterized in Lemma 2. The assumption can be stated as follows.\\nAssumption 1 (Restricted strong convexity). We suppose that there is a positive constant ?(X) such\\nthat the operator X satis?es the inequality\\n??? ???2\\n1\\n?X(?)?22 ??(X)???????F ,\\n(8)\\nM\\nPK\\nfor all ? ? Rn1 ?????nK such that for each k = 1, . . . , K, rank(??k ) ? 2rk and k=1 ????k ?S1 ?\\nPK\\n3 k=1 ???k ?S1 , where ??k and ???k are de?ned through the decomposition (4).\\nNow using the above two ingredients, we are ready to prove the following deterministic guarantee\\non the performance of the estimation procedure (7).\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nTheorem 1. Let W\\n/M .\\nmean\\nSuppose that the operator X satis?es the restricted strong convexity condition. Then the following\\nbound is true:\\nPK ?\\n???\\n???\\n? ? W ? ??? ? 32?M k=1 rk .\\n???W\\n(9)\\nF\\n?(X)K\\n4\\n\\n\\f? ? W ? . Combining the fact that the objective value for W\\n?\\nProof. Let ? = W\\n??? ? ???\\n??? is??smaller\\n?\\n??than\\n? ??? that for\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\nW , the H?older-like inequality (3), the triangular inequality W S ? W S ? ?????S , and\\n1\\n1\\n1\\n???\\n???\\nthe assumption ???X? (?)/M ???\\n? ?M /2, we obtain\\nmean\\n\\n??? ???\\n???\\n???\\n??? ???\\n??? ???\\n1\\n(10)\\n?X(?)?22 ? ???X? (?)/M ???mean ???????S1 + ?M ???????S1 ? 2?M ???????S1 .\\n2M\\nNow the left-hand side can be lower-bounded using the restricted strong convexity (8). On the other\\nhand, using Lemma 2, the right-hand side can be upper-bounded as follows:\\n??? ???\\n??? ???\\n??? ???\\n?\\n??????? ? 1 PK (???k ?S1 + ????k ?S1 ) ? 4 PK ???k ?S1 ? 4 ? F PK\\n2rk , (11)\\nk=1\\nk=1\\nk=1\\nK\\nK\\nK\\nS1\\n??? ???\\nwhere the last inequality follows because ???????F = ??(k) ?F for k = 1, . . . , K. Combining inequalities (8), (10), and (11), we obtain our claim (9).\\nNegahban et al. [15] (see also [17]) pointed out that the key properties for establishing a sharp convergence result for a regularized M -estimator is the decomposability of the regularizer and the restricted strong convexity. What we have shown suggests that the weaker mode-k decomposability (5)\\nsuf?ce to obtain the above convergence result for the overlapped Schatten 1-norm (1) regularization.\\n3.2 Noisy Tensor Decomposition\\nIn this subsection, we consider the setting where all the elements are observed (with noise) and the\\ngoal is to recover the underlying low-rank tensor without noise.\\nSince all the elements are observed only once, X is simply a vectorization\\n(M =\\n???\\n??? N ), and the left2\\n? ? W ? ??? . Therefore, the\\n?\\n?\\n?\\n=\\nW\\nhand side of inequality (10)???gives the\\nquantity\\nof\\ninterest\\n?X(?)?\\n2\\nF\\n???\\nremaining task is to bound ???X? (?)???mean as in the following lemma.\\nLemma 3. Suppose\\n???\\n?that\\n?? X : n1 ?? ? ??nK ? N is a vectorization of a tensor. With high probability\\nthe quantity ???X? (?)???mean is concentrated around its mean, which can be bounded as follows:\\nK\\n???\\n???\\n?\\np\\n? X ??\\nE???X? (?)???mean ?\\nnk + n\\n? \\\\k .\\nK\\n\\n(12)\\n\\nk=1\\n\\n???\\n???\\nSetting the regularization constant as ?M = c0 E???X? (?)???mean /N , we obtain the following theorem.\\nTheorem 2. Suppose that X : n1 ?? ? ??nK ? N is a vectorization of a tensor. There are universal\\nconstants c0 and c1 , such that, with high probability, any solution of the minimization problem (7)\\nPK ?\\np\\nwith regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(KN ) satis?es the following bound:\\n?\\n!2 ?\\n!2\\nK\\nK\\nX\\nX\\n???\\n???2\\n??\\n?\\np\\n?\\n1\\n1\\n?\\n2\\n? ? W ??? ? c1 ?\\n???W\\nnk + n\\n? \\\\k\\nrk .\\nF\\nK\\nK\\nk=1\\n\\nk=1\\n\\nProof. Combining Equations (10)?(11) with the fact that X is simply a vectorization and M = N ,\\nwe have\\n?\\n1\\n? ? W ? ?F ? 16 2?M PK ?rk .\\n?W\\nN\\n\\nK\\n\\nk=1\\n\\nSubstituting the choice of regularization constant ?M and squaring both sides, we obtain our claim.?\\nWe can simplify the result of Theorem 2 by noting that n\\n? \\\\k = N/nk ? nk , when the dimenPK ? 2\\n1\\nsions are of the same order. Introducing the notation ?r?1/2 = ( K\\nrk ) and n?1 :=\\nk=1\\n(1/n1 , . . . , 1/nK ), we have\\n???\\n???\\n? ? W ? ???2\\n???W\\n?\\n?\\nF\\n? Op ? 2 ?n?1 ?1/2 ?r?1/2 .\\n(13)\\nN\\nWe call the quantity r? = ?n?1 ?1/2 ?r?1/2 the normalized rank, because r? = r/n when the dimensions are balanced (nk = n and rk = r for all k = 1, . . . , K).\\n5\\n\\n\\f3.3\\n\\nRandom Gaussian Design\\n\\nIn this subsection, we consider the case the elements of the input tensors Xi (i = 1, . . . , M ) in the\\nobservation model (6) are distributed according to independent identical standard Gaussian distributions. We call this setting random Gaussian design.\\n???\\n???\\nFirst we show an upper bound on the norm ???X? (?)???mean , which we use to specify the scaling of\\nthe regularization constant ?M in Theorem 1.\\nLemma 4. Let X : Rn1 ?????nK ? RM be a random Gaussian design. In addition, we assume\\nthat\\n?i is sampled independently from N (0, ? 2 ). Then with high probability the quantity\\n?\\n??? ? the ??noise\\n???X (?)???\\nis concentrated around its mean, which can be bounded as follows:\\nmean\\n???\\n???\\nE???X? (?)???\\n\\nmean\\n\\n?\\nK\\n?\\np\\n? M X ??\\n?\\nnk + n\\n? \\\\k .\\nK\\nk=1\\n\\nNext the following lemma, which is a generalization of a result presented in Negahban and Wainwright [17, Proposition 1], provides a ground for the restricted strong convexity assumption (8).\\nLemma 5. Let X : Rn1 ?????nK ? RM be a random Gaussian design. Then it satis?es\\n?r\\n!\\nr\\nK\\nn\\n? \\\\k ?????? ??????\\n1 X\\n?X(?)?2\\n1 ?????? ??????\\nnk\\n?\\n? F?\\n+\\n? S1 ,\\n?\\n4\\nK\\nM\\nM\\nM\\nk=1\\n\\nwith probability at least 1 ? 2 exp(?N/32).\\nProof. The proof is analogous to that of Proposition 1 in [17] except that we use H?older-like inequality (3) for tensors instead of inequality (2) for matrices.\\nFinally, we obtain the following convergence bound.\\nTheorem 3. Under the random Gaussian design setup, there are universal constants c0 , c1 , and c2\\nPK ?\\nPK ? 2\\np\\n1\\n1\\nsuch that for a sample size M ? c1 ( K\\nn\\n? \\\\k ))2 ( K\\nrk ) , any solution of the\\nk=1 ( nk +\\n?\\nPk=1\\np\\n?\\nK\\nminimization problem (7) with regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(K M )\\nsatis?es the following bound:\\nPK ?\\nPK ? 2\\np\\n1\\n1\\n???\\n???\\n?2 ( K\\nn\\n? \\\\k ))2 ( K\\nk=1 ( nk +\\nk=1 rk )\\n? ? W ? ???2 ? c2\\n???W\\n,\\nF\\nM\\nwith high probability.\\nAgain we can simplify the result of Theorem 3 as follows: for sample size M ? c1 N r? we have\\n?\\n?\\n?1\\n???\\n???\\n? ? W ? ???2 ? Op ? 2 N ?n ?1/2 ?r?1/2 ,\\n???W\\n(14)\\nF\\nM\\nwhere r? = ?n?1 ?1/2 ?r?1/2 is the normalized rank. Note that the condition on the number of\\nsamples M does not depend on the noise variance ? 2 . Therefore in the limit ? 2 ? 0, the bound (14)\\nis suf?ciently small but only valid for sample size M that exceeds c1 N r?, which implies a threshold\\nbehavior as in Figure 1.\\nNote also that in the matrix case (K = 2), r1 = r2 = r and N ?n?1 ?1/2 = O(n1 + n2 ). Therefore\\n? ? W ? ?2 ?\\nwe can restate the above result as for sample size M ? c1 r(n1 + n2 ), we have ?W\\nF\\nOp (r(n1 + n2 )/M ), which is compatible with the result in [17, 18].\\n\\n4\\n\\nExperiments\\n\\nIn this section, we conduct two numerical experiments to con?rm our analysis in Section 3.2 and\\nSection 3.3.\\n6\\n\\n\\f?4\\n\\n3\\n\\nx 10\\n\\n0.03\\n\\nsize=[50 50 20] ?M=0.03/N\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[50 50 20] ?M=2.34/N\\n\\nsize=[50 50 20] ? =0.54/N\\n\\n0.025\\n\\nM\\n\\nsize=[100 100 50] ?M=0.66/N\\n\\nsize=[100 100 50] ?M=0.69/N\\n\\nMean squared error\\n\\nMean squared error\\n\\nsize=[50 50 20] ? =6/N\\nM\\n\\nsize=[100 100 50] ?M=0.06/N\\n2\\n\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[100 100 50] ? =1.11/N\\nM\\n\\n1\\n\\n0.02\\n\\nsize=[100 100 50] ? =4.5/N\\nM\\n\\nsize=[100 100 50] ?M=12/N\\n0.015\\n\\n0.01\\n\\n0.005\\n\\n0\\n0\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n0\\n0\\n\\n1\\n\\n(a) Small noise (? = 0.01).\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n1\\n\\n(b) Large noise (? = 0.1).\\n\\nFigure 2: Result of noisy tensor decomposition for tensors of size 50 ? 50 ? 20 and 100 ? 100 ? 50.\\n\\n4.1\\n\\nNoisy Tensor Decomposition\\n\\nWe randomly generated low-rank tensors of dimensions n(1) = (50, 50, 20) and n(2) =\\n(100, 100, 50) for various ranks (r1 , . . . , rK ). For a speci?c rank, we generated the true tensor\\nby drawing elements of the r1 ? ? ? ? ? rK ?core tensor? from the standard normal distribution and\\nmultiplying its each mode by an orthonormal factor randomly drawn from the Haar measure. As\\ndescribed in Section 3.2, the observation y consists of all the elements of the original tensor once\\n(M = N ) with additive independent Gaussian noise with variance ? 2 . We used the alternating\\ndirection method of multipliers (ADMM) for ?constraint? approaches described in [23, 10] to solve\\nthe minimization problem (7). The whole experiment was repeated 10 times and averaged.\\n???\\n???\\n? ? W ? ???2 /N is plotted against\\nThe results are shown in Figure 2. The mean squared error ???W\\nF\\nthe normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true tensor) de?ned in Equation (13). Since the\\nchoice of the regularization constant ?M only depends on the size of the tensor and not on the ranks\\nof the underlying tensor in Theorem 2, we ?x the regularization constant to some different values\\nand report the dependency of the estimation error on the normalized rank r? of the true tensor.\\nFigure 2(a) shows the result for small noise (? = 0.01) and Figure 2(b) shows the result for large\\n???\\n???\\n? ? W ? ???2 grows linearly\\nnoise (? = 0.1). As predicted by Theorem 2, the squared error ???W\\nF\\nagainst the normalized rank r?. This behaviour is consistently observed not only around the preferred\\nregularization constant value (triangles) but also in the over-?tting case (circles) and the under?tting case (crosses). Moreover, as predicted by Theorem 2, the preferred regularization constant\\nvalue scales linearly and the squared error scales quadratically to the noise standard deviation ?.\\nAs predicted by Lemma 3, the curves for the smaller 50 ? 50 ? 20 tensor and those for the larger\\n100 ? 100 ? 50 tensor seem to agree when the regularization constant\\nis scaled by the factor two.\\np\\nNote that the dominant term in inequality (12) is the second term n\\n? \\\\k , which is roughly scaled by\\nthe factor two from 50 ? 50 ? 20 to 100 ? 100 ? 50.\\n4.2\\n\\nTensor completion from partial observations\\n\\nIn this subsection, we repeat the simulation originally done by Tomioka et al. [23] and demonstrate\\nthat our results in Section 3.3 can precisely predict the empirical scaling behaviour with respect to\\nboth the size and rank of a tensor.\\nWe present results for both matrix completion (K = 2) and tensor completion (K = 3). For\\nthe matrix case, we randomly generated low-rank matrices of dimensions 50 ? 20, 100 ? 40, and\\n250 ? 200. For the tensor case, we randomly generated low-rank tensors of dimensions 50 ? 50 ? 20\\nand 100 ? 100 ? 50. We generated the matrices or tensors as in the previous subsection for various\\nranks. We randomly selected some elements of the true matrix/tensor for training and kept the\\n7\\n\\n\\f1\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n0.4\\nsize=[50 20]\\nsize=[100 40]\\nsize=[250 200]\\n\\n0.2\\n0\\n0\\n\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nNormalized rank ||n?1|| ||r||\\n1/2\\n\\nFraction at Error<=0.01\\n\\nFraction at err<=0.01\\n\\n1\\n\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.6\\n\\n1/2\\n\\n(a) Matrix completion (K = 2).\\n\\nsize=[50 50 20]\\nsize=[100 100 50]\\n0.2\\n0.4\\n0.6\\nNormalized rank ||n?1||1/2||r||1/2\\n\\n0.8\\n\\n(b) Tensor completion (K = 3).\\n\\nFigure 3: Scaling behaviour of matrix/tensor completion with respect to the size n and the rank r.\\n\\nremaining elements for testing. No observation noise is added. We used the ADMM for ?as a\\nmatrix? and ?constraint? approaches described in [23] to solve the minimization problem (7) for\\nmatrix completion and tensor completion, respectively. Since there is no observation noise, we\\nchose the regularization constant ? ? 0. A single experiment for a speci?c size and rank can be\\nvisualized as in Figure 1.\\n?In\\n?? Figure ?3,\\n??? we plot the minimum fraction of observations m = M/N that achieved error\\n? ? W ??? smaller than 0.01 against the normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true ten???W\\nF\\nsor) de?ned in Equation (13). The matrix case is plotted in Figure 3(a) and the tensor case is plotted\\nin Figure 3(b). Each series (blue crosses or red circles) corresponds to different matrix/tensor size\\nand each data-point corresponds to a different core size (rank). We can see that the fraction of observations m = M/N scales linearly against the normalized rank r?, which agrees with the condition\\nM/N ? c1 ?n?1 ?1/2 ?r?1/2 = c1 r? in Theorem 3 (see Equation (14)). The agreement is especially\\ngood for tensor completion (Figure 3(b)), where the two series almost overlap. Interestingly, we\\ncan see that when compared at the same normalized rank, tensor completion is easier than matrix\\ncompletion. For example, when nk = 50 and rk = 10 for each k = 1, . . . , K, the normalized rank\\nis 0.2. From Figure 3, we can see that we only need to see 30% of the entries in the tensor case to\\nachieve error smaller than 0.01, whereas we need about 60% of the entries in the matrix case.\\n\\n5\\n\\nConclusion\\n\\nWe have analyzed the statistical performance of a tensor decomposition algorithm based on the\\noverlapped Schatten 1-norm regularization (7). Numerical experiments show that our theory can\\npredict the empirical scaling behaviour well. The fraction of observation m = M/N at the threshold\\npredicted by our theory is proportional to the quantity we call the normalized rank, which re?nes\\nconjecture (sum of the mode-k ranks) in [23].\\nThere are numerous directions that the current study can be extended. In this paper, we have focused\\non the convergence of the estimation error; it would be meaningful to also analyze the condition for\\nthe consistency of the estimated rank as in [2]. Second, although we have succeeded in predicting\\nthe empirical scaling behaviour, the setting of random Gaussian design does not match the tensor\\ncompletion setting in Section 4.2. In order to analyze the latter setting, the notion of incoherence in\\n[5] or spikiness in [16] might be useful. This might also explain why tensor completion is easier than\\nmatrix completion at the same normalized rank. Moreover, when the target tensor is only low-rank\\nin a certain mode, Schatten 1-norm regularization fails badly (as predicted by the high normalized\\nrank). It would be desirable to analyze the ?Mixture? approach that aims at this case [23]. In\\na broader context, we believe that the current paper could serve as a basis for re-examining the\\nconcept of tensor rank and low-rank approximation of tensors based on convex optimization.\\nAcknowledgments. We would like to thank Franz Kir?aly and Hiroshi Kajino for their valuable\\ncomments and discussions. This work was supported in part by MEXT KAKENHI 22700138,\\n23240019, 23120004, 22700289, and NTT Communication Science Laboratories.\\n8\\n\\n\\fReferences\\n[1] E. Acar and B. Yener. Unsupervised multiway data analysis: A literature survey. IEEE T. Knowl. Data.\\nEn., 21(1):6?20, 2009.\\n[2] F.R. Bach. Consistency of trace norm minimization. J. Mach. Learn. Res., 9:1019?1048, 2008.\\n[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\\n[4] R. Bro. PARAFAC. Tutorial and applications. Chemometr. Intell. Lab., 38(2):149?171, 1997.\\n[5] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,\\n9(6):717?772, 2009.\\n[6] J.D. Carroll and J.J. Chang. Analysis of individual differences in multidimensional scaling via an n-way\\ngeneralization of ?Eckart-Young? decomposition. Psychometrika, 35(3):283?319, 1970.\\n[7] P. Comon. Tensor decompositions. In J. G. McWhirter and I. K. Proudler, editors, Mathematics in signal\\nprocessing V. Oxford University Press, 2002.\\n[8] L. De Lathauwer and J. Vandewalle. Dimensionality reduction in higher-order signal processing and\\nrank-(r1 , r2 , . . . , rn ) reduction in multilinear algebra. Linear Algebra Appl., 391:31?55, 2004.\\n[9] K. Fukumizu. Generalization error of linear neural networks in unidenti?able cases. In Algorithmic\\nLearning Theory, pages 51?62. Springer, 1999.\\n[10] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27:025010, 2011.\\n[11] J. H?astad. Tensor rank is NP-complete. Journal of Algorithms, 11(4):644?654, 1990.\\n[12] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455?500,\\n2009.\\n[13] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data.\\nIn Prof. ICCV, 2009.\\n[14] M. M?rup. Applications of tensor (multiway array) factorizations and decompositions in data mining.\\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):24?40, 2011.\\n[15] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uni?ed framework for high-dimensional\\nanalysis of m-estimators with decomposable regularizers. In Y. Bengio, D. Schuurmans, J. Lafferty,\\nC. K. I. Williams, and A. Culotta, editors, Advances in NIPS 22, pages 1348?1356. 2009.\\n[16] S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal\\nbounds with noise. Technical report, arXiv:1009.2118, 2010.\\n[17] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. Ann. Statist., 39(2), 2011.\\n[18] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via\\nnuclear norm minimization. SIAM Review, 52(3):471?501, 2010.\\n[19] A. Rohde and A.B. Tsybakov.\\n39(2):887?930, 2011.\\n\\nEstimation of high-dimensional low-rank matrices.\\n\\nAnn. Statist.,\\n\\n[20] N.D. Sidiropoulos, R. Bro, and G.B. Giannakis. Parallel factor analysis in sensor array processing. IEEE\\nT. Signal Proces., 48(8):2377?2388, 2000.\\n[21] M. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for convex\\nmultilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.\\n[22] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Lawrence K.\\nSaul, Yair Weiss, and L?eon Bottou, editors, Advances in NIPS 17, pages 1329?1336. MIT Press, Cambridge, MA, 2005.\\n[23] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization.\\nTechnical report, arXiv:1010.0789, 2011.\\n[24] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279?311,\\n1966.\\n[25] M. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. Computer\\nVision?ECCV 2002, pages 447?460, 2002.\\n[26] H. Wang and N. Ahuja. Facial expression decomposition. In Proc. 9th ICCV, pages 958 ? 965, 2003.\\n\\n9\\n\\n\\f\",\n          \"Effects of Spatial and Temporal Contiguity on\\nthe Acquisition of Spatial Information\\n\\nThea B. Ghiselli-Crippa and Paul W. Munro\\nDepartment of Information Science and Telecommunications\\nUniversity of Pittsburgh\\nPittsburgh, PA 15260\\ntbgst@sis.pitt.edu, munro@sis.pitt.edu\\n\\nAbstract\\nSpatial information comes in two forms: direct spatial information (for\\nexample, retinal position) and indirect temporal contiguity information,\\nsince objects encountered sequentially are in general spatially close. The\\nacquisition of spatial information by a neural network is investigated\\nhere. Given a spatial layout of several objects, networks are trained on a\\nprediction task. Networks using temporal sequences with no direct spatial information are found to develop internal representations that show\\ndistances correlated with distances in the external layout. The influence\\nof spatial information is analyzed by providing direct spatial information\\nto the system during training that is either consistent with the layout or\\ninconsistent with it. This approach allows examination of the relative\\ncontributions of spatial and temporal contiguity.\\n\\n1 Introduction\\nSpatial information is acquired by a process of exploration that is fundamentally temporal, whether it be on a small scale, such as scanning a picture, or on a larger one, such as\\nphysically navigating through a building, a neighborhood, or a city. Continuous scanning\\nof an environment causes locations that are spatially close to have a tendency to occur in\\ntemporal proximity to one another. Thus, a temporal associative mechanism (such as a\\nHebb rule) can be used in conjunction with continuous exploration to capture the spatial\\nstructure of the environment [1]. However, the actual process of building a cognitive map\\nneed not rely solely on temporal associations, since some spatial information is encoded in\\nthe sensory array (position on the retina and proprioceptive feedback). Laboratory studies\\nshow different types of interaction between the relative contributions of temporal and spatial contiguities to the formation of an internal representation of space. While Clayton and\\nHabibi's [2] series of recognition priming experiments indicates that priming is controlled\\nonly by temporal associations, in the work of McNamara et al. [3] priming in recognition is observed only when space and time are both contiguous. In addition, Curiel and\\nRadvansky's [4] work shows that the effects of spatial and temporal contiguity depend on\\nwhether location or identity information is emphasized during learning. Moreover, other\\nexperiments ([3]) also show how the effects clearly depend on the task and can be quite\\ndifferent if an explicitly spatial task is used (e.g., additive effects in location judgments).\\n\\n\\fT. B. Ghiselli-Crippa and P W. Munro\\n\\n18\\n\\nlabels\\n\\nlabels\\n\\nlabels\\n(A coeff.)\\n\\nlabels\\n\\nlabels\\n\\ncoordinates\\n\\ncoordinates\\n(B coeff.)\\n\\nlabels\\n\\nFigure 1: Network architectures: temporal-only network (left); spatio-temporal network\\nwith spatial units part of the input representation (center); spatio-temporal network with\\nspatial units part of the output representation (right) .\\n\\n2 Network architectures\\nThe goal of the work presented in this paper is to study the structure of the internal representations that emerge from the integration of temporal and spatial associations. An\\nencoder-like network architecture is used (see Figure 1), with a set of N input units and a\\nset of N output units representing N nodes on a 2-dimensional graph. A set of H units is\\nused for the hidden layer. To include space in the learning process, additional spatial units\\nare included in the network architecture. These units provide a representation of the spatial\\ninformation directly available during the learning/scanning process. In the simulations described in this paper, two units are used and are chosen to represent the (x, y) coordinates of\\nthe nodes in the graph . The spatial units can be included as part of the input representation\\nor as part of the output representation (see Figure 1, center and right panels): both choices\\nare used in the experiments, to investigate whether the spatial information could better benefit training as an input or as an output [5]. In the second case, the relative contribution of\\nthe spatial information can be directly manipulated by introducing weighting factors in the\\ncost function being minimized. A two-term cost function is used, with a cross-entropy term\\nfor the N label units and a squared error term for the 2 coordinate units,\\n\\nri indicates the actual output of unit i and ti its desired output. The relative influence of\\n\\nthe spatial information is controlled by the coefficients A and B.\\n\\n3\\n\\nLearning tasks\\n\\nThe left panel of Figure 2 shows an example of the type of layout used; the effective\\nlayout used in the study consists of N = 28 nodes. For each node, a set of neighboring\\nnodes is defined, chosen on the basis of how an observer might scan the layout to learn the\\nnode labels and their (spatial) relationships; in Figure 2, the neighborhood relationships are\\nrepresented by lines connecting neighboring nodes. From any node in the layout, the only\\nallowed transitions are those to a neighbor, thus defining the set of node pairs used to train\\nthe network (66 pairs out of C(28, 2) = 378 possible pairs). In addition, the probability\\nof occurrence of a particular transition is computed as a function of the distance to the\\ncorresponding neighbor. It is then possible to generate a sequence of visits to the network\\nnodes, aimed at replicating the scanning process of a human observer studying the layout.\\n\\n\\f19\\n\\nSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\neraser\\n\\nknife\\n\\ncup\\n\\ncoin\\n\\neraser\\n\\nbutton\\n\\nFigure 2: Example of a layout (left) and its permuted version (right). Links represent\\nallowed transitions. A larger layout of 28 units was used in the simulations.\\n\\nThe basic learning task is similar to the grammar learning task of Servan-Schreiber et al.\\n[6] and to the neighborhood mapping task described in [1] and is used to associate each of\\nthe N nodes on the graph and its (x, y) coordinates with the probability distribution of the\\ntransitions to its neighboring nodes. The mapping can be learned directly, by associating\\neach node with the probability distribution of the transitions to all its neighbors: in this\\ncase, batch learning is used as the method of choice for learning the mapping. On the\\nother hand, the mapping can be learned indirectly, by associating each node with itself\\nand one of its neighbors, with online learning being the method of choice in this case;\\nthe neighbor chosen at each iteration is defined by the sequence of visits generated on\\nthe basis of the transition probabilities. Batch learning was chosen because it generally\\nconverges more smoothly and more quickly than online learning and gives qualitatively\\nsimilar results. While the task and network architecture described in [1] allowed only\\nfor temporal association learning, in this study both temporal and spatial associations are\\nlearned simultaneously, thanks to the presence of the spatial units. However, the temporalonly (T-only) case, which has no spatial units, is included in the simulations performed\\nfor this study, to provide a benchmark for the evaluation of the results obtained with the\\nspatio-temporal (S- T) networks.\\nThe task described above allows the network to learn neighborhood relationships for which\\nspatial and temporal associations provide consistent information, that is, nodes experienced\\ncontiguously in time (as defined by the sequence) are also contiguous in space (being spatial neighbors). To tease apart the relative contributions of space and time, the task is kept\\nthe same, but the data employed for training the network is modified: the same layout is\\nused to generate the temporal sequence, but the x , y coordinates of the nodes are randomly\\npermuted (see right panel of Figure 2). If the permuted layout is then scanned following the\\nsame sequence of node visits used in the original version, the net effect is that the temporal\\nassociations remain the same, but the spatial associations change so that temporally neighboring nodes can now be spatially close or distant: the spatial associations are no longer\\nconsistent with the temporal associations. As Figure 4 illustrates, the training pairs (filled\\ncircles) all correspond to short distances in the original layout, but can have a distance\\nanywhere in the allowable range in the permuted layout. Since the temporal and spatial\\ndistances were consistent in the original layout, the original spatial distance can be used\\nas an indicator of temporal distance and Figure 4 can be interpreted as a plot of temporal\\ndistance vs. spatial distance for the permuted layout.\\nThe simulations described in the following include three experimental conditions: temporal\\nonly (no direct spatial information available); space and time consistent (the spatial coordinates and the temporal sequence are from the same layout); space and time inconsistent\\n(the spatial coordinates and the temporal sequence are from different layouts).\\n\\n\\fT. B. Ghise/li-Crippa and P. W. Munro\\n\\n20\\n\\nHidden unit representations are compared using Euclidean distance (cosine and inner product measures give consistent results); the internal representation distances are also used to\\ncompute their correlation with Euclidean distances between nodes in the layout (original\\nand permuted). The correlations increase with the number of hidden units for values of\\nH between 5 and 10 and then gradually taper off for values greater than 10. The results\\npresented in the remainder of the paper all pertain to networks trained with H = 20 and\\nwith hidden units using a tanh transfer function; all the results pertaining to S-T networks\\nrefer to networks with 2 spatial output units and cost function coefficients A = 0.625 and\\nB = 6.25.\\n\\n4 Results\\nFigure 3 provides a combined view of the results from all three experiments. The left panel\\nillustrates the evolution of the correlation between internal representation distances and\\nlayout (original and permuted) distances. The right panel shows the distributions of the\\ncorrelations at the end of training (1000 epochs). The first general result is that, when spatial information is available and consistent with the temporal information (original layout),\\nthe correlation between hidden unit distances and layout distances is consistently better\\nthan the correlation obtained in the case of temporal associations alone. The second general result is that, when spatial information is available but not consistent with the temporal\\ninformation (permuted layout), the correlation between hidden unit distances and original\\nlayout distances (which represent temporal distances) is similar to that obtained in the case\\nof temporal associations alone, except for the initial transient. When the correlation is computed with respect to the permuted layout distances, its value peaks early during training\\nand then decreases rapidly, to reach an asymptotic value well below the other three cases.\\nThis behavior is illustrated in the box plots in the right panel of Figure 3, which report the\\ndistribution of correlation values at the end of training.\\n\\n4.1\\n\\nTemporal-only vs. spatio-temporal\\n\\nAs a first step in this study, the effects of adding spatial information to the basic temporal\\nassociations used to train the network can be examined. Since the learning task is the same\\nfor both the T-only and the S-T networks except for the absence or presence of spatial\\ninformation during training, the differences observed can be attributed to the additional\\nspatial information available to the S-T networks. The higher correlation between internal\\nrepresentation distances and original layout distances obtained when spatial information is\\n\\n0\\n\\n~\\n\\n.,\\n\\n8\\n\\n.,\\n\\nS and T CO\\\"Isistent\\n\\n0\\n\\n.\\n\\n0\\n\\n.\\n\\nT-o\\\"\\nSand T InCOnsistent\\n\\n0\\n\\ni:i\\n\\n-==~\\n\\n0\\n\\n(corr with T distance)\\n\\nii\\n\\n...\\n\\n?8 \\\"\\n\\n\\\"0\\n\\n0\\n\\n=s:\\n...........\\nE:2\\n\\nS and T Ir'ICOOSlStent\\n(corr. Wflh S distance)\\n\\n'\\\"ci\\n\\n~\\n\\n--'----'\\n\\nN\\n\\n0\\n\\n0\\n0\\n\\n0\\n0\\n\\n200\\n\\n400\\n600\\nOllnber 01 epochs\\n\\n800\\n\\n1000\\n\\nSandT\\n\\ncon_atent\\n\\nT-only\\n\\nSandT\\nSandT\\nInconsistent\\nineon.stant\\n(corr \\\" th T ast ) (corr wth 5 dst )\\n\\nFigure 3: Evolution of correlation during training (0 - 1000 epochs) (left). Distributions of\\ncorrelations at the end of training (1000 epochs) (right).\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n-\\n\\n21\\n\\nN\\n\\ndHU = 0.6 + 3.4d T + 0.3ds - 2.1( dT)2 + 0.4( d S )2 - 0.4d T ds\\n\\n0\\n\\n.,\\n\\n25\\n\\n0\\n\\n\\\",\\nE\\n~\\n\\n'\\\"\\n0\\n\\n15\\n\\n...\\n0\\n\\n05\\nN\\n\\n0\\n\\n15\\n0\\n0\\n\\n00\\n\\n02\\n\\n04\\n\\n08\\n\\n10\\n\\n12\\n\\nFigure 4: Distances in the original layout\\n(x) vs_ distances in the permuted layout\\n(y)_ The 66 training pairs are identified by\\nfilled circles_\\n\\n\\\"\\nFigure 5: Similarities (Euclidean distances)\\nbetween internal representations developed\\nby a S-T network (after 300 epochs)_ Figure\\n4 projects the data points onto the x, y plane_\\n\\navailable (see Figure 3) is apparent also when the evolution of the internal representations\\nis examined_ As Figure 6 illustrates, the presence of spatial information results in better\\ngeneralization for the pattern pairs outside the training set While the distances between\\ntraining pairs are mapped to similar distances in hidden unit space for both the T-only and\\nthe S-T networks, the T-only network tends to cluster the non-training pairs into a narrow\\nband of distances in hidden unit space. In the case of the S-T network instead, the hidden\\nunit distances between non-training pairs are spread out over a wider range and tend to\\nreflect the original layout distances.\\n4.2\\n\\nPermuted layout\\n\\nAs described above, with the permuted layout it is possible to decouple the spatial and\\ntemporal contributions and therefore study the effects of each. A comprehensive view of\\nthe results at a particular point during training (300 epochs) is presented in Figure 5, where\\nthe x, y plane represents temporal distance vs. spatial distance (see also Figure 4) and the z\\naxis represents the similarity between hidden unit representations. The figure also includes\\na quadratic regression surface fitted to the data points. The coefficients in the equation of\\nthe surface provide a quantitative measure of the relative contributions of spatial (ds) and\\ntemporal distances (dT ) to the similarity between hidden unit representations (d HU ):\\n(2)\\n\\nIn general, after the transient observed in early training (see Figure 3), the largest and most\\nsignificant coefficients are found for dT and (dT?, indicating a stronger dependence of\\ndHU on temporal distance than on spatial distance.\\nThe results illustrated in Figure 5 represent the situation at a particular point during training\\n(300 epochs). Similar plots can be generated for different points during training, to study\\nthe evolution of the internal representations. A different view of the evolution process is\\nprovided by Figure 7, in which the data points are projected onto the x,Z plane (top panel)\\nand the y,z plane (bottom panel) at four different times during training. In the top panel,\\n\\n14\\n\\n\\fT. B. Ghiselli-Crippa and P W Munro\\n\\n22\\n\\n,.. ,..\\n.. roo\\n:::\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~ ~\\n~ ~\\n\\n~\\n\\n-. .. -\\n\\n:::\\n~\\n\\n~\\n\\n02\\n\\n\\\"\\n\\n06\\n\\nO.\\n\\n\\\"_d\\n\\n\\\"\\n\\n12\\n\\n.\\n\\n,,\\n\\n~'\\n\\n~ :\\n~\\n\\n~\\n~,\\n\\n02\\n\\nos\\n\\n\\\"-'\\n\\n..\\n\\n'\\n\\n02\\n\\n.. .\\n06\\n\\n-\\n\\n,\\n\\ntP\\n\\n.\\n\\nDO\\n\\n0\\n\\n, ,\\n.I'\\n\\n~\\n\\n12\\n\\n.',\\n\\n00\\n\\n02\\n\\n\\\" \\\"-'\\n06\\n\\n.\\\"\\n\\n02\\n\\n~\\n\\n~\\n\\n.~.\\n','\\n\\n~\\n\\n.. .. \\\" \\\"\\n\\n~\\n\\n06\\n\\n00\\n\\n02\\n\\n\\\"_d\\n\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n:\\n\\n~,\\n\\n~ ~\\n\\n..\\n\\nf/Po\\n\\n<P\\n\\n\\\"\\n\\ne,\\n\\n.\\n\\nDO\\n\\n.:.\\n\\n~\\n\\n00\\n\\n02\\n\\n\\\"\\n\\n.. ..\\n\\n\\\"-'\\n\\n10\\n\\n12\\n\\n.. .. .. \\\"\\n\\n12\\n\\n\\\" _d\\n\\n:::\\n\\n~\\n,\\n\\n~\\n12\\n\\n0\\nN\\n\\n~ ~\\n\\n',~-,\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n_\\n?\\n\\n~\\n\\n~\\n\\n12\\n\\n\\\"\\n\\n0\\n\\n00\\n\\n~\\n\\n:::\\n\\ng\\n10\\n\\n~\\n\\n~\\n\\n~ ~\\n\\nO.\\n\\n,\\n\\n:::\\n\\n;; ~\\n\\n.~\\n00\\n\\n00\\n\\n~.\\n\\n~\\n\\n,\\n\\n\\\"_d\\n\\n,\\n\\n:; ~\\n\\n~\\n\\n~\\n\\n~\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\ni\\n\\n~\\n\\n~\\n\\n~,\\n\\n:::\\n\\n~\\n\\n~\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\no\\n\\n,.~,o\\n\\n: s\\n\\nrIP 0\\n\\n00\\n\\n0\\n\\n?\\n\\n','\\n\\n00\\n\\n02\\n\\n\\\"\\n\\nO.\\n\\n\\\"-'\\n\\no.\\n\\n\\\"\\n\\n12\\n\\nFigure 6: Internal representation distances vs. original layout distances: S-T network (top)\\nvs. T-only network (bottom). The training pairs are identified by filled circles. The presence\\nof spatial information results in better generalization for the pairs outside the training set.\\nthe internal representation distances are plotted as a function of temporal distance (i.e., the\\nspatial distance from the original layout), while in the bottom panel they are plotted as a\\nfunction of spatial distance (from the permuted layout). The higher asymptotic correlation\\nbetween internal representation distances and temporal distances, as opposed to spatial\\ndistances (see Figure 3), is apparent also from the examination of the evolutionary plots,\\nwhich show an asymptotic behavior with respect to temporal distances (see Figure 7, top\\npanel) very similar to the T-only case (see Figure 6, bottom panel).\\n\\n5 Discussion\\nThe first general conclusion that can be drawn from the examination of the results described\\nin the previous section is that, when the spatial information is available and consistent with\\nthe temporal information (original layout), the similarity structure of the hidden unit representations is closer to the structure of the original layout than that obtained by using\\ntemporal associations alone. The second general conclusion is that, when the spatial information is available but not consistent with the temporal information (permuted layout),\\nthe similarity structure of the hidden unit representations seems to correspond to temporal\\nmore than spatial proximity. Figures 5 and 7 both indicate that temporal associations take\\nprecedence over spatial associations. This result is in agreement with the results described\\nin [1], showing how temporal associations (plus some high-level constraints) significantly\\ncontribute to the internal representation of global spatial information. However, spatial information certainly is very beneficial to the (temporal) acquisition of a layout, as proven by\\nthe results obtained with the S-T network vs. the T-only network.\\nIn terms of the model presented in this paper, the results illustrated in Figures 5 and 7 can\\nbe compared with the experimental data reported for recognition priming ([2], [3], [4]),\\nwith distance between internal representations corresponding to reaction time. The results\\nof our model indicate that distances in both the spatially far and spatially close condition\\nappear to be consistently shorter for the training pairs (temporally close) than for the nontraining pairs (temporally distant), highlighting a strong temporal effect consistent with the\\ndata reported in [2] and [4] (for spatially far pairs) and in [3] (only for the spatially close\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n~~\\n\\n0_\\n\\nri\\n\\n; -~-'\\n~. ~~.. .\\nSl\\n...........\\n0\\n\\n\\\" ...... .\\nj!I!A\\n\\n..\\n,.\\n\\n0\\n\\n,.\\n\\n~\\n\\n~\\n\\n~\\n\\n23\\n\\n\\\\\\n\\n0\\n\\n?\\n\\nlfIiiIo\\n\\n'0'\\n\\n110\\n\\n0\\n\\n~'--_ _ _ _ _-.J\\n\\n00\\n\\n02\\n\\nO.\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\nO.\\n\\n~\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\n0.4\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n02\\n\\n0\\\"\\n\\n01\\nIn_d (S)\\n\\n01\\n\\n'0\\n\\n12\\n\\n~l.-\\n\\n00\\n\\n0.2\\n\\no.\\n\\not\\n.._d(S)\\n\\n02\\n\\nO.\\n\\n01\\n\\n10\\n\\n12\\n\\n0.0\\n\\n02\\n\\n04\\n\\n08\\n...u:I (S)\\n\\n01\\n\\noa\\n\\n10\\n\\n12\\n\\nIn_den\\n\\nL..-_ _ _ _- . l\\n00\\n\\n0 0\\n\\nl'I_d (T)\\n\\nIn_d(TI\\n\\nIn _d (T}\\n\\nall\\n\\n10\\n\\n12\\n\\n00\\n\\n_ _ _ _ _-.J\\n02\\n\\nO.\\n\\n06\\n\\noa\\n\\n10\\n\\n12\\n\\n!rUi (S)\\n\\nFigure 7: Internal representation distances vs. temporal distances (top) and vs. spatial\\ndistances (bottom) for a S-T network (permuted layout). The training pairs are identified\\nby filled circles. The asymptotic behavior with respect to temporal distances (top panel) is\\nsimilar to the T-only condition. The bottom panel indicates a weak dependence on spatial\\ndistances.\\ncase). For the training pairs (temporally close), slightly shorter distances are obtained for\\nspatially close pairs vs. spatially far pairs; this result does not provide support for the\\nexperimental data reported in either [3] (strong spatial effect) or [2] (no spatial effect).\\nFor the non-training pairs (temporally distant), long distances are found throughout, with\\nno strong dependence on spatial distance; this effect is consistent with all the reported\\nexperimental data. Further simulations and statistical analyses are necessary for a more\\nconclusive comparison with the experimental data.\\nReferences\\n[1] Ghiselli-Crippa, TB. & Munro, P.w. (1994). Emergence of global structure from local associations. In J.D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural Information Processing\\nSystems 6, pp. 1101-1108. San Francisco, CA: Morgan Kaufmann.\\n[2] Clayton, K.N. & Habibi, A. (1991). The contribution of temporal contiguity to the spatial priming\\neffect. Journal of Experimental Psychology: Learning. Memory. and Cognition 17:263-271.\\n[3] McNamara, TP., Halpin. J.A. & Hardy, J.K. (1992). Spatial and temporal contributions to the\\nstructure of spatial memory. Journal of Experimental Psychology: Learning. Memory. and Cognition\\n18:555-564.\\n[4] Curiel, J.M. & Radvansky, G.A. (1998). Mental organization of maps. Journal of Experimental\\nPsychology: Learning. Memory. and Cognition 24:202-214.\\n[5] Caruana, R. & de Sa, VR. (1997). Promoting poor features to supervisors: Some inputs work\\nbetter as outputs . In M.e. Mozer, M.I. Jordan, & T Petsche (Eds.), Advances in Neural Information\\nProcessing Systems 9, pp. 389-395. Cambridge, MA: MIT Press.\\n[6] Servan-Schreiber, D., Cleeremans, A. & McClelland, J.L. (1989). Learning sequential structure\\nin simple recurrent networks. In D.S. Touretzky (Ed.), Advances in Neural Information Processing\\nSystems 1, pp. 643-652. San Mateo, CA: Morgan Kaufmann.\\n\\n\\fNeural Representation of Multi-Dimensional\\nStimuli\\n\\nChristian W. Eurich, Stefan D. Wilke and Helmut Schwegler\\nInstitut fUr Theoretische Physik\\nUniversitat Bremen, Germany\\n(eurich,swilke,schwegler)@physik.uni-bremen.de\\n\\nAbstract\\nThe encoding accuracy of a population of stochastically spiking neurons\\nis studied for different distributions of their tuning widths. The situation\\nof identical radially symmetric receptive fields for all neurons, which\\nis usually considered in the literature, turns out to be disadvantageous\\nfrom an information-theoretic point of view. Both a variability of tuning widths and a fragmentation of the neural population into specialized\\nsubpopulations improve the encoding accuracy.\\n\\n1 Introduction\\nThe topic of neuronal tuning properties and their functional significance has focused much\\nattention in the last decades. However, neither empirical findings nor theoretical considerations have yielded a unified picture of optimal neural encoding strategies given a sensory\\nor motor task. More specifically, the question as to whether narrow tuning or broad tuning\\nis advantageous for the representation of a set of stimulus features is still being discussed.\\nEmpirically, both situations are encountered: small receptive fields whose diameter is less\\nthan one degree can, for example, be found in the human retina [7] , and large receptive\\nfields up to 180 0 in diameter occur in the visual system of tongue-projecting salamanders\\n[10]. On the theoretical side, arguments have been put forward for small [8] as well as for\\nlarge [5, 1,9, 3, 13] receptive fields.\\nIn the last years, several approaches have been made to calculate the encoding accuracy\\nof a neural population as a function of receptive field size [5, 1,9,3, 13]. It has turned\\nout that for a firing rate coding, large receptive fields are advantageous provided that D 2:\\n3 stimulus features are encoded [9, 13]. For binary neurons, large receptive fields are\\nadvantageous also for D = 2 [5,3].\\nHowever, so far only radially symmetric tuning curves have been considered. For neural\\npopulations which lack this symmetry, the situation may be very different. Here we study\\nthe encoding accuracy of a popUlation of stochastically spiking neurons. A Fisher information analysis performed on different distributions of tunings widths will indeed reveal a\\nmuch more detailed picture of neural encoding strategies.\\n\\n\\fC. W. Eurich. S. D. Wilke and H. Schwegler\\n\\nJ J6\\n\\n2 Model\\nConsider a D-dimensional stimulus space, X. A stimulus is characterized by a position\\nx\\n(Xl, ... , XD) E X, where the value of feature i, Xi (i\\n1, ... , D), is measured\\nrelative to the total range of values in the i-th dimension such that it is dimensionless.\\nInformation about the stimulus is encoded by a popUlation of N stochastically spiking\\nneurons. They are assumed to have independent spike generation mechanisms such that the\\njoint probability distribution for observing n = (n(l), ... ,n(k), ... ,n(N?) spikes within a\\ntime interval T, Ps(n; x), can be written in the form\\n\\n=\\n\\n=\\n\\nN\\n\\nPs(n;x) =\\n\\nII\\n\\nps(k) (n(k);\\n\\nx),\\n\\n(1)\\n\\nk=l\\nwhere Ps(k) (n(k); x) is the single-neuron probability distribution of the number of observed\\nspikes given the stimulus at position x. Note that (1) does not exclude a correlation of the\\nneural firing rates, i.e., the neurons may have common input or even share the same tuning\\nfunction.\\nThe firing rates depend on the stimulus via the local values of the tuning functions, such that\\nx) can be written in the form Ps(k) (n(k); x) = S (n(k), j(k) (x), T), where the\\ntuning function of neuron k, j(k) (x), gives its mean firing rate in response to the stimulus\\nat position x. We assume here a form of the tuning function that is not necessarily radially\\nsymmetric,\\nPs(k) (n(k);\\n\\nf(') (x)\\n\\n= F4>\\n\\n(t\\n\\n(Xi\\n\\n~~r) )2) =, F? ( e( ')2) ,\\n\\n(2)\\n\\nwhere e(k) = (c~k), ... , c};?) is the center of the tuning curve of neuron k, O'~k) is its\\ntuning width in the i-th dimension, k )2 := (Xi - c~k?)2/O'ik)2 for i = 1, ... ,D, and\\n~(k)2 := ~~k)2 + ... + ~~)2. F > 0 denotes the maximal firing rate of the neurons, which\\nrequires that maxz~o fj>(z) = 1.\\n\\nd\\n\\nWe assume that the tuning widths O't), . .. ,O'~) of each neuron k are drawn from a distribution PO' (0'1, ... ,O'D). For a population oftuning functions with centers e(l), ... , e(N), a\\ndensity 1}(x) is introduced according to 1}(x) := L:~=l 8(x - e(k?).\\nThe encoding accuracy can be quantified by the Fisher information matrix, J, which is\\ndefined as\\n(3)\\n\\nwhere E[ . ..J denotes the expectation value over the probability distribution P(n; x) [2].\\nThe Fisher information yields a lower bound on the expected error of an unbiased estimator\\nthat retrieves the stimulus x from the noisy neural activity (Cramer-Rao inequality) [2]. The\\nminimal estimation error for the i-th feature Xi, ti,min, is given by t;,min = (J - 1 )ii which\\nreduces to t;,min = 1/ Jii(X) if J is diagonal.\\nWe shall now derive a general expression for the popUlation Fisher information. In the\\nnext chapter, several cases and their consequences for neural encoding strategies will be\\ndiscussed.\\nFor model neuron (k), the Fisher information (3) reduces to\\n(k)\\n\\nJ ij\\n\\n.\\n\\n(k)\\n\\n(X'O'I\\n\\n(k) _\\n\\\"\\\"'O'D) -\\n\\n1\\n\\n(k)\\nO'i\\n\\n(k)Aq..\\nO'j\\n\\n(\\n\\n~\\n\\n(k)2\\n\\n,F,T\\n\\n)\\n\\n(k) (k)\\n\\n~i ~j\\n\\n,\\n\\n(4)\\n\\n\\f117\\n\\nNeural Representation of Multi-Dimensional Stimuli\\n\\nwhere the dependence on the tuning widths is indicated by the list of arguments. The\\nfunction A.p depends on the shape of the tuning function and is given in [13]. The independence assumption (1) implies that the population Fisher information is the sum of\\n. d??d\\nI\\n\\\",N J(k)(\\n(k)\\n(k)) . U7\\nt he contn?b?\\nutlOns 0 f the III\\nIVI ua neurons, L.Jk=1 ij x; 0\\\"1 , ... ,0\\\"D\\nne now define\\na population Fisher information which is averaged over the distribution of tuning widths\\nPt:T(0\\\"1, . .. ,O\\\"D):\\nN\\n\\n(Jij (x)) 17 =\\n\\nL / d0\\\"1 . .. dO\\\"D Pt:T(0\\\"1,? .. , O\\\"D) Ji~k) (x; 0\\\"1, ? .. , O\\\"D) .\\n\\n(5)\\n\\nk= 1\\n\\nIntroducing the density of tuning curves, 1J(x), into (5) and assuming a constant distribution, 1J(x) == 1J == const., one obtains the result that the population Fisher information\\nbecomes independentofx and that the off-diagonal elements of J vanish [13]. The average\\npopulation Fisher information then becomes\\n(Jij)t:T =\\n\\n1JD K.p (F, r, D ) \\\\/\\n\\nflt:l\\n0\\\"1) ~\\n0\\\";\\nVij,\\n17\\n\\n(6)\\n\\nwhere K.p depends on the geometry of the tuning curves and is defined in [13].\\n\\n3 Results\\nIn this section, we consider different distributions of tuning widths in (6) and discuss advantageous and disadvantageous strategies for obtaining a high representational accuracy\\nin the neural population.\\nRadially symmetric tuning curves.\\nthe tuning-width distribution reads\\n\\nFor radially symmetric tuning curves of width a,\\nD\\n\\nPt:T(O\\\"l, .. . ,O\\\"D)\\n\\n= II O(O\\\"i -a);\\ni=l\\n\\nsee Fig. 1a for a schematic visualization of the arrangement of the tuning widths for the\\ncase D = 2. The average population Fisher information (6) for i = j becomes\\n(Jii)t:T =\\n\\n1JDK.p(F, r, D) aD -\\n\\n2,\\n\\n(7)\\n\\na result already obtained by Zhang and Sejnowski [13]. Equation (7) basically shows that\\nthe minimal estimation error increases with a for D = 1, that it does not depend on a for\\nD = 2, and that it decreases as a increases for D 2: 3. We shall discuss the relevance of\\nthis case below.\\nIdentical tuning curves without radial symmetry. Next we discuss tuning curves which\\nare identical but not radially symmetric; the tuning-width distribution for this case is\\nD\\n\\nPt:T(0\\\"1, . .. ,O\\\"D)\\n\\n=\\n\\nII\\n\\nO(O\\\"i -\\n\\nad,\\n\\ni=l\\n\\nwhere ai denotes the fixed width in dimension i. For i = j, the average population Fisher\\ninformation (6) reduces to [11,4]\\n(Jii)t:T = 1JDK.p ( F,\\n\\nr,\\n\\nD)\\n\\nDfl 1=1\\n0\\\"1\\n-2\\n\\nO\\\"i\\n\\n.\\n\\n(8)\\n\\n\\fc.\\n\\n118\\n\\n(a)\\n\\nW. Eurich, S. D. Wilke and H. Schwegler\\n\\n(b)\\n\\n/\\n\\nFigure 1: Visualization of different distributions of\\ntuning widths for D = 2. (a) Radially symmetric tuning curves. The dot indicates a fixed (j, while the diagonalline symbolizes a variation in (j discussed in [13].\\n(b) Identical tuning curves which are not radially symmetric. (c) Tuning widths uniformly distributed within\\na small rectangle. (d) Two sUbpopulations each of\\nwhich is narrowly tuned in one dimension and broadly\\ntuned in the other direction.\\n\\n.\\n\\n(c)\\n\\n(d)\\n\\n.\\n\\nb _ b\\n2\\n\\n.\\n\\nEquation (8) contains (7) as a special case. From (8) it becomes immediately clear that the\\nexpected minimal square encoding error for the i-th stimulus feature, ?~ min = 1/ (Jii(X))u,\\ndepends on i, i. e., the population specializes in certain features. The error obtained in\\ndimension i thereby depends on the tuning widths in all dimensions.\\nWhich encoding strategy is optimal for a population whose task it is to encode a single\\nfeature, say feature i, with high accuracy while not caring about the other dimensions? In\\norder to answer this question, we re-write (8) in terms of receptive field overlap.\\nFor the tuning functions f(k) (x) encountered empirically, large values ofthe single-neuron\\nFisher information (4) are typically restricted to a region around the center of the tuning\\nfunction, c(k). The fraction p({3) of the Fisher information that falls into a region ED\\nJ~(k)2 ~ (3 aroundc(k) is given by\\n\\nf\\np({3)\\n\\n:=\\n\\nd\\n\\nE; d\\nX\\n\\nD\\n\\nD\\n\\n\\\"\\\",D\\nX L....i=l\\n\\nX\\n\\n(k) ( )\\nJ ii X\\n\\n2:~t=l J~~)\\n( )\\nu\\nX\\n\\nj3\\n\\nf\\n\\nd~ ~D+l At/>(e, F, T)\\n\\no\\n\\n(9)\\n\\n00\\n\\nf\\n\\nd~ ~D+l At/>(~2, F, T)\\n\\no\\n\\nwhere the index (k) was dropped because the tuning curves are assumed to have identical shapes. Equation (9) allows the definition of an effective receptive field, RF~~,\\ninside of which neuron k conveys a major fraction Po of Fisher information, RF~~ :=\\n\\n{xl~ ~ {3o} , where (3o is chosen such that p({3o)\\n\\n= Po. The Fisher information a\\n\\nneuron k carries is small unless x E RF~~. This has the consequence that a fixed stimulus\\nx is actually encoded only by a subpopulation of neurons. The point x in stimulus space is\\ncovered by\\n27r D/ 2({30)D D _\\n(10)\\nNcode:= 1] Dr(D/2)\\n(Jj\\n\\n}1\\n\\nreceptive fields. With the help of (10), the average population Fisher information (8) can\\nbe re-written as\\n(11)\\n\\nEquation (11) can be interpreted as follows: We assume that the population of neurons\\nencodes stimulus dimension i accurately, while all other dimensions are of secondary importance. The average population Fisher information for dimension i, (Jii ) u, is determined\\nby the tuning width in dimension i, (ji, and by the size of the active subpopulation, N code '\\nThere is a tradeoff between these quantities. On the one hand, the encoding error can be\\ndecreased by decreasing (ji, which enhances the Fisher information carried by each single\\n\\n\\fNeural Representation ofMulti-Dimensional Stimuli\\n\\n119\\n\\nneuron. Decreasing ai, on the other hand, will also shrink the active subpopulation via\\n(10). This impairs the encoding accuracy, because the stimulus position is evaluated from\\nthe activity of fewer neurons. If (11) is valid due to a sufficient receptive field overlap,\\nNcode can be increased by increasing the tuning widths, aj, in all other dimensions j i- i.\\nThis effect is illustrated in Fig. 2 for D = 2.\\n\\nX2\\nc=:>\\n\\nx2, s\\n\\nX2\\n,II\\\"\\\\..\\\\\\n\\nU\\nx2,s\\n\\nFigure 2: Encoding strategy for a stimulus characterized by parameters Xl,s and X2,s' Feature Xl is to be encoded accurately. Effective receptive field shapes are indicated for both\\npopulations. If neurons are narrowly tuned in X2 (left), the active population (solid) is\\nsmall (here: Ncode = 3). Broadly tuned receptive fields for X2 (right) yield a much larger\\npopulation (here: Ncode = 27) thus increasing the encoding accuracy.\\nIt shall be noted that although a narrow tuning width ai is advantageous, the limit ai ---t 0\\nyields a bad representation. For narrowly tuned cells, gaps appear between the receptive\\nfields: The condition 17(X) == const. breaks down, and (6) is no longer valid. A more\\ndetailed calculation shows that the encoding error diverges as ai --* 0 [4]. The fact that\\nthe encoding error decreases for both narrow tuning and broad tuning - due to (11) - proves\\nthe existence of an optimal tuning width, An example is given in Fig. 3a.\\n3\\n\\nrTI~--~------~----~------~\\n\\n1\\\\\\n\\n(b)\\n\\nIi\\n\\n1\\\\\\n\\nIi\\n\\n0.8\\n\\nII\\nII\\nI;\\n\\n2\\n\\n1\\\\\\n\\nI ,\\n\\n;to.6\\n~\\n\\n~~~~;::~-:.~~;:\\n\\nA\\n\\nN~O.4\\nw\\n\\n----- ---- ----- -- ---\\n\\nv\\n\\n0.2\\n\\nO'----~--~--~-----'-------'\\n\\no\\n\\n0.5\\n\\n1\\nA\\n\\n1.5\\n\\n2\\n\\nFigure 3: (a) Example for the encoding behavior with narrow tuning curves arranged on\\na regular lattice of dimension D = 1 (grid spacing ~). Tuning curves are Gaussian, and\\nneural firing is modeled as a Poisson process, Dots indicate the minimal square encoding\\nerror averaged over a uniform distribution of stimuli, (E~in)' as a function ofa. The minimum is clearly visible. The dotted line shows the corresponding approximation according\\nto (8). The inset shows Gaussian tuning curves of optimal width, ao pt ~ 0.4~. (b) 9D()..)\\nas a function of ).. for different values of D.\\n\\n\\fc. W.\\n\\n120\\n\\nEurich, S. D. Wilke and H. Schwegler\\n\\nNarrow distribution of tuning curves. In order to study the effects of encoding the\\nstimulus with distributed tuning widths instead of identical tuning widths as in the previous\\ncases, we now consider the distribution\\n\\ng:i e\\nD\\n\\nPu(lT1,'\\\" ,lTD)\\n\\n=\\n\\n[lTi - (O'i -\\n\\ni)] e [(O'i + i) -lTi] ,\\n\\n(12)\\n\\ne\\n\\ndenotes the Heaviside step function. Equation (12) describes a uniform distriwhere\\nbution in a D-dimensional cuboid of size b1 , ... , bD around (0'1, .. . 0'D); cf. Fig. 1c. A\\nstraightforward calculation shows that in this case, the average population Fisher information (6) for i = j becomes\\n\\n(Jii)u\\n\\n= f/DKtj) (F, T, D) n~l\\nO'~ 0'1\\n\\n{\\n\\n1\\n1 + 12\\n\\n(bO'i 2+ 0 [( O'ib 4] }.\\ni )\\n\\ni )\\n\\n(13)\\n\\nA comparison with (8) yields the astonishing result that an increase in bi results in an\\nincrease in the i-th diagonal element of the average population Fisher information matrix\\nand thus in an improvement in the encoding of the i-th stimulus feature, while the encoding\\nin dimensions j :f. i is not affected. Correspondingly, the total encoding error can be\\ndecreased by increasing an arbitrary number of edge lengths of the cube. The encoding by\\na population with a variability in the tuning curve geometries as described is more precise\\nthan that by a uniform population. This is true/or arbitrary D. Zhang and Sejnowski [13]\\nconsider the more artificial situation of a correlated variability ofthe tuning widths: tuning\\ncurves are always assumed to be radially symmetric. This is indicated by the diagonal\\nline in Fig. 1a. A distribution of tuning widths restricted to this subset yields an average\\npopulation Fisher information ex: (O'D-2) and does not improve the encoding for D = 2 or\\n\\nD=3.\\nFragmentation into D subpopulations. Finally, we study a family of distributions of\\ntuning widths which also yields a lower minimal encoding error than the uniform population. Let the density of tuning curves be given by\\n1 D\\n\\nPu(lT1,'\\\" ,lTD) = D\\n\\nL 6( lTi i=l\\n\\nAO')\\n\\nII 6(lTj - 0'),\\n\\n(14)\\n\\nj?-i\\n\\nwhere A > O. For A = 1, the population is uniform as in (7). For A :f. 1, the population\\nis split up into D subpopulations; in subpopulation i, lTi is modified while lTj == 0' for\\nj :f. i. See Fig. Id for an example. The diagonal elements ofthe average population Fisher\\ninformation are\\n\\n(Jii)u\\n\\n{1 + (D = f/DKtj)(F, T, D) -D-2\\nIT\\nDA\\n\\nI)A 2 }\\n\\n'\\n\\n(15)\\n\\nwhere the term in brackets will be abbreviated as 9D(A). (Jii)u does not depend on i in\\nthis case because of the symmetry in the sUbpopulations. Equation (15) and the uniform\\ncase (7) differ by 9D(A) which will now be discussed. Figure 3b shows 9D(A) for different\\nvalues of D. For A = 1, 9D(A) = 1 and (7) is recovered as expected. 9D(A) = 1\\nalso holds for A = 1/ (D - 1) < 1: narrowing one tuning width in each subpopulation\\nwill at first decrease the resolution provided D 2: 3; this is due to the fact that Ncode is\\ndecreased. For A < 1/(D - 1), however, 9D(A) > 1, and the resolution exceeds (Jii)u in\\n(7) because each neuron in the i-th subpopulation carries a high Fisher information in the\\ni-th dimension. D = 2 is a special case where no impairment of encoding occurs because\\nthe effect of a decrease of Ncode is less pronounced. Interestingly, an increase in A also\\nyields an improvement in the encoding accuracy. This is a combined effect resulting from\\nan increase in Ncode on the one hand and the existence of D subpopulations, D - 1 of\\n\\n\\fNeural Representation of Multi-Dimensional Stimuli\\n\\n121\\n\\nwhich maintain their tuning widths in each dimension on the other hand. The discussion\\nof 9D(>\\\") leads to the following encoding strategy. For small >.., (Jii)u increases rapidly,\\nwhich suggests a fragmentation of the population into D subpopulations each of which\\nencodes one feature with high accuracy, i.e., one tuning width in each subpopulation is\\nsmall whereas the remaining tuning widths are broad. Like in the case discussed above, the\\ntheoretical limit of this method is a breakdown of the approximation of TJ == const. and the\\nvalidity of (6) due to insufficient receptive field overlap.\\n\\n4 Discussion and Outlook\\nWe have discussed the effects of a variation of the tuning widths on the encoding accuracy\\nobtained by a population of stochastically spiking neurons. The question of an optimal\\ntuning strategy has turned out to be more complicated than previously assumed. More\\nspecifically, the case which focused most attention in the literature - radially symmetric\\nreceptive fields [5, 1,9, 3, 13] - yields a worse encoding accuracy than most other cases we\\nhave studied: uniform populations with tuning curves which are not radially symmetric;\\ndistributions of tuning curves around some symmetric or non-symmetric tuning curve; and\\nthe fragmentation of the population into D subpopulations each of which is specialized in\\none stimulus feature.\\nIn a next step, the theoretical results will be compared to empirical data on encoding properties of neural popUlations. One aspect is the existence of sensory maps which consist\\nof neural subpopulations with characteristic tuning properties for the features which are\\nrepresented. For example, receptive fields of auditory neurons in the midbrain of the barn\\nowl have elongated shapes [6]. A second aspect concerns the short-term dynamics of receptive fields. Using single-unit recordings in anaesthetized cats, Worgotter et al. [12]\\nobserved changes in receptive field size taking place in 50-lOOms. Our findings suggest\\nthat these dynamics alter the resolution obtained for the corresponding stimulus features.\\nThe observed effect may therefore realize a mechanism of an adaptable selective signal\\nprocessing.\\n\\nReferences\\n[1] Baldi, P. & HeiJigenberg, W. (1988) BioI. Cybern. 59:313-318.\\n[2] Deco, G. & Obradovic, D. (1997) An Information-Theoretic Approach to Neural Computing.\\nNew York: Springer.\\n[3] Eurich, C. W. & Schwegler, H. (1997) BioI. Cybern. 76: 357-363.\\n\\n[4] Eurich, C. W. & Wilke, S. D. (2000) NeuraL Compo (in press).\\n[5] Hinton, G. E., McClelland, J. L. & Rumelhart, D. E (1986) In Rumelhart, D. E. & McClelland,\\nJ. L. (eds.), ParaLLeL Distributed Processing, Vol. 1, pp. 77-109. Cambridge MA: MIT Press.\\n[6] Knudsen, E. I. & Konishi, M. (1978) Science 200:795-797.\\n[7] Kuffter, S. W. (1953) 1. Neurophysiol. 16:37-68.\\n[8] Lettvin, J. Y., Maturana, H. R., McCulloch, W. S. & Pitts, W. H. (1959) Proc. Inst. Radio Eng.\\nNY 47:1940-1951.\\n[9] Snippe, H. P. & Koenderink, J. J. (1992) BioI. Cybern. 66:543-551.\\n[10] Wiggers, W., Roth, G., Eurich, C. W. & Straub, A. (1995) J. Camp. Physiol. A 176:365-377.\\n[11] Wilke, S. D. & Eurich, C. W. (1999) In Verleysen, M. (ed.), ESANN 99, European Symposium\\non Artificial Neural Networks, pp. 435-440. Brussels: D-Facto.\\n[12] Worgotter, F., Suder, K., Zhao, Y., Kerscher, N., Eysel, U. T. & Funke, K. (1998) Nature\\n396:165-168.\\n[13] Zhang, K. & Sejnowski, T. J. (1999) NeuraL Compo 11:75-84.\\n\\n\\f\",\n          \"Searching for Character Models\\n\\nJaety Edwards\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\njaety@cs.berkeley.edu\\n\\nDavid Forsyth\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\ndaf@cs.berkeley.edu\\n\\nAbstract\\nWe introduce a method to automatically improve character models for a\\nhandwritten script without the use of transcriptions and using a minimum\\nof document specific training data. We show that we can use searches for\\nthe words in a dictionary to identify portions of the document whose\\ntranscriptions are unambiguous. Using templates extracted from those\\nregions, we retrain our character prediction model to drastically improve\\nour search retrieval performance for words in the document.\\n\\n1 Introduction\\nAn active area of research in machine transcription of handwritten documents is reducing\\nthe amount and expense of supervised data required to train prediction models. Traditional\\nOCR techniques require a large sample of hand segmented letter glyphs for training. This\\nper character segmentation is expensive and often impractical to acquire, particularly if the\\ncorpora in question contain documents in many different scripts.\\nNumerous authors have presented methods for reducing the expense of training data by\\nremoving the need to segment individual characters. Both Kopec et al [3] and LeCun et al\\n[5] have presented models that take as input images of lines of text with their ASCII transcriptions. Training with these datasets is made possible by explicitly modelling possible\\nsegmentations in addition to having a model for character templates.\\nIn their research on ?wordspotting?, Lavrenko et al [4] demonstrate that images of entire\\nwords can be highly discriminative, even when the individual characters composing the\\nword are locally ambiguous. This implies that images of many sufficiently long words\\nshould have unambiguous transcriptions, even when the character models are poorly tuned.\\nIn our previous work, [2], the discriminatory power of whole words allowed us to achieve\\nstrong search results with a model trained on a single example per character.\\nThe above results have shown that A) one can learn new template models given images of\\ntext lines and their associated transcriptions, [3, 5] without needing an explicit segmentation\\nand that B) entire words can often be identified unambiguously, even when the models for\\nindividual characters are poorly tuned. [2, 4]. The first of these two points implies that\\ngiven a transcription, we can learn new character models. The second implies that for at\\nleast some parts of a document, we should be able to provide that transcription ?for free?,\\nby matching against a dictionary of known words.\\n\\n\\fs1\\n\\ns2\\n\\ns3\\n\\ns4\\n\\ns5\\n\\ns6\\n\\ns7\\n\\ns8\\n\\n?d\\n\\ndi\\n\\nix\\n\\nxe\\n\\ner\\n\\nri\\n\\nis\\n\\ns?\\n\\nFigure 1: A line, and the states that generate it. Each state st is defined by its left and\\nright characters ctl and ctr (eg ?x? and ?e? for s4 ). In the image, a state spans half of each\\nof these two characters, starting just past the center of the left character and extending to\\nthe center of the right character, i.e. the right half of the ?x? and the left half of the ?e?\\nin s4 . The relative positions of the two characters is given by a displacement vector dt\\n(superimposed on the image as white lines). Associating states with intracharacter spaces\\ninstead of with individual characters allows for the bounding boxes of characters to overlap\\nwhile maintaining the independence properties of the Markov chain.\\nIn this work we combine these two observations in order to improve character models\\nwithout the need for a document specific transcription. We provide a generic dictionary of\\nwords in the target language. We then identify ?high confidence? regions of a document.\\nThese are image regions for which exactly one word from our dictionary scores highly\\nunder our model. Given a set of high confidence regions, we effectively have a training\\ncorpus of text images with associated transcriptions. In these regions, we infer a segmentation and extract new character examples. Finally, we use these new exemplars to learn\\nan improved character prediction model. As in [2], our document in this work is a 12th\\ncentury manuscript of Terence?s Comedies obtained from Oxford?s Bodleian library [1].\\n\\n2 The Model\\nHidden Markov Models are a natural and widely used method for modeling images of text.\\nIn their simplest incarnation, a hidden state represents a character and the evidence variable\\nis some feature vector calculated at points along the line. If all characters were known to\\nbe of a single fixed width, this model would suffice. The probability of a line under this\\nmodel is given as\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(im[w?(t?1):w?t]|ct )\\n(1)\\nt>1\\n\\nwhere ct represents the tth character on the line, ? represents the start state, w is the width\\nof a character, and im[w(t?1)+1:wt] represents the column of pixels beginning at column\\nw ? (t ? 1) + 1 of the image and ending at column w ? t, (i.e. the set of pixels spanned by\\nc)\\nUnfortunately, character?s widths do vary quite substantially and so we must extend the\\nmodel to accommodate different possible segmentations. A generalized HMM allows us to\\ndo this. In this model a hidden state is allowed to emit a variable length series of evidence\\nvariables. We introduce an explicit distribution over the possible widths of a character.\\nLetting dt be the displacement vector associated with the tth character, and ctx refer to the\\nx location of the left edge of a character on the line, the probability of a line under this\\nrevised model is\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(dt |ct )p(im[ctx +1:ctx +d] |dt , ct )\\n(2)\\nt>1\\n\\nThis is the model we used in [2]. It performs far better than using an assumption of fixed\\nwidths, but it still imposes unrealistic constraints on the relative positions of characters. In\\n\\n\\fparticular, the portion of the ink generated by the current character is assumed to be independent of the preceding character. In other words, the model assumes that the bounding\\nboxes of characters do not overlap. This constraint is obviously unrealistic. Characters\\nroutinely overlap in our documents. ?f?s, for instance, form ligatures with most following characters. In previous work, we treated this overlap as noise, hurting our ability to\\ncorrectly localize templates. Under this model, local errors of alignment would also often propagate globally, adversely affecting the segmentation of the whole line. For search,\\nthis noisy segmentation still provides acceptable results. In this work, however, we need\\nto extract new templates, and thus correct localization and segmentation of templates is\\ncrucial.\\nIn our current work, we have relaxed this constraint, allowing characters to partially overlap. We achieve this by changing hidden states to represent character bigrams instead of\\nsingle characters (Figure 1). In the image, a state now spans the pixels from just past the\\ncenter of the left character to the pixel containing the center of the right character. We\\nadjust our notation somewhat to reflect this change, letting st now represent the tth hidden state and ctl and ctr be the left and right characters associated with s. dt is now the\\ndisplacement vector between the centers of ctl and ctr .\\nThe probability of a line under this, our actual, model is\\nY\\np(line) = p(s1 |?)\\np(st |st?1 )p(dt |ctl , ctr )p(im[stx +1:stx +dt ]|ctl , ctr , dt )\\n\\n(3)\\n\\nt>1\\n\\nThis model allows overlap of bounding boxes, but it does still make the assumption that\\nthe bounding box of the current character does not extend past the center of the previous\\ncharacter. This assumption does not fully reflect reality either. In Figure 1, for example,\\nthe left descender of the x extends back further than the center of the preceding character.\\nIt does, however, accurately reflect the constraints within the heart of the line (excluding\\nascenders and descenders). In practice, it has proven to generate very accurate segmentations. Moreover, the errors we do encounter no longer tend to affect the entire line, since\\nthe model has more flexibility with which to readjust back to the correct segmentation.\\n2.1 Model Parameters\\nOur transition distribution between states is simply a 3-gram character model. We train this\\nmodel using a collection of ASCII Latin documents collected from the web. This set does\\nnot include the transcriptions of our documents.\\nConditioned on displacement vector, the emission model for generating an image chunk\\ngiven a state is a mixture of gaussians. We associate with each character a set of image\\nwindows extracted from various locations in the document. We initialize these sets with\\none example a piece from our hand cut set (Figure 2). We adjust the probability of an image\\ngiven the state to include the distribution over blocks by expanding the last term of Equation\\n3 to reflect this mixture. Letting bck represent the k th exemplar in the set associated with\\ncharacter c, the conditional probability of an image region spanning the columns from x to\\nx? is given as\\nX\\np(imx:x? |ctl , ctr , dt ) =\\np(imx:x? |bctl i , bctr j , dt )\\n(4)\\ni,j\\n\\nIn principle, the displacement vectors should now be associated with an individual block,\\nnot a character. This is especially true when we have both upper and lower case letters.\\nHowever, our model does not seem particularly sensitive to this displacement distribution\\nand so in practice, we have a single, fairly loose, displacement distribution per character.\\nGiven a displacement vector, we can generate the maximum likelihood template image\\nunder our model by compositing the correct halves of the left and right blocks. Reshaping\\n\\n\\fthe image window into a vector, the likelihood of an image window is then modeled as\\na gaussian, using the corresponding pixels in the template as the means, and assuming\\na diagonal covariance matrix. The covariance matrix largely serves to mask out empty\\nregions of a character?s bounding box, so that we do not pay a penalty when the overlap of\\ntwo characters? bounding boxes contains only whitespace.\\n2.2 Efficiency Considerations\\nThe number of possible different templates for a state is O(|B| ? |B| ? |D|), where |B| is\\nthe number of different possible blocks and |D| is the number of candidate displacement\\nvectors. To make inference in this model computationally feasible, we first restrict the\\ndomain of d. For a given pair of blocks bl and br , we consider only displacement vectors\\nwithin some small x distance from a mean displacement mbl ,br , and we have a uniform\\ndistribution within this region. m is initialized from the known size of our single hand cut\\ntemplate. In the current work, we do not relearn the m. These are held fixed and assumed\\nto be the same for all blocks associated with the same letter.\\nEven when restricting the number of d?s under consideration as discussed above, it is computationally infeasible to consider every possible location and pair of blocks. We therefore\\nprune our candidate locations by looking at the likelihood of blocks in isolation and only\\nconsidering locations where there is a local optimum in the response function and whose\\nvalue is better than a given threshold. In this case our threshold for a given location is that\\nL(block) < .7L(background) (where L(x) represents the negative log likelihood of x).\\nIn other words, a location has to look at least marginally more like a given block than it\\nlooks like the background.\\nAfter pruning locations in this manner, we are left with a discrete set of ?sites,? where we\\ndefine a site as the tuple (block type, x location, y location). We can enumerate the set of\\npossible states by looking at every pair of sites whose displacement vector has a non-zero\\nprobability.\\n2.3 Inference In The Model\\nThe statespace defined above is a directed acyclic graph, anchored at the left edge and\\nright edges of a line of text. A path through this lattice defines both a transcription and\\na segmentation of the line into individual characters. Inference in this model is relatively\\nstraightforward because of our constraint that each character may overlap only one preceding and one following character, and our restriction of displacement vectors to a small\\ndiscrete range. The first restriction means that we need only consider binary relations between templates. The second preserves the independence relationships of an HMM. A\\ngiven state st is independent of the rest of the line given the values of all other states within\\ndmax of either edge of st (where dmax is the legal displacement vector with the longest\\nx component.) We can therefore easily calculate the best path or explicitly calculate the\\nposterior of a node by traversing the state graph in topological order, sorted from left to\\nright. The literature on Weighted Finite State Transducers ([6], [5]) is a good resource for\\nefficient algorithms on these types of statespace graph.\\n\\n3 Learning Better Character Templates\\nWe initialize our algorithm with a set of handcut templates, exactly 1 per character, (Figure\\n2), and our goal is to construct more accurate character models automatically from unsupervised data. As noted above, we can easily calculate the posterior of a given site under\\nour model. (Recall that a site is a particular character template at a given (x,y) location in\\nthe line.) The traditional EM approach to estimating new templates would be to use these\\n\\n\\fFigure 2: Original Training Data These 22 glyphs are our only document specific training\\ndata. We use the model based on these characters to extract the new examples shown below\\n\\nFigure 3: Examples of extracted templates We extract new templates from high confidence\\nregions. From these, we choose a subset to incorporate into the model as new exemplars.\\nTemplates are chosen iteratively to best cover the space of training examples. Notice that\\nfor ?q? and ?a?, we have extracted capital letters, of which there were no examples in\\nour original set of glyphs. This happens when the combination of constraints from the\\ndictionary the surrounding glyphs make a ?q? or ?a? the only possible explanation for\\nthis region, even though its local likelihood is poor.\\n\\nsites as training examples, weighted by their posteriors. Unfortunately, the constraints imposed by 3 and even 4-gram character models seem to be insufficient. The posteriors of\\nsites are not discriminative enough to get learning off the ground.\\nThe key to successfully learning new templates lies is the observation from our previous\\nwork [2], that even when the posteriors of individual characters are not discriminative, one\\ncan still achieve very good search results with the same model. The search word in effect\\nserves as its own language model, only allowing paths through the state graph that actually\\ncontain it, and the longer the word the more it constrains the model. Whole words impose\\nmuch tighter constraints than a 2 or 3-gram character model, and it is only with this added\\npower that we can successfully learn new character templates.\\nWe define the score for a search as the negative log likelihood of the best path containing\\nthat word. With sufficiently long words, it becomes increasingly unlikely that a spurious\\npath will achieve a high score. Moreover, if we are given a large dictionary of words and\\nno alternative word explains a region of ink nearly as well as the best scoring word, then\\nwe can be extremely confident that this is a true transcription of that piece of ink.\\nStarting with a weak character model, we do not expect to find many of these ?high confidence? regions, but with a large enough document, we should expect to find some. From\\nthese regions, we can extract new, reliable templates with which to improve our character\\nmodels. The most valuable of these new templates will be those that are significantly different from any in our current set. For example, in Figure 3, note that our system identifies\\ncapital Q?s, even though our only input template was lower case. It identifies this ink as\\na Q in much the same way that a person solves a crossword puzzle. We can easily infer\\nthe missing character in the string ?obv-ous? because the other letters constrain us to one\\npossible solution. Similarly, if other character templates in a word match well, then we can\\nunambiguously identify the other, more ambiguous ones. In our Latin case, ?Quid? is the\\nonly likely explanation for ?-uid?.\\n3.1 Extracting New Templates and Updating The Model\\nWithin a high confidence region we have both a transcription and a localization of template\\ncenters. It remains only to cut out new templates. We accomplish this by creating a template\\nimage for the column of pixels from the corresponding block templates and then assigning\\nimage pixels to the nearest template character (measured by Euclidean distance).\\nGiven a set of templates extracted from high confidence regions, we choose a subset of\\n\\n\\fScore Under Model\\n\\nworse\\n3400\\n3350\\n3300\\nbest\\nConfidence Margins\\n\\nFigure 4: Each line segment in the lower figure represents a proposed location for a word\\nfrom our dictionary. It?s vertical height is the score of that location under our model. A\\nlower score represents a better fit. The dotted line is the score of our model?s best possible\\npath. Three correct words, ?nec?, ?quin? and ?dari?, are actually on the best path. We\\ndefine the confidence margin of a location as the difference in score between the best\\nfitting word from our dictionary and the next best.\\n\\nFigure 5: Extracting Templates For a region with sufficiently high confidence margin, we\\nconstruct the maximum likelihood template from our current exemplars. left, and we assign\\npixels from the original image to a template based on its distance to the nearest pixel in\\nthe template image, extracting new glyph exemplars right. These new glyphs become the\\nexemplars for our next round of training.\\n\\ntemplates that best explain the remaining examples. We do this in a greedy fashion by\\nchoosing the example whose likelihood is lowest under our current model and adding it to\\nour set. Currently, we threshold the number of new templates for the sake of efficiency. Finally, given the new set of templates, we can add them to the model and rerun our searches,\\npotentially identifying new high confidence regions.\\n\\n4 Results\\nOur algorithm iteratively improves the character model by gathering new training data from\\nhigh confidence regions. Figure 3 shows that this method finds new templates significantly\\ndifferent from the originals. In this document, our set of examples after one round appears\\nto cover the space of character images well, at least those in lower case. Our templates are\\nnot perfect. The ?a?, for instance, has become associated with at least one block that is in\\nfact an ?o?. These mistakes are uncommon, particularly if we restrict ourselves to longer\\nwords. Those that do occur introduce a tolerable level noise into our model. They make\\ncertain regions of the document more ambiguous locally, but that local ambiguity can be\\novercome with the context provided by surrounding characters and a language model.\\nImproved Character Models We evaluate the method more quantitatively by testing the\\nimpact of the new templates on the quality of searches performed against the document.\\nTo search for a given word, we rank lines by the ratio of the maximum likelihood transcription/segmentation that contains the search word to the likelihood of the best possible\\nsegmentation/transcription under our model. The lowest possible search score is 1, happening when the search word is actually a substring of the maximum likelihood transcription.\\nHigher scores mean that the word is increasingly unlikely under our model. In Figure 7, the\\nfigure on the left shows the improvement in ranking of the lines that truly contain selected\\nsearch words. The odd rows (in red) are search results using only the original 22 glyphs,\\n\\n\\f20\\n40\\n60\\n80\\n100\\n\\n200\\n\\n300\\n\\n400\\n\\n500\\n\\n600\\n\\nRnd 2\\n\\nRnd 1\\n\\n2700\\n2650\\n2600\\ndotted (wrong):\\nsolid (correct):\\n1920\\n1900\\n1880\\n1860\\n1840\\ndotted (wrong):\\nsolid (correct):\\n\\niam\\n\\nnupta\\nnuptiis\\n\\ninquam\\n\\n(v|u)ideo\\nvidet\\n\\nnupta\\nnuptiis\\n\\npost inquam\\npostquam\\n\\n(v|u)ideo\\nvidet\\n\\nFigure 6: Search Results with (Rnd 1) initial templates only and with (Rnd 2) templates\\nextracted from high confidence regions. We show results that have a score within 5% of the\\nbest path. Solid Lines are the results for the correct word. Dotted lines represent other\\nsearch results, where we have made a few larger in order to show those words that are\\nthe closest competitors to the true word. Many alternative searches, like the highlighted\\n?post? are actually portions of the correct larger words. These restrict our selection of\\nconfidence regions, but do not impinge on search quality.\\nEach correct word has significantly improved after one round of template reestimation.\\n?iam? has been correctly identified, and is a new high confidence region. Both ?nuptiis?\\nand ?postquam? are now the highest likelihood words for their region barring smaller\\nsubsequences, and ?videt? has narrowed the gap between its competitor ?video?.\\nwhile the even rows (in green) use an additional 332 glyphs extracted from high confidence\\nregions. Search results are markedly improved in the second model. The word ?est?, for\\ninstance, only had 15 of 24 of the correct lines in the top 100 under the original model,\\nwhile under the learned model all 24 are not only present but also more highly ranked.\\nImproved Search Figure 6 shows the improved performance of our refitted model for\\na single line. Most words have greatly improved relative to their next best alternative.\\n?postquam? and ?iam? were not even considered by the original model and now are nearly\\noptimal. The right of Figure 7 shows the average precision/recall curve under each model\\nfor 21 words with more than 4 occurrences in the dataset. Precision is the percentage\\nof lines truly containing a word in the top n search results, and recall is the percentage\\nof all lines containing the word returned in the top n results. The learned model clearly\\ndominates. The new model also greatly improves performance for rare words. For 320\\nwords ocurring just once in the dataset, 50% are correctly returned as the top ranked result\\nunder the original model. Under the learned model, this number jumps to 78%.\\n\\n5 Conclusions and Future Work\\nIn most fonts, characters are quite ambiguous locally. An ?n? looks like a ?u?, looks like\\n?ii?, etc. This ambiguity is the major hurdle to the unsupervised learning of character\\ntemplates. Language models help, but the standard n-gram models provide insufficient\\nconstraints, giving posteriors for character sites too uninformative to get EM off the ground.\\n\\n\\fAggregate Precision/Recall Curve\\n\\nSelected Words, Top 100 Returned Lines\\n\\nPrecision\\n\\nest\\n(15,24)/24\\nnescio\\n( 1, 1)/ 1\\npostquam\\n( 0, 2)/ 2\\nquod\\n(14,14)/14\\nmoram\\n( 0, 2)/ 2\\nnon\\n( 8, 8)/ 8\\nquid\\n( 9, 9)/ 9\\n10 20 30 40 50 60 70 80 90100\\n\\n0.75\\n0.7\\n0.65\\n0.6\\n0.55\\n0.5\\n0.45\\n0.4\\n0.35\\n\\nOriginal Model\\nRefit Model\\n0.2\\n\\n0.4\\n0.6\\nRecall\\n\\n0.8\\n\\n1\\n\\nFigure 7: The figure on the left shows the those lines with the top 100 scores that actually\\ncontain the specified word. The first of each set of two rows (in red) is the results from\\nRound 1. The second (in green) is the results for Round 2. Almost all search words in our\\ncorpus show a significant improvement. The numbers to the right (x/y) mean that out of\\ny lines that actually contained the search word in our document, x of them made it into\\nthe top ten. On the right are average precision/recall curves for 21 high frequency words\\nunder the model with our original templates (Rnd 1) and after refitting with new extracted\\ntemplates (Rnd 2). Extracting new templates vastly improves our search quality\\nAn entire word is much different. Given a dictionary, we expect many word images to have\\na single likely transcription even if many characters are locally ambiguous. We show that\\nwe can identify these high confidence regions even with a poorly tuned character model. By\\nextracting new templates only from these regions of the document, we overcome the noise\\nproblem and significantly improve our character models. We demonstrate this improvement\\nfor the task of search where the refitted models have drastically better search responses than\\nwith the original. Our method is indifferent to the form of the actual character emission\\nmodel. There is a rich literature in character prediction from isolated image windows, and\\nwe expect that incorporating more powerful character models should provide even greater\\nreturns and help us in learning less regular scripts.\\nFinding high confidence regions to extract good training examples is a broadly applicable concept. We believe this work should extend to other problems, most notably speech\\nrecognition. Looked at more abstractly, our use of language model in this work is actually encoding spatial constraints. The probability of a character given an image window\\ndepends not only on the identify of surrounding characters but also on their spatial configuration. Integrating context into recognition problems is an area of intense research in\\nthe computer vision community, and we are investigating extending the idea of confidence\\nregions to more general object recognition problems.\\n\\nReferences\\n[1] Early Manuscripts at Oxford University. Bodleian library ms. auct. f. 2.13. http://image.ox.ac.uk/.\\n[2] J. Edwards, Y.W. Teh, D. Forsyth, R. Bock, M. Maire, and G. Vesom. Making latin manuscripts\\nsearchable using ghmm?s. In NIPS 17, pages 385?392. 2005.\\n[3] G. Kopec and M. Lomelin. Document-specific character template estimation. In Proceedings,\\nDocument Image Recognition III, SPIE, 1996.\\n[4] V. Lavrenko, T. Rath, and R. Manmatha. Holistic word recognition for handwritten historical\\ndocuments. In dial, pages 278?287, 2004.\\n[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE, 86(11):2278?2324, 1998.\\n[6] M. Mohri, F. Pereira, and M. Riley. Weighted finite state transducers in speech recognition. ISCA\\nITRW Automatic Speech Recognition, pages 97?106, 2000.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAspZcWIpFpB","executionInfo":{"status":"ok","timestamp":1738205838548,"user_tz":-330,"elapsed":30,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"69241faa-f5d2-4445-b956-6c76086d9600"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7241, 7)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df = df.iloc[:5000,:]"],"metadata":{"id":"aHzPrzEVpVcQ","executionInfo":{"status":"ok","timestamp":1738205838549,"user_tz":-330,"elapsed":29,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxb5B2xDpcj1","executionInfo":{"status":"ok","timestamp":1738205838549,"user_tz":-330,"elapsed":29,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"c1ac3485-503b-4786-d3e9-e0f6741ccd6a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 7)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AG5BlTXapd4B","executionInfo":{"status":"ok","timestamp":1738205838549,"user_tz":-330,"elapsed":26,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"8a81975e-8dd1-4d70-8eab-9977e98b120d"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract',\n","       'paper_text'],\n","      dtype='object')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["df['paper_text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"x9Gfj-fpppot","executionInfo":{"status":"ok","timestamp":1738205838549,"user_tz":-330,"elapsed":24,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"27b58985-0af5-4ba4-d8b0-d42334fe1cd2"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n","1       683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n","2       394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n","3       Bayesian Query Construction for Neural\\nNetwor...\n","4       Neural Network Ensembles, Cross\\nValidation, a...\n","                              ...                        \n","4995    Low-Rank Time-Frequency Synthesis\\n\\nMatthieu ...\n","4996    A State-Space Model for Decoding Auditory\\nAtt...\n","4997    Efficient Structured Matrix Rank Minimization\\...\n","4998    Ef?cient Minimax Signal Detection on Graphs\\n\\...\n","4999    Signal Aggregate Constraints in Additive Facto...\n","Name: paper_text, Length: 5000, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paper_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>Low-Rank Time-Frequency Synthesis\\n\\nMatthieu ...</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>A State-Space Model for Decoding Auditory\\nAtt...</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>Efficient Structured Matrix Rank Minimization\\...</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>Ef?cient Minimax Signal Detection on Graphs\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>Signal Aggregate Constraints in Additive Facto...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 1 columns</p>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["df['paper_text'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"IxQHdprgpsRl","executionInfo":{"status":"ok","timestamp":1738205838550,"user_tz":-330,"elapsed":22,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"c82591f0-0815-495e-94c1-8667ec5db631"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisashi Suzuki and Suguru Arimoto\\nOsaka University, Toyonaka, Osaka 560, Japan\\nABSTRACT\\nAn efficient method of self-organizing associative databases is proposed together with\\napplications to robot eyesight systems. The proposed databases can associate any input\\nwith some output. In the first half part of discussion, an algorithm of self-organization is\\nproposed. From an aspect of hardware, it produces a new style of neural network. In the\\nlatter half part, an applicability to handwritten letter recognition and that to an autonomous\\nmobile robot system are demonstrated.\\n\\nINTRODUCTION\\nLet a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another\\nfinite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly\\nfrom X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some\\nestimate j : X -+ Y of f to make small, the estimation error in some measure.\\nUsually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance\\nis incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,\\nlet us discuss for a while on some types of learning machines. And, let us advance the\\nunderstanding of the self-organization of associative database .\\n. Parameter Type\\nAn ordinary type of learning machine assumes an equation relating x\\'s and y\\'s with\\nparameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a\\nset F of candidates of\\n(F is some subset of mappings from X to Y.) And, it computes\\nvalues of the parameters based on the observed samples. We call such type a parameter\\ntype.\\nFor a learning machine defined well, if F 3 f, j approaches f as the number of samples\\nincreases. In the alternative case, however, some estimation error remains eternally. Thus,\\na problem of designing a learning machine returns to find out a proper structure of f in this\\nsense.\\nOn the other hand, the assumed structure of f is demanded to be as compact as possible\\nto achieve a fast learning. In other words, the number of parameters should be small. Since,\\nif the parameters are few, some j can be uniquely determined even though the observed\\nsamples are few. However, this demand of being proper contradicts to that of being compact.\\nConsequently, in the parameter type, the better the compactness of the assumed structure\\nthat is proper, the better the learning machine. This is the most elementary conception\\nwhen we design learning machines .\\n\\n1.\\n\\n. Universality and Ordinary Neural Networks\\nNow suppose that a sufficient knowledge on f is given though J itself is unknown. In\\nthis case, it is comparatively easy to find out proper and compact structures of J. In the\\nalternative case, however, it is sometimes difficult. A possible solution is to give up the\\ncompactness and assume an almighty structure that can cover various 1\\'s. A combination\\nof some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2\\nare its approximations obtained by truncating finitely the dimension for implementation.\\n\\n? American Institute of Physics 1988\\n\\n\\x0c768\\nA main topic in designing neural networks is to establish such desirable structures of 1.\\nThis work includes developing practical procedures that compute values of coefficients from\\nthe observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel\\nfor speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.\\nNevertheless, in neural networks, there always exists a danger of some error remaining\\neternally in estimating /. Precisely speaking, suppose that a combination of the bases of a\\nfinite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or\\n1 is located near F. In such case, the estimation error is none or negligible. However, if 1\\nis distant from F, the estimation error never becomes negligible. Indeed, many researches\\nreport that the following situation appears when 1 is too complex. Once the estimation\\nerror converges to some value (> 0) as the number of samples increases, it decreases hardly\\neven though the dimension is heighten. This property sometimes is a considerable defect of\\nneural networks .\\n. Recursi ve Type\\nThe recursive type is founded on another methodology of learning that should be as\\nfollows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates\\nof I equals to the set of all mappings from X to Y. After observing the first sample\\n(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing\\nthe second sample (X2\\' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\\nI(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation\\nof samples proceeds. The after observing i-samples, which we write\\nis one of the most\\nlikelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the\\nrecursive type guarantees surely that j approaches to 1 as the number of samples increases.\\nThe recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for\\nsome x\\'s correlated to the sample. Hence, this type has an architecture composed of a rule\\nfor rewriting and a free memory space. Such architecture forms naturally a kind of database\\nthat builds up management systems of data in a self-organizing way. However, this database\\ndiffers from ordinary ones in the following sense. It does not only record the samples already\\nobserved, but computes some estimation of l(x) for any x E X. We call such database an\\nassociative database.\\nThe first subject in constructing associative databases is how we establish the rule for\\nrewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty\\nmeans a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0\\nwhenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is\\ndefinable with, for example, a collection of rules written in forms of \"if? .. then?? .. \"\\nThe dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though\\nthe knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,\\ncontrarily to neural networks, it is possible to accelerate the speed of learning by establishing\\nd well. Especially, we can easily find out simple d\\'s for those l\\'s which process analogically\\ninformation like a human. (See the applications in this paper.) And, for such /\\'s, the\\nrecursive type shows strongly its effectiveness.\\nWe denote a sequence of observed samples by (Xl, Yd, (X2\\' Y2),???. One of the simplest\\nconstructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.\\n\\ni\\n\\ni\"\\n\\nI,\\n\\nAlgorithm 1. At the initial stage, let So be the empty set. For every i =\\n1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and\\n\\nd(x, x*) =\\n\\nmin\\n(%,y)ES.-t\\n\\nd(x, x) .\\n\\nFurthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"\\n\\n(1)\\n\\ny,n.\\n\\n\\x0c769\\n\\nAnother version improved to economize the memory is as follows.\\n\\nAlgorithm 2, At the initial stage, let So be composed of an arbitrary element\\nin X x Y. For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such\\nthat (x?, y.) E Si-l and\\nd(x, x?) =\\n\\nmin\\n\\nd(x, x) .\\n\\n(i,i)ES.-l\\n\\nFurthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to\\nproduce Si, i.e., Si = Si-l U {(Xi, Yi)}\\'\\nIn either construction, ii approaches to f as i increases. However, the computation time\\ngrows proportionally to the size of Si. The second subject in constructing associative\\ndatabases is what addressing rule we should employ to economize the computation time. In\\nthe subsequent chapters, a construction of associative database for this purpose is proposed.\\nIt manages data in a form of binary tree.\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nGiven a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative\\ndatabase is as follows.\\n\\nAlgorithm 3,\\'\\n\\nStep I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are\\nvariables assigned for respective nodes to memorize data.. Furthermore, let t = 1.\\nStep 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat\\nthe following until n arrives at some terminal node, i.e., leaf.\\nNotations nand\\nd(xt, x[n)), let n\\n\\nn mean the descendant nodes of n.\\n=n. Otherwise, let n =n.\\n\\nIf d(x\" r[n)) ~\\n\\nStep 3: Display yIn] as the related information. Next, put y, in. If yIn] = y\" back\\nto step 2. Otherwise, first establish new descendant nodes n and n. Secondly,\\nlet\\n\\n(x[n], yIn))\\n(x[n], yIn))\\n\\n(x[n], yIn)),\\n(Xt, y,).\\n\\n(2)\\n(3)\\n\\nFinally, back to step 2. Here, the loop of step 2-3 can be stopped at any time\\nand also can be continued.\\nNow, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements\\nbeing randomly connected by this algorithm.\\n\\nLETTER RECOGNITION\\nRecen tly, the vertical slitting method for recognizing typographic English letters3 , the\\nelastic matching method for recognizing hand written discrete English letters4 , the global\\ntraining and fuzzy logic search method for recognizing Chinese characters written in square\\nstyleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.\\n\\n\\x0c770\\n\\n9 /wn\"\\n\\nNOV\\n\\n~ ~ ~ -xk :La.t\\n\\n~~ ~ ~~~\\n\\ndw1lo\\'\\n\\n~~~~~of~~\\n\\n~~~ 4,-?~~4Fig. 1. Source document.\\n2~~---------------\\'\\n\\nlOO~---------------\\'\\n\\nH\\n\\no\\n\\no\\nFig. 2. Windowing.\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNumber of samples\\n\\no\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNUAlber of sampl es\\n\\nFig. 3. An experiment result.\\n\\nAn image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the\\nsequence of letters while shifting the window. That is, the recognizer scans a word in a\\nslant direction. And, it places the window so that its left vicinity may be on the first black\\npoint detected. Then, the window catches a letter and some part of the succeeding letter.\\nIf recognition of the head letter is performed, its end position, namely, the boundary line\\nbetween two letters becomes known. Hence, by starting the scanning from this boundary\\nand repeating the above operations, the recognizer accomplishes recursively the task. Thus\\nthe major problem comes to identifying the head letter in the window.\\nConsidering it, we define the following.\\n? Regard window images as x\\'s, and define X accordingly.\\n? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on\\nwindow image X. Project each B onto window image x. Then, measure the Euclidean\\ndistance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be\\nthe summation of 6\\'s for all black points B\\'s on x divided by the number of B\\'s.\\n? Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y\\naccordingly.\\nAn operator teaches the recognizer in interaction the relation between window image and\\nreading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the\\noperator teaches a correct reading via the console. Moreover, if the boundary position is\\nincorrect, he teaches a correct position via the mouse.\\nFig. 1 shows partially a document image used in this experiment. Fig. 3 shows the\\nchange of the number of nodes and that of the recognition rate defined as the relative\\nfrequency of correct answers in the past 1000 trials. Speciiications of the window are height\\n= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree\\nwere distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.\\nExperimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at\\na rare case. However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable\\nbecause of excessive lluctuation in writing. If the consistency of the x, y-relation is not\\nassured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to\\nstop the learning when the recognition rate attains some upper limit. To improve further\\nthe recognition rate, we must consider the spelling of words. It is one of future subjects.\\n\\n\\x0c771\\n\\nOBSTACLE AVOIDING MOVEMENT\\nVarious systems of camera type autonomous mobile robot are reported flourishingly6-1O.\\nThe system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as\\na cost minimization problem under some cost criterion established artificially. Contrarily,\\nthe self-organization of associative database reproduces faithfully the cost criterion of an\\noperator. Therefore, motion of the robot after learning becomes very natural.\\nNow, the length, width and height of the robot are all about O.7m, and the weight is\\nabout 30kg. The visual angle of camera is about 55deg. The robot has the following three\\nfactors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less\\nthan 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building\\nwhich the authors\\' laboratories exist in (Fig. 5). Because of an experimental intention, we\\narrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at\\nrandom. We let the robot take an image through the camera, recall a similar image, and\\ntrace the route preliminarily recorded on it. For this purpose, we define the following.\\n? Let the camera face 28deg downward to take an image, and process it through a low\\npass filter. Scanning vertically the filtered image from the bottom to the top, search\\nthe first point C where the luminance changes excessively. Then, su bstitu te all points\\nfrom the bottom to C for white, and all points from C to the top for black (Fig. 6).\\n(If no obstacle exists just in front of the robot, the white area shows the \\'\\'free\\'\\' area\\nwhere the robot can move around.) Regard binary 32 x 32dot images processed thus\\nas x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or\\nimage between x and X.\\n? Regard as y\\'s the images obtained by drawing routes on images x\\'s, and define Y\\naccordingly.\\nThe robot superimposes, on the current camera image x, the route recalled for x, and\\ninquires the operator instructions. The operator judges subjectively whether the suggested\\nroute is appropriate or not. In the negative answer, he draws a desirable route on x with the\\nmouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence\\nof (x, y) reflecting the cost criterion of the operator.\\n\\n.::l\" !\\n-\\n\\nIibUBe\\n\\n_. -\\n\\n22\\n\\n11\\n\\nRoan\\n\\n12\\n\\n{-\\n\\n13\\n\\nStationary uni t\\n\\nFig. 4. Configuration of\\nautonomous mobile robot system.\\n\\n~\\n\\nI\\n\\n,\\n\\n23\\n\\n24\\n\\nNorth\\n14\\n\\nrmbi Ie unit (robot)\\n\\n-\\n\\nRoan\\n\\ny\\n\\nt\\n\\nFig. 5. Experimental\\nenvironment.\\n\\n\\x0c772\\n\\nWall\\n\\nCamera image\\n\\nPreprocessing\\n\\nA\\n\\n::: !fa\\n\\n?\\n\\nPreprocessing\\n\\n0\\n\\nO\\n\\nCourse\\nsuggest ion\\n\\n??\\n\\n..\\n\\nSearch\\n\\nA\\n\\nFig. 6. Processing for\\nobstacle avoiding movement.\\n\\nx\\n\\nFig. 1. Processing for\\nposition identification.\\nWe define the satisfaction rate by the relative frequency of acceptable suggestions of\\nroute in the past 100 trials. In a typical experiment, the change of satisfaction rate showed\\na similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that\\nthe rest 5% does not mean directly the percentage of collision. (In practice, we prevent the\\ncollision by adopting some supplementary measure.) At time 800, the number of nodes was\\n145, and the levels of tree were distributed in 6-17.\\nThe proposed method reflects delicately various characters of operator. For example, a\\nrobot trained by an operator 0 moves slowly with enough space against obstacles while one\\ntrained by another operator 0\\' brushes quickly against obstacles. This fact gives us a hint\\non a method of printing \"characters\" into machines.\\nPOSITION IDENTIFICATION\\nThe robot can identify its position by recalling a similar landscape with the position data\\nto a camera image. For this purpose, in principle, it suffices to regard camera images and\\nposition data as x\\'s and y\\'s, respectively. However, the memory capacity is finite in actual\\ncompu ters. Hence, we cannot but compress the camera images at a slight loss of information.\\nSuch compression is admittable as long as the precision of position identification is in an\\nacceptable area. Thus, the major problem comes to find out some suitable compression\\nmethod.\\nIn the experimental environment (Fig. 5), juts are on the passageway at intervals of\\n3.6m, and each section between adjacent juts has at most one door. The robot identifies\\nroughly from a surrounding landscape which section itself places in. And, it uses temporarily\\na triangular surveying technique if an exact measure is necessary. To realize the former task,\\nwe define the following .\\n? Turn the camera to take a panorama image of 360deg. Scanning horizontally the\\ncenter line, substitute the points where the luminance excessively changes for black\\nand the other points for white (Fig. 1). Regard binary 360dot line images processed\\nthus as x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, project each black point A on x onto x. And, measure the\\nEuclidean distance 6 between A and a black point A on x being the closest to A. Let\\nthe summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.\\nDenoting the numbers of A\\'s and A\\'s respectively by nand n, define\\n\\n\\x0c773\\n\\nd(x, x) =\\n\\n~(~\\n+ ~).\\n2 n\\nn\\n\\n(4)\\n\\n? Regard positive integers labeled on sections as y\\'s (cf. Fig. 5), and define Y accordingly.\\nIn the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area\\nand learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic\\nexcepting the periodic reset of counter, namely, it is a kind of learning without teacher.\\nWe define the identification rate by the relative frequency of correct recalls of position\\ndata in the past 100 trials. In a typical example, it converged to about 83% around time\\n400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no\\npro blem arises in practical use. In order to improve the identification rate, the compression\\nratio of camera images must be loosened. Such possibility depends on improvement of the\\nhardware in the future.\\nFig. 8 shows an example of actual motion of the robot based on the database for obstacle\\navoiding movement and that for position identification. This example corresponds to a case\\nof moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.\\n\\n,~. .~ (\\n;~\"i..\\n~\\n\\n\"\\n\\n\"\\n\\n.\\n\\n..I\\n\\nI\\n\\n?\\n?\\n\\n\"\\n\\nI\\'\\n.\\n\\'.1\\nt\\n\\n;\\n\\ni\\n\\n-:\\n, . . , \\'II\\n\\nFig. 8. Actual motion of the robot.\\n\\n\\x0c774\\n\\nCONCLUSION\\nA method of self-organizing associative databases was proposed with the application to\\nrobot eyesight systems. The machine decomposes a global structure unknown into a set of\\nlocal structures known and learns universally any input-output response. This framework\\nof problem implies a wide application area other than the examples shown in this paper.\\nA defect of the algorithm 3 of self-organization is that the tree is balanced well only\\nfor a subclass of structures of f. A subject imposed us is to widen the class. A probable\\nsolution is to abolish the addressing rule depending directly on values of d and, instead, to\\nestablish another rule depending on the distribution function of values of d. It is now under\\ninvestigation.\\n\\nREFERENCES\\n1. Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'\\n\\nScience 233 (1986), pp. 625-633.\\n2. Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp. 533-536.\\n\\n3. Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word\\nRecognition,\" IEEE Expert, Fall (1986), pp. 63-70.\\n4. Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.\\n\\n5. Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and\\nGlobal Training,\" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1\\n(1987) pp. 91-102.\\n6. Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile\\nRobots,\" 4th Int. Symp. of Robotics Research (1987), pp. 267-276.\\n7. Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987\\nIEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.\\n8. Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\"\\nIEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.\\n9. Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.\\n10. Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\"\\n1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.\\n\\n\\x0c'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["len(df['paper_text'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfVRNBxlp8rr","executionInfo":{"status":"ok","timestamp":1738205838550,"user_tz":-330,"elapsed":20,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"225bb494-3462-4445-d00a-c7ab6a0328d7"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21643"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('stopwords')\n","import nltk\n","nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4yyo6HurO0C","executionInfo":{"status":"ok","timestamp":1738205840590,"user_tz":-330,"elapsed":2057,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"1ccad696-8de2-4149-9740-c7ffedbe200b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["stopwords = stopwords.words('english')"],"metadata":{"id":"kHyby2EMrz0M","executionInfo":{"status":"ok","timestamp":1738205840590,"user_tz":-330,"elapsed":26,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"RPfuuKbusv5a","executionInfo":{"status":"ok","timestamp":1738205840591,"user_tz":-330,"elapsed":26,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"94354735-32da-469a-c6aa-fdda0ead9021"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["\n","new_words = ['fig','figure','image','sample','using','show','result','large','also','one','two','three','five','six','seven','eight','nine','ten']"],"metadata":{"id":"83NHfvoys3Wx","executionInfo":{"status":"ok","timestamp":1738205840591,"user_tz":-330,"elapsed":23,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["stopwords = stopwords + new_words"],"metadata":{"id":"X9uL2g5-tpsx","executionInfo":{"status":"ok","timestamp":1738205840591,"user_tz":-330,"elapsed":22,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bdMqzx8JtutR","executionInfo":{"status":"ok","timestamp":1738205840592,"user_tz":-330,"elapsed":23,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"1b23b961-7d31-4f24-ef8f-f5cecc181d02"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\",\n"," 'fig',\n"," 'figure',\n"," 'image',\n"," 'sample',\n"," 'using',\n"," 'show',\n"," 'result',\n"," 'large',\n"," 'also',\n"," 'one',\n"," 'two',\n"," 'three',\n"," 'five',\n"," 'six',\n"," 'seven',\n"," 'eight',\n"," 'nine',\n"," 'ten']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["def preprocessing_text(txt):\n","  txt = txt.lower()\n","  txt = re.sub(r'<.*?>',' ',txt)\n","  txt = re.sub(r'[^a-zA-Z]',' ',txt)\n","  txt = nltk.word_tokenize(txt)\n","  txt = [word for word in txt if word not in stopwords]\n","  txt = [word for word in txt if len(word)>3]\n","  stemming = PorterStemmer()\n","  txt = [stemming.stem(word) for word in txt]\n","  txt = ' '.join(txt)\n","\n","  return txt\n"],"metadata":{"id":"T4HidKrDtwOK","executionInfo":{"status":"ok","timestamp":1738205840592,"user_tz":-330,"elapsed":16,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["preprocessing_text('SIDDHI <hg>vndjkfhbdjkb<fjkg> kjkjsdhcush878967&**&& naming ' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"herAj6Kkuu50","executionInfo":{"status":"ok","timestamp":1738205840592,"user_tz":-330,"elapsed":14,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"d0b27517-fda0-4504-8cf5-f44524a0b846"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'siddhi vndjkfhbdjkb kjkjsdhcush name'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["docs = df['paper_text'].apply(lambda x:preprocessing_text(x))"],"metadata":{"id":"76eQmk0xu0M7","executionInfo":{"status":"ok","timestamp":1738206176308,"user_tz":-330,"elapsed":335728,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"id":"wwUFBW1b9ESX","colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"status":"ok","timestamp":1738206282792,"user_tz":-330,"elapsed":425,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"604e4506-aeff-448f-f855-12df6dfed108"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       self organ associ databas applic hisashi suzuk...\n","1       mean field theori layer visual cortex applic a...\n","2       store covari associ long term potenti depress ...\n","3       bayesian queri construct neural network model ...\n","4       neural network ensembl cross valid activ learn...\n","                              ...                        \n","4995    rank time frequenc synthesi matthieu kowalski ...\n","4996    state space model decod auditori attent modul ...\n","4997    effici structur matrix rank minim adam wanli y...\n","4998    cient minimax signal detect graph jing qian di...\n","4999    signal aggreg constraint addit factori hmm app...\n","Name: paper_text, Length: 5000, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paper_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>self organ associ databas applic hisashi suzuk...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mean field theori layer visual cortex applic a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>store covari associ long term potenti depress ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bayesian queri construct neural network model ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>neural network ensembl cross valid activ learn...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>rank time frequenc synthesi matthieu kowalski ...</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>state space model decod auditori attent modul ...</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>effici structur matrix rank minim adam wanli y...</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>cient minimax signal detect graph jing qian di...</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>signal aggreg constraint addit factori hmm app...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 1 columns</p>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv= CountVectorizer(max_df=0.95, max_features=5000, ngram_range=(1,3))\n","word_count_vector = cv.fit_transform(docs)"],"metadata":{"id":"LyCLGkXQKv_g","executionInfo":{"status":"ok","timestamp":1738206420254,"user_tz":-330,"elapsed":126443,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n","tfidf_transformer = tfidf_transformer.fit(word_count_vector)"],"metadata":{"id":"t9fVCDcrLdnl","executionInfo":{"status":"ok","timestamp":1738206547423,"user_tz":-330,"elapsed":385,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["feature_names = cv.get_feature_names_out()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yeGjv-CBNtdz","executionInfo":{"status":"ok","timestamp":1738207065947,"user_tz":-330,"elapsed":367,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"0f7f540c-45b7-4895-f561-e8435d47c180"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['aaai', 'abil', 'abl', ..., 'zhang', 'zhou', 'zisserman'],\n","      dtype=object)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def get_keywords(idx,docs,topN=10):\n","  docs_words_count = tfidf_transformer.transform(cv.transform([docs[idx]]))\n","\n","  docs_words_count = docs_words_count.tocoo()\n","  tuple = zip(docs_words_count.col, docs_words_count.data)\n","  sorted_items = sorted(tuple, key = lambda x:(x[1],x[0]),reverse=True)\n","\n","  sorted_items = sorted_items[:topN]\n","\n","  score_vals = []\n","  feature_vals = []\n","\n","  for idx, score in sorted_items:\n","    score_vals.append(round(score,3))\n","    feature_vals.append(feature_names[idx])\n","\n","\n","  result = {}\n","  for idx in range(len(feature_vals)):\n","    result[feature_vals[idx]] = score_vals[idx]\n","\n","  return result\n","\n","\n","\n","\n","\n","def print_keyword(idx,keywords,df):\n","  print(\"\\n ================title==============\")\n","  print(df['title'][idx])\n","  print(\"\\n ================abstract==============\")\n","  print(df['abstract'][idx])\n","  print(\"\\n ================keywords==============\")\n","  for k in keywords:\n","    print(k, keywords[k])\n","\n","idx = 4995\n","keywords = get_keywords(idx,docs)\n","print_keyword(idx,keywords,df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDdwD-9dPrH7","executionInfo":{"status":"ok","timestamp":1738207774331,"user_tz":-330,"elapsed":391,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"2f3aac74-fe28-4207-80dc-2de5dde111fa"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," ================title==============\n","Low-Rank Time-Frequency Synthesis\n","\n"," ================abstract==============\n","Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram -- the (power) magnitude of the short-time Fourier transform (STFT) -- has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.\n","\n"," ================keywords==============\n","transient 0.3\n","signal 0.269\n","speech 0.256\n","synthesi 0.222\n","time frequenc 0.214\n","coeffici 0.21\n","frequenc 0.209\n","audio 0.171\n","resolut 0.166\n","rank 0.132\n"]}]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684},"collapsed":true,"id":"wDd5E00nMER4","executionInfo":{"status":"ok","timestamp":1738206639869,"user_tz":-330,"elapsed":1785,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}},"outputId":"174c9091-0ee1-410d-c6ad-b85e6f9c4f97"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id  year                                              title  \\\n","0        1  1987  Self-Organization of Associative Database and ...   \n","1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n","2      100  1988  Storing Covariance by the Associative Long-Ter...   \n","3     1000  1994  Bayesian Query Construction for Neural Network...   \n","4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n","...    ...   ...                                                ...   \n","4995  5522  2014                  Low-Rank Time-Frequency Synthesis   \n","4996  5523  2014  A State-Space Model for Decoding Auditory Atte...   \n","4997  5524  2014      Efficient Structured Matrix Rank Minimization   \n","4998  5525  2014       Efficient Minimax Signal Detection on Graphs   \n","4999  5526  2014  Signal Aggregate Constraints in Additive Facto...   \n","\n","     event_type                                           pdf_name  \\\n","0           NaN  1-self-organization-of-associative-database-an...   \n","1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n","2           NaN  100-storing-covariance-by-the-associative-long...   \n","3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n","4           NaN  1001-neural-network-ensembles-cross-validation...   \n","...         ...                                                ...   \n","4995     Poster         5522-low-rank-time-frequency-synthesis.pdf   \n","4996     Poster  5523-a-state-space-model-for-decoding-auditory...   \n","4997     Poster  5524-efficient-structured-matrix-rank-minimiza...   \n","4998     Poster  5525-efficient-minimax-signal-detection-on-gra...   \n","4999     Poster  5526-signal-aggregate-constraints-in-additive-...   \n","\n","                                               abstract  \\\n","0                                      Abstract Missing   \n","1                                      Abstract Missing   \n","2                                      Abstract Missing   \n","3                                      Abstract Missing   \n","4                                      Abstract Missing   \n","...                                                 ...   \n","4995  Many single-channel signal decomposition techn...   \n","4996  Humans are able to segregate auditory objects ...   \n","4997  We study the problem of finding structured low...   \n","4998  Several problems such as network intrusion, co...   \n","4999  Blind source separation problems are difficult...   \n","\n","                                             paper_text  \n","0     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n","1     683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n","2     394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n","3     Bayesian Query Construction for Neural\\nNetwor...  \n","4     Neural Network Ensembles, Cross\\nValidation, a...  \n","...                                                 ...  \n","4995  Low-Rank Time-Frequency Synthesis\\n\\nMatthieu ...  \n","4996  A State-Space Model for Decoding Auditory\\nAtt...  \n","4997  Efficient Structured Matrix Rank Minimization\\...  \n","4998  Ef?cient Minimax Signal Detection on Graphs\\n\\...  \n","4999  Signal Aggregate Constraints in Additive Facto...  \n","\n","[5000 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-5abecf6d-a071-4849-96d2-47591548f486\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year</th>\n","      <th>title</th>\n","      <th>event_type</th>\n","      <th>pdf_name</th>\n","      <th>abstract</th>\n","      <th>paper_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1987</td>\n","      <td>Self-Organization of Associative Database and ...</td>\n","      <td>NaN</td>\n","      <td>1-self-organization-of-associative-database-an...</td>\n","      <td>Abstract Missing</td>\n","      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10</td>\n","      <td>1987</td>\n","      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n","      <td>NaN</td>\n","      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n","      <td>Abstract Missing</td>\n","      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100</td>\n","      <td>1988</td>\n","      <td>Storing Covariance by the Associative Long-Ter...</td>\n","      <td>NaN</td>\n","      <td>100-storing-covariance-by-the-associative-long...</td>\n","      <td>Abstract Missing</td>\n","      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000</td>\n","      <td>1994</td>\n","      <td>Bayesian Query Construction for Neural Network...</td>\n","      <td>NaN</td>\n","      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n","      <td>Abstract Missing</td>\n","      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1001</td>\n","      <td>1994</td>\n","      <td>Neural Network Ensembles, Cross Validation, an...</td>\n","      <td>NaN</td>\n","      <td>1001-neural-network-ensembles-cross-validation...</td>\n","      <td>Abstract Missing</td>\n","      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>5522</td>\n","      <td>2014</td>\n","      <td>Low-Rank Time-Frequency Synthesis</td>\n","      <td>Poster</td>\n","      <td>5522-low-rank-time-frequency-synthesis.pdf</td>\n","      <td>Many single-channel signal decomposition techn...</td>\n","      <td>Low-Rank Time-Frequency Synthesis\\n\\nMatthieu ...</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>5523</td>\n","      <td>2014</td>\n","      <td>A State-Space Model for Decoding Auditory Atte...</td>\n","      <td>Poster</td>\n","      <td>5523-a-state-space-model-for-decoding-auditory...</td>\n","      <td>Humans are able to segregate auditory objects ...</td>\n","      <td>A State-Space Model for Decoding Auditory\\nAtt...</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>5524</td>\n","      <td>2014</td>\n","      <td>Efficient Structured Matrix Rank Minimization</td>\n","      <td>Poster</td>\n","      <td>5524-efficient-structured-matrix-rank-minimiza...</td>\n","      <td>We study the problem of finding structured low...</td>\n","      <td>Efficient Structured Matrix Rank Minimization\\...</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>5525</td>\n","      <td>2014</td>\n","      <td>Efficient Minimax Signal Detection on Graphs</td>\n","      <td>Poster</td>\n","      <td>5525-efficient-minimax-signal-detection-on-gra...</td>\n","      <td>Several problems such as network intrusion, co...</td>\n","      <td>Ef?cient Minimax Signal Detection on Graphs\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>5526</td>\n","      <td>2014</td>\n","      <td>Signal Aggregate Constraints in Additive Facto...</td>\n","      <td>Poster</td>\n","      <td>5526-signal-aggregate-constraints-in-additive-...</td>\n","      <td>Blind source separation problems are difficult...</td>\n","      <td>Signal Aggregate Constraints in Additive Facto...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5abecf6d-a071-4849-96d2-47591548f486')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5abecf6d-a071-4849-96d2-47591548f486 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5abecf6d-a071-4849-96d2-47591548f486');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0da72562-68f7-43ed-832b-108c52d8d95d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0da72562-68f7-43ed-832b-108c52d8d95d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0da72562-68f7-43ed-832b-108c52d8d95d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_3348334c-a379-4dce-8bdf-3fc3306ecff2\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_3348334c-a379-4dce-8bdf-3fc3306ecff2 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1522,\n        \"min\": 1,\n        \"max\": 5526,\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          2365,\n          3345,\n          3405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1987,\n        \"max\": 2014,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          2000,\n          2006,\n          1987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"Large Scale Online Learning\",\n          \"The Value of Labeled and Unlabeled Examples when the Model is Imperfect\",\n          \"Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"2365-large-scale-online-learning.pdf\",\n          \"3345-the-value-of-labeled-and-unlabeled-examples-when-the-model-is-imperfect.pdf\",\n          \"3405-hierarchical-semi-markov-conditional-random-fields-for-recursive-sequential-data.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2167,\n        \"samples\": [\n          \"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners.\",\n          \"We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.\",\n          \"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4998,\n        \"samples\": [\n          \"Visual gesture-based robot guidance\\nwith a modular neural system\\n\\nE. Littmann,\\n\\nA. Drees, and H. Ritter\\n\\nAbt. Neuroinformatik, Fak. f. Informatik\\nUniversitat Ulm, D-89069 Ulm, FRG\\nenno@neuro.informatik.uni-ulm.de\\n\\nAG Neuroinformatik, Techn. Fakultat\\nUniv. Bielefeld, D-33615 Bielefeld, FRG\\nandrea,helge@techfak.uni-bielefeld.de\\n\\nAbstract\\nWe report on the development of the modular neural system \\\"SEEEAGLE\\\" for the visual guidance of robot pick-and-place actions.\\nSeveral neural networks are integrated to a single system that visually recognizes human hand pointing gestures from stereo pairs\\nof color video images. The output of the hand recognition stage is\\nprocessed by a set of color-sensitive neural networks to determine\\nthe cartesian location of the target object that is referenced by the\\npointing gesture. Finally, this information is used to guide a robot\\nto grab the target object and put it at another location that can\\nbe specified by a second pointing gesture. The accuracy of the current system allows to identify the location of the referenced target\\nobject to an accuracy of 1 cm in a workspace area of 50x50 cm. In\\nour current environment, this is sufficient to pick and place arbitrarily positioned target objects within the workspace. The system\\nconsists of neural networks that perform the tasks of image segmentation, estimation of hand location, estimation of 3D-pointing\\ndirection, object recognition, and necessary coordinate transforms.\\nDrawing heavily on the use of learning algorithms, the functions of\\nall network modules were created from data examples only.\\n\\n1\\n\\nIntroduction\\n\\nThe rapidly developing technology in the fields of robotics and virtual reality requires the development of new and more powerful interfaces for configuration and\\ncontrol of such devices. These interfaces should be intuitive for the human advisor\\nand comfortable to use. Practical solutions so far require the human to wear a\\ndevice that can transfer the necessary information. One typical example is the data\\nglove [14, 12]. Clearly, in the long run solutions that are contactless will be much\\nmore desirable, and vision is one of the major modalities that appears especially\\nsuited for the realization of such solutions.\\nIn the present paper, we focus on a still restricted but very important task in robot\\ncontrol, the guidance of robot pick-and-place actions by unconstrained human pointing gestures in a realistic laboratory environment. The input of target locations by\\n\\n\\f904\\n\\nE. LITTMANN, A. DREES, H. RITTER\\n\\npointing gestures provides a powerful, very intuitive and comfortable functionality\\nfor a vision-based man-machine interface for guiding robots and extends previous\\nwork that focused on the detection of hand location or the discrimination of a small,\\ndiscrete number of hand gestures only [10, 1, 2, 8]. Besides two color cameras, no\\nspecial device is necessary to evaluate the gesture of the human operator.\\nA second goal of our approach is to investigate how to build a neural system for\\nsuch a complex task from several neural modules. The development of advanced\\nartificial neural systems challenges us with the task of finding architect.ures for the\\ncooperat.ion of multiple functional modules such that. part of the structure of the\\noverall system can be designed at a useful level of abstraction, but at the same t.ime\\nlearning can be used to create or fine-tune the functionality of parts of t.he system\\non the basis of suit.able training examples.\\nTo approach this goal requires to shift the focus from exploring t.he properties of\\nsingle networks to exploring the propert.ies of entire systems of neural networks.\\nThe work on \\\"mixtures of experts\\\" [3, 4] is one important contribution along these\\nlines. While this is a widely applicable and powerful approach, there clearly is\\na need to go beyond the exploration of strictly hierarchical systems and to gain\\nexperience with architectures t.hat admit more complex types of information flow\\nas required e.g. by the inclusion of feat.ures such as control of focal attention or\\nreent.rant processing branches. The need for such features arose very naturally in\\nthe context of the task described above, and in the following sect.ion we will report\\nour results wit.h a system architecture that is crucially based on the exploitation of\\nsuch elements.\\n\\n2\\n\\nSystem architecture\\n\\nOur system, described in fig. 1, is situated in a complex laboratory environment. A\\nrobot arm with manipulator is mounted at one side of a table with several objects\\nof different color placed on it. A human operator is positioned at the next side to\\nthe right of the robot. This scenery is watched by two cameras from the other two\\nsides from high above. The cameras yield a stereo color image of t.he scene (images\\n10). The operator points with one hand at one of the objects on the table. On the\\nbasis of the image information, the object is located and the robot grabs it. Then,\\nthe operator points at another location, where the robot releases the object. 1\\nThe syst.em consists of several hardware components: a PUMA 560 robot arm with\\nsix axes and a three-fingered manipulator 2; two single-chip PULNIX color cameras;\\ntwo ANDRox vision boards with software for data acquisition and processing; a\\nwork space consisting of a table with a black grid on a yellow surface. Robot and\\nperson refer to the same work space. Bot.h cameras must show both the human\\nhand and the table with the objects. Within this constraint, the position of the\\ncameras can be chosen freely as long as they yield significantly different views.\\nAn important prerequisite for the recognition of the pointing direction is the segmentation of the human hand from the background scenery. This task is solved by\\na LLM network (Sl) trained to yield a probability value for each image pixel to\\nbelong to the hand region. The training is based on t.he local color information.\\nThis procedure has been investigated in [7].\\nAn important feature of the chosen method is the great reliability and robustness\\nof both the classification performance and the localization accuracy of the searched\\nobject. Furthermore, the performance is quite constant over a wide range of image\\nresolutions. This allows a fast two-step procedure: First, the images are segmented\\nin low resolution (Sl: 11 -+ A1) and the hand position is extracted. Then, a small\\n1 In analogy to the sea eagle who watches its prey from high above, shoots down to grab\\nthe prey, and then flies to a safe place to feed, we nicknamed our system \\\"SEE-EAGLE\\\".\\n2Development by Prof. Pfeiffer, TV Munich\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n905\\n\\nFig. 1: System architecture. From two color camera images 10 we extract the hand position\\n(11 I> Sl I> A1 (pixel coord.) I> P1 I> cartesian hand coord.). In a subframe centered on\\nthe hand location (12) we determine the pointing direction (12 I> S2 I> A2 (pixel coord.) I>\\nG I> D I> pointing angles). Pointing direction and hand location define a cartesian target\\nlocation that is mapped to image coord. that define the centers of object subframes (10 I>\\nP2 I> 13). There we determine the target object (13 I> S3 I> A3) and map the pixel coord.\\nof its centers to world coord. (A3 I> P3 I> world target loc.). These coordinates are used\\nto guide the robot R to the target object.\\n\\n\\f906\\n\\nE. LITTMANN. A. DREES. H. RlTIER\\n\\nsubframe (12) around the estimated hand position is processed in high resolution\\nby another dedicated LLM network (S2: 12 - t A2). For details of the segmentation\\nprocess, refer to [6].\\nThe extraction of hand information by LLMs on the basis of Gabor masks has\\nalready been studied for hand posture [9] and orientation [5]. The method is based\\non a segmented image containing the hand only (A2). This image is filtered by 36\\nGabor masks that are arranged on a 3x3 grid with 4 directions per grid position\\nand centered on the hand. The filter kernels have a radius of 10 pixels, the distance\\nbetween the grid points is 20 pixels. The 36 filter responses (G) form the input\\nvector for a LLM network (D). Further details of the processing are reported in [6].\\nThe network yields the pointing direction of the hand (D: 12 - t G - t pointing\\ndirection). Together with the hand position which is computed by a parametrized\\nself-organizing map (\\\"PSOM\\\", see below and [11, 13]) (P1: Al - t cartesian hand\\nposition), a (cartesian) target location in the workspace can be calculated. This\\nlocation can be retransformed by the PSOM into pixel coordinates (P2: cartesian\\ntarget location - t target pixel coordinates). These coordinates define the center of\\nan \\\"attention region\\\" (13) that is searched for a set of predefined target objects.\\nThis object recognition is performed by a set of LLM color segmentation networks\\n(S3: 13 - t A3), each previously trained for one of the defined targets. A ranking\\nprocedure is used to determine the target object. The pixel coordinates ofthe target\\nin the segmented image are mapped by the PSOM to world coordinates (P3: A3 - t\\ncartesian target position). The robot R now moves to above these world coordinates,\\nmoves vertically down, grabs whatever is there, and moves upward again. Now, the\\nsystem evaluates a second pointing gesture that specifies the place where to place\\nthe object. This time, the world coordinates calculated on the basis of the pointing\\ndirection from network D and the cartesian hand location from PSOM PI serve\\ndirectly as target location for the robot.\\nFor our processing we must map corresponding pixels in the stereo images to cartesian world coordinates. For these transformations, training data was generated\\nwith aid of the robot on a precise sampling grid. We automatically extract the\\npixel coordinates of a LED at the tip of the robot manipulator from both images.\\nThe seven-dimensional feature vector serves as training input for an PSOM network [11]. By virtue of its capability to represent a transformation in a symmetric,\\n\\\"multiway\\\" -fashion, this offers the additional benefit that both the camera-to-world\\nmapping and its inverse can be obtained with a single network trained only once on\\na data set of 27 calibration positions of the robot. A detailed description for such\\na procedure can be found in [13].\\n\\n3\\n\\nResults\\n\\n3.1 System performance\\nThe accuracy of the current system allows to estimate the pointing target to an\\naccuracy of 1 ? 0.4 cm (average over N = 7 objects at randomly chosen locations\\nin the workspace) in a workspace area of 50x50 cm. In our current environment,\\nthis is sufficient to pick and place any of the seven defined target objects at any\\nlocation in the workspace. This accuracy can only be achieved if we use the object\\nrecognition module described in sec. 2. The output of the pointing direction module\\napproximates the target location with an considerably lower accuracy of 3.6? 1.6 cm.\\n3.2 Image segmentation\\nThe problem to evaluate these preprocessing steps has been discussed previously [7],\\nespecially the relation of specifity and sensitivity of the network for the given task.\\nAs the pointing recognition is based on a subframe centered on the hand center, it\\nis very sensitive to deviations from this center so that a good localization accuracy\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n907\\n\\nis even more important than the classification rate. The localization accuracy is\\ncalculated by measuring the pixel distance between the centers determined manually on the original image and as the center of mass in the image obtained after\\napplication of the neural network. Table 1 provides quantitative results.\\nOn the whole) the two-step cascade of LLM networks yields for 399 out of 400 images\\nan activity image precisely centered on the human hand. Only in one image) the\\nfirst LLM net missed the hand completely) due to a second hand in the image that\\ncould be clearly seen in this view. This image was excluded from further processing\\nand from the evaluation of the localization accuracy.\\n\\nPerson A\\nPerson H\\n\\nCamera A\\nPixel deviatIOn\\nNRMSE\\n0.8 ? 1.2\\n0.03 ? 0.06\\n1.3 ? 1.4\\n0.06 ? 0.11\\n\\nCamera B\\nPixel deViatIOn\\nNRMSE\\n0.8 ? 2.2\\n0.03 ? 0.09\\n2.2 ? 2.8\\n0.11 ? 0.21\\n\\nTable 1: Estimation error of the hand localization on the test set. Absolute error in pixels\\nand normalized error for both persons and both camera images.\\n\\n3.3 Recognition performance\\nOne major problem in recognizing human pointing gestures is the variability of these\\ngestures and their measurement for the acquisition of reliable training information.\\nDifferent persons follow different strategies where and how to point (fig. 2 (center)\\nand (right?. Therefore) we calculate this information indirectly. The person is\\ntold to point at a certain grid position with known world coordinates. From the\\ncamera images we extract the pixel positions of the hand center and map them to\\nworld coordinates using the PSOM net (PI in fig . 1). Given these coordinates the\\nangles of the intended pointing vector with the basis vectors of the world coordinate\\nsystem can be calculated trigonometrically. These angles form the target vector for\\nthe supervised training of a LLM network (D in fig. 1).\\n\\nAfter training) the output of the net is used to calculate the point where the pointing\\nvector intersects the table surface. For evaluation of the network performance we\\nmeasure the Euclidian distance between this point and the actual grid point where\\nthe person intended to point at. Fig. 3 (left) shows the mean euclidean error MEE\\nof the estimated target position as a function of the number of learning steps. The\\nerror on the training set can be considerably reduced) whereas on the test set the\\nimprovement stagnates after some 500 training steps. If we perform even more\\ntraining steps the performance might actually suffer from overfitting. The graph\\ncompares training and test results achieved on images obtained by two different\\nways of determining the hand center. The \\\"manual\\\" curves show the performance\\nthat can be achieved if the Gabor masks are manually centered on the hand. For\\nthe \\\"neuronal)) curves) the center of mass calculated in the fine-segmented and postprocessed subframe was used. This allows us to study the influence of the error of\\nthe segmentation and localization steps on the pointing recognition. This influence\\nis rather small. The MEE increases from 17 mm for the optimal method to 19 mm\\nfor the neural method) which is hardly visible in practice.\\nThe curves in fig. 3 (center) are obtained if we apply the networks to images of\\nanother person. The MEE is considerably larger but a detailed analysis' shows\\nthat part of this deviation is due to systematic differences in the pointing strategy\\nas shown in fig. 2 (right). Over a wide range, the number of nodes used for the\\nLLM network has only minor influence on the performance. While obviously the\\nperformance on the training set can be arbitrarily improved by spending more nodes,\\nthe differences in the MEE on the test set are negligible in a range of 5 to 15 nodes.\\nUsing more nodes is problematic as the training data consists of 50 examples only.\\nIf not indicated otherwise) we use LLM networks with 10 nodes. Further results)\\n\\n\\f908\\n\\nE. LIITMANN. A. DREES. H. RIITER\\n\\nFig. 2: The table grid points can be reconstructed according to the network output. The\\ntarget grid is dotted . Reconstruction of training grid (left) and test grid (center) for one\\nperson, and of the test grid for another person (right).\\nMEB on test oet of unknown perron\\n\\nMER\\n30\\n\\n20\\n\\ne?\\n\\nI~\\n\\n10\\n\\n~\\n\\n---- ~--.---\\n\\n~\\n~-\\n\\n0\\n\\nn\\n\\nm..... aI,trainneuronal, train manual, test -\\n\\n:l~\\n\\n100\\n\\n250\\n\\nsao\\n\\n1000 2SOO SOOO\\n\\ntrain.., itHabonr\\n\\ne\\n?\\n\\n70\\n68\\n66\\n64\\n62\\n60\\n58\\n56\\n\\n4\\n\\n-~.\\n\\n100\\n\\n:l~\\n\\nsao\\n\\n1000\\n\\n2SOO SOOO\\n\\nFig. 3: The euclidean error of\\nestimated target point calculated using the network output depends on the preprocessing (left), and the person\\n(center).\\n\\ntrairq IteratioN\\n\\ncomparing the pointing recognition based on only one of the camera images, indicate\\nthat the method works better if the camera takes a lateral view rather than a frontal\\nview . All evaluations were done for both persons. The performance was always very\\nsimilar.\\n\\n4\\n\\nDiscussion\\n\\nWhile we begin to understand many properties of neural networks at the single\\nnetwork level, our insight into principled ways of how to build neural systems is\\nstill rather limited . Due to the complexity of this task, theoretical progress is\\n(and probably will continue to be) very slow. What we can do in the mean time,\\nhowever, is to experiment with different design strategies for neural systems and\\ntry to \\\"evolve\\\" useful approaches by carefully chosen case studies.\\nThe current work is an effort along these lines. It is focused on a challenging,\\npractically important vision task with a number of generic features that are shared\\nwith vision tasks for which biological vision systems were evolved.\\nOne important issue is how to achieve robustness at the different processing levels\\nof the system. There are only very limited possibilities to study this issue in simulations, since practically nothing is known about the statistical properties of the\\nvarious sources of error that occur when dealing with real world data. Thus, a real\\nimplementation that works with actual data is practically the only way to study\\nthe robustness issue in a realistic fashion. Therefore, the demonstrated integration\\nof several functional modules that we had developed previously in more restricted\\nsettings [7, 6] was a non-trivial test of the feasability of having these functions\\ncooperate in a larger, modular system. It also gives confidence that the scaling\\nproblem can be dealt with successfully if we apply modular neural nets.\\nA related and equally important issue was the use of a processing strategy in which\\nearlier processing stages incrementally restrict the search space for the subsequent\\nstages. Thus, the responsibility for achieving the goal is not centralized in any single\\nmodule and subsequent modules have always the chance to compensate for limited\\nerrors of earlier stages. This appears to be a generally useful strategy for achieving\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n909\\n\\nrobustness and for cutting computational costs that is related to the use of \\\"focal\\nattention\\\" , which is clearly an important element of many biological vision systems.\\nA third important point is the extensive use of learning to build the essential constituent functions of the system from data examples. We are not yet able to train\\nthe assembled system as a whole. Instead, different modules are trained separately\\nand are integrated only later. Still, the experience gained with assembling a complex system via this \\\"engineering-type\\\" of approach will be extremely valuable for\\ngradually developing the capability of crafting larger functional building blocks by\\nlearning methods.\\nWe conclude that carefully designed experiments with modular neural systems that\\nare based on the use of real world data and that focus on similar tasks for which\\nalso biological neural systems were evolved can make a significant contribution in\\ntackling the challenge that lies ahead of us: to develop a reliable technology for the\\nconstruction of large-scale artificial neural systems that can solve complex tasks in\\nreal world environments.\\nAcknowledgements\\nWe want to thank Th. Wengerek (robot control), J. Walter (PSOM implementation), and\\nP. Ziemeck (image acquisition software). This work was supported by BMFT Grant No.\\nITN9104AO.\\n\\nReferences\\n[1] T. J. Darell and A. P. Pentland. Classifying hand gestures with a view-based distributed representation. In J . D. Cowan, G. Tesauro, and J. Alspector, editors, Neural\\nInformation Processing Systems 6, pages 945-952. Morgan Kaufman, 1994.\\n[2] J. Davis and M. Shah. Recognizing hand gestures. In J.-O. Eklundh, editor, Computer\\nVision - ECCV '94, volume 800 of Lecture Notes in Computer Science, pages 331340. Springer-Verlag, Berlin Heidelberg New York, 1994.\\n[3] R.A. Jacobs, M.1. Jordan, S.J. Nowlan, and G.E. Hinton. Adaptive mixtures of local\\nexperts. Neural Computation, 3:79- 87, 1991.\\n[4] M.1. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\\nNeural Computation, 6(2):181-214, 1994.\\n[5] F. Kummert, E. Littmann, A. Meyering, S. Posch, H. Ritter, and G. Sagerer. A\\nhybrid approach to signal interpretation using neural and semantic networks. In\\nMustererkennung 1993, pages 245-252. Springer, 1993.\\n[6] E. Littmann, A. Drees, and H. Ritter. Neural recognition of human pointing gestures\\nin real images. Submitted to Neural Processing Letters, 1996.\\n[7] E. Littmann and H. Ritter. Neural and statistical methods for adaptive color segmentation - a comparison. In G. Sagerer, S. Posch, and F. Kummert, editors,\\nMustererkennung 1995, pages 84-93. Springer-Verlag, Heidelberg, 1995.\\n[8] C. Maggioni. A novel device for using the hand as a human-computer interface. In\\nProceedings HC1'93 - Human Control Interface, Loughborough, Great Britain, 1993.\\n[9] A. Meyering and H. Ritter. Learning 3D shape perception with local linear maps. In\\nProc. of the lJCNN, volume IV, pages 432-436, Baltimore, MD, 1992.\\n[10] Steven J. Nowlan and John C. Platt. A convolutional neural network hand tracker.\\nIn Neural Information Processing Systems 7. Morgan Kaufman Publishers, 1995.\\n[11] H. Ritter. Parametrized self-organizing maps for vision learning tasks. In P. Morasso,\\neditor, ICANN '94. Springer-Verlag, Berlin Heidelberg New York, 1994.\\n[12] K. Viiiina.nen and K. Bohm. Gesture driven interaction as a human factor in virtual\\nenvironments - an approach with neural networks. In R. Earnshaw, M. Gigante, and\\nH. Jones, editors, Virtual reality systems, pages 93-106. Academic Press, 1993.\\n[13] J. Walter and H. Ritter. Rapid learning with parametrized self-organizing maps.\\nNeural Computing, 1995. Submitted.\\n[14] T. G. Zimmermann, J. Lanier, C. Blanchard, S. Bryson, and Y. Harvill. A hand\\ngesture interface device. In Proc. CHI+GI, pages 189-192, 1987.\\n\\n\\f\",\n          \"Iterative Non-linear Dimensionality Reduction by\\nManifold Sculpting\\n\\nMike Gashler, Dan Ventura, and Tony Martinez ?\\nBrigham Young University\\nProvo, UT 84604\\n\\nAbstract\\nMany algorithms have been recently developed for reducing dimensionality by\\nprojecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose significant precision in this transformation. Manifold Sculpting\\nis a new algorithm that iteratively reduces dimensionality by simulating surface\\ntension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both\\ngenerated and natural data-sets. Manifold Sculpting is also able to benefit from\\nboth prior dimensionality reduction efforts.\\n\\n1\\n\\nIntroduction\\n\\nDimensionality reduction is a two-step process: 1) Transform the data so that more information\\nwill survive the projection, and 2) project the data into fewer dimensions. The more relationships\\nbetween data points that the transformation step is required to preserve, the less flexibility it will have\\nto position the points in a manner that will cause information to survive the projection step. Due\\nto this inverse relationship, dimensionality reduction algorithms must seek a balance that preserves\\ninformation in the transformation without losing it in the projection. The key to finding the right\\nbalance is to identify where the majority of the information lies.\\nNonlinear dimensionality reduction (NLDR) algorithms seek this balance by assuming that the relationships between neighboring points contain more informational content than the relationships\\nbetween distant points. Although non-linear transformations have more potential than do linear\\ntransformations to lose information in the structure of the data, they also have more potential to\\nposition the data to cause more information to survive the projection. In this process, NLDR algorithms expose patterns and structures of lower dimensionality (manifolds) that exist in the original\\ndata. NLDR algorithms, or manifold learning algorithms, have potential to make the high-level\\nconcepts embedded in multidimensional data accessible to both humans and machines.\\nThis paper introduces a new algorithm for manifold learning called Manifold Sculpting, which discovers manifolds through a process of progressive refinement. Experiments show that it yields\\nmore accurate results than other algorithms in many cases. Additionally, it can be used as a postprocessing step to enhance the transformation of other manifold learning algorithms.\\n\\n2\\n\\nRelated Work\\n\\nMany algorithms have been developed for performing non-linear dimensionality reduction. Recent\\nworks include Isomap [1], which solves for an isometric embedding of data into fewer dimensions\\nwith an algebraic technique. Unfortunately, it is somewhat computationally expensive as it requires\\nsolving for the eigenvectors of a large dense matrix, and has difficulty with poorly sampled areas of\\n?\\n\\nmikegashler@gmail.com, ventura@cs.byu.edu, martinez@cs.byu.edu\\n\\n1\\n\\n\\fFigure 1: Comparison of several manifold learners on a Swiss Roll manifold. Color is used to\\nindicate how points in the results correspond to points on the manifold. Isomap and L-Isomap have\\ntrouble with sampling holes. LLE has trouble with changes in sample density.\\n\\nthe manifold. (See Figure 1.A.) Locally Linear Embedding (LLE) [2] is able to perform a similar\\ncomputation using a sparse matrix by using a metric that measures only relationships between vectors in local neighborhoods. Unfortunately it produces distorted results when the sample density is\\nnon-uniform. (See Figure 1.B.) An improvement to the Isomap algorithm was later proposed that\\nuses landmarks to reduce the amount of necessary computation [3]. (See Figure 1.C.) Many other\\nNLDR algorithms have been proposed, including Kernel Principle Component Analysis [4], Laplacian Eigenmaps [5], Manifold Charting [6], Manifold Parzen Windows [7], Hessian LLE [8], and\\nothers [9, 10, 11]. Hessian LLE preserves the manifold structure better than the other algorithms but\\nis, unfortunately, computationally expensive. (See Figure 1.D.).\\nIn contrast with these algorithms, Manifold Sculpting is robust to sampling issues and still produces\\nvery accurate results. This algorithm iteratively transforms data by balancing two opposing heuristics, one that scales information out of unwanted dimensions, and one that preserves local structure\\nin the data. Experimental results show that this technique preserves information into fewer dimensions with more accuracy than existing manifold learning algorithms. (See Figure 1.E.)\\n\\n3\\n\\nThe Algorithm\\n\\nAn overview of the Manifold Sculpting algorithm is given in Figure 2a.\\n\\nFigure 2: ? and ? define the relationships that Manifold Sculpting attempts to preserve.\\n\\n2\\n\\n\\fStep 1: Find the k nearest neighbors of each point. For each data point pi in P (where P is the set\\nof all data points represented as vectors in Rn ), find the k-nearest neighbors Ni (such that nij ? Ni\\nis the j th neighbor of point pi ).\\nStep 2: Compute relationships between neighbors. For each j (where 0 < j ? k) compute the\\nEuclidean distance ?ij between pi and each nij ? Ni . Also compute the angle ?ij formed by the\\ntwo line segments (pi to nij ) and (nij to mij ), where mij is the most colinear neighbor of nij with\\npi . (See Figure 2b.) The most colinear neighbor is the neighbor point that forms the angle closest\\nto ?. The values of ? and ? are the relationships that the algorithm will attempt to preserve during\\ntransformation. The global average distance between all the neighbors of all points ?ave is also\\ncomputed.\\nStep 3: Optionally preprocess the data. The data may optionally be preprocessed with the transformation step of Principle Component Analysis (PCA), or another efficient algorithm. Manifold\\nSculpting will work without this step; however, preprocessing can result in significantly faster convergence. To the extent that there is a linear component in the manifold, PCA will move the information in the data into as few dimensions as possible, thus leaving less work to be done in step 4\\n(which handles the non-linear component). This step is performed by computing the first |Dpres |\\nprinciple components of the data (where Dpres is the set of dimensions that will be preserved in\\nthe projection), and rotating the dimensional axes to align with these principle components. (An\\nefficient algorithm for computing principle components is presented in [12].)\\nStep 4: Transform the data. The data is iteratively transformed until some stopping criterion has\\nbeen met. One effective technique is to stop when the sum change of all points during the current\\niteration falls below a threshold. The best stopping criteria depend on the desired quality of results ?\\nif precision is important, the algorithm may iterate longer; if speed is important it may stop earlier.\\nStep 4a: Scale values. All the values in Dscal (The set of dimensions that will be eliminated by the\\nprojection) are scaled by a constant factor ?, where 0 < ? < 1 (? = 0.99 was used in this paper).\\nOver time, the values in Dscal will converge to 0. When Dscal is dropped by the projection (step 5),\\nthere will be very little informational content left in these dimensions.\\nStep 4b: Restore original relationships. For each pi ? P , the values in Dpres are adjusted to\\nrecover the relationships that are distorted by scaling. Intuitively, this step simulates tension on the\\nmanifold surface. A heuristic error value is used to evaluate the current relationships among data\\npoints relative to the original relationships:\\n\\u0012\\n\\u00132 \\u0012\\n\\u00132 !\\nk\\nX\\n?ij ? ?ij0\\n?ij ? ?ij0\\nwij\\n\\u000fpi =\\n+\\n(1)\\n2?ave\\n?\\nj=0\\nwhere ?ij is the current distance to nij , ?ij0 is the original distance to nij measured in step 2, ?ij\\nis the current angle, and ?ij0 is the original angle measured in step 2. The denominator values\\nwere chosen as normalizing factors because the value of the angle term can range from 0 to ?, and\\nthe value of the distance term will tend to have a mean of about ?ave with some variance in both\\ndirections. We adjust the values in Dpres for each point to minimize this heuristic error value.\\nThe order in which points are adjusted has some impact on the rate of convergence. Best results were\\nobtained by employing a breadth-first neighborhood graph traversal from a randomly selected point.\\n(A new starting point is randomly selected for each iteration.) Intuitively this may be analogous to\\nthe manner in which a person smoothes a crumpled piece of paper by starting at an arbitrary point\\nand smoothing outward. To further speed convergence, higher weight, wij , is given to the component\\nof the error contributed by neighbors that have already been adjusted in the current iteration. For all\\nof our experiments, we use wij = 1 if ni has not yet been adjusted in this iteration, and wij = 10,\\nif nij has been adjusted in this iteration.\\nUnfortunately the equation for the true gradient of the error surface defined by this heuristic is\\ncomplex, and is in O(|D|3 ). We therefore use the simple hill-climbing technique of adjusting in\\neach dimension in the direction that yields improvement.\\nSince the error surface is not necessarily convex, the algorithm may potentially converge to local\\nminima. At least three factors, however, mitigate this risk: First, the PCA pre-processing step often\\ntends to move the whole system to a state somewhat close to the global minimum. Even if a local\\n3\\n\\n\\fFigure 3: The mean squared error of four algorithms with a Swiss Roll manifold using a varying\\nnumber of neighbors k. When k > 57, neighbor paths cut across the manifold. Isomap is more\\nrobust to this problem than other algorithms, but HLLE and Manifold Sculpting still yield better\\nresults. Results are shown on a logarithmic scale.\\nminimum exists so close to the globally optimal state, it may have a sufficiently small error as to be\\nacceptable. Second, every point has a unique error surface. Even if one point becomes temporarily\\nstuck in a local minimum, its neighbors are likely to pull it out, or change the topology of its error\\nsurface when their values are adjusted. Very particular conditions are necessary for every point to\\nsimultaneously find a local minimum. Third, by gradually scaling the values in Dscaled (instead of\\ndirectly setting them to 0), the system always remains in a state very close to the current globally\\noptimal state. As long as it stays close to the current optimal state, it is unlikely for the error\\nsurface to change in a manner that permanently separates it from being able to reach the globally\\noptimal state. (This is why all the dimensions need to be preserved in the PCA pre-processing step.)\\nAnd perhaps most significantly, our experiments show that Manifold Sculpting generally tends to\\nconverge to very good results.\\nStep 5: Project the data. At this point Dscal contains only values that are very close to zero. The\\ndata is projected by simply dropping these dimensions from the representation.\\n\\n4\\n\\nEmpirical Results\\n\\nFigure 1 shows that Manifold Sculpting appears visually to produce results of higher quality than\\nLLE and Isomap with the Swiss Roll manifold, a common visual test for manifold learning algorithms. Quantitative analysis shows that it also yields better results than HLLE. Since the actual\\nstructure of this manifold is known prior to using any manifold learner, we can use this prior information to quantitatively measure the accuracy of each algorithm.\\n4.1\\n\\nVarying number of neighbors.\\n\\nWe define a Swiss Roll in 3D space with n points (xi , yi , zi ) for each 0 ? i < n, such that xi =\\nt sin(t), yi is a random number ?6 ? yi < 6, and zi = t cos(t), ?where t = 8i/n + 2. In 2D\\n?1\\nt2 +1\\nand vi = yi .\\nmanifold coordinates, the point is (ui , vi ), such that ui = sinh (t)+t\\n2\\nWe created a Swiss Roll with 2000 data points and reduced the dimensionality to 2 with each of four\\nalgorithms. Next we tested how well these results align with the expected values by measuring the\\nmean squared distance from each point to its expected value. (See Figure 3.) We rotated, scaled,\\nand translated the values as required to obtain the minimum possible error measurement for each\\nalgorithm. These results are consistent with a qualitative assessment of Figure 1. Results are shown\\nwith a varying number of neighbors k. In this example, when k = 57, local neighborhoods begin\\nto cut across the manifold. Isomap is more robust to this problem than other algorithms, but HLLE\\nand Manifold Sculpting still yield better results.\\n4\\n\\n\\fFigure 4: The mean squared error of points from an S-Curve manifold for four algorithms with a\\nvarying number of data points. Manifold Sculpting shows a trend of increasing accuracy with an\\nincreasing number of points. This experiment was performed with 20 neighbors. Results are shown\\non a logarithmic scale.\\n4.2\\n\\nVarying sample densities.\\n\\nA similar experiment was performed with an S-Curve manifold. We defined the S-Curve points in\\n3D space with n points (xi , yi , zi ) for each 0 ? i < n, such that xi = t, yi = sin(t), and zi is\\na random number 0 ? zi < 2, where t = (2.2i?0.1)?\\n. In 2D manifold coordinates, the point is\\nn\\nZ t \\u0010p\\n\\u0011\\ncos2 (w) + 1 dw and vi = yi .\\n(ui , vi ), such that ui =\\n0\\n\\nFigure 4 shows the mean squared error of the transformed points from their expected values using\\nthe same regression technique described for the experiment with the Swiss Roll problem. We varied\\nthe sampling density to show how this affects each algorithm. A trend can be observed in this data\\nthat as the number of sample points increases, the quality of results from Manifold Sculpting also\\nincreases. This trend does not appear in the results from other algorithms.\\nOne drawback to the Manifold Sculpting algorithm is that convergence may take longer when the\\nvalue for k is too small. This experiment was also performed with 6 neighbors, but Manifold Sculpting did not always converge within a reasonable time when so few neighbors were used. The other\\nthree algorithms do not have this limitation, but the quality of their results still tend to be poor when\\nvery few neighbors are used.\\n4.3\\n\\nEntwined spirals manifold.\\n\\nA test was also performed with an Entwined Spirals manifold. In this case, Isomap was able to\\nproduce better results than Manifold Sculpting (see Figure 5), even though Isomap yielded the worst\\naccuracy in previous problems. This can be attributed to the nature of the Isomap algorithm. In cases\\nwhere the manifold has an intrinsic dimensionality of exactly 1, a path from neighbor to neighbor\\nprovides an accurate estimate of isolinear distance. Thus an algorithm that seeks to globally optimize isolinear distances will be less susceptible to the noise from cutting across local corners. When\\nthe intrinsic dimensionality is higher than 1, however, paths that follow from neighbor to neighbor\\nproduce a zig-zag pattern that introduces excessive noise into the isolinear distance measurement. In\\nthese cases, preserving local neighborhood relationships with precision yields better overall results\\nthan globally optimizing an error-prone metric. Consistent with this intuition, Isomap is the closest\\ncompetitor to Manifold Sculpting in other experiments that involved a manifold with a single intrinsic dimension, and yields the poorest results of the four algorithms when the intrinsic dimensionality\\nis larger than one.\\n5\\n\\n\\fFigure 5: Mean squared error for four algorithms with an Entwined Spirals manifold.\\n4.4\\n\\nImage-based manifolds.\\n\\nThe accuracy of Manifold Sculpting is not limited to generated manifolds in three dimensional\\nspace. Unfortunately, the manifold structure represented by most real-world problems is not known\\na priori. The accuracy of a manifold learner, however, can still be estimated when the problem\\ninvolves a video sequence by simply counting the percentage of frames that are sorted into the same\\norder as the video sequence. Figure 6 shows several frames from a video sequence of a person\\nturning his head while gradually smiling. Each image was encoded as a vector of 1, 634 pixel\\nintensity values. This data was then reduced to a single dimension. (Results are shown on three\\nseparate lines in order to fit the page.) The one preserved dimension could then characterize each\\nframe according to the high-level concepts that were previously encoded in many dimensions. The\\ndot below each image corresponds to the single-dimensional value in the preserved dimension for\\nthat image. In this case, the ordering of every frame was consistent with the video sequence.\\n4.5\\n\\nControlled manifold topologies.\\n\\nFigure 7 shows a comparison of results obtained from a manifold generated by translating an image\\nover a background of random noise. Nine of the 400 input images are shown as a sample, and\\nresults with each algorithm are shown as a mesh. Each vertex is placed at a position corresponding\\nto the two values obtained from one of the 400 images. For increased visibility of the inherent\\nstructure, the vertexes are connected with their nearest input space neighbors. Because two variables\\n(horizontal position and vertical position) were used to generate the dataset, this data creates a\\nmanifold with an intrinsic dimensionality of two in a space with an extrinsic dimensionality of\\n2,401 (the total number of pixels in each image). Because the background is random, the average\\ndistance between neighboring points in the input space is uniform, so the ideal result is known to\\nbe a square. The distortions produced by Manifold Sculpting tend to be local in nature, while the\\ndistortions produced by other algorithms tend to be more global. Note that the points are spread\\nnearly uniformly across the manifold in the results from Manifold Sculpting. This explains why the\\nresults from Manifold Sculpting tend to fit the ideal results with much lower total error (as shown in\\n\\nFigure 6: Images of a face reduced by Manifold Sculpting into a single dimension. The values are\\nare shown here on three wrapped lines in order to fit the page. The original image is shown above\\neach point.\\n6\\n\\n\\fFigure 7: A comparison of results with a manifold generated by translating an image over a background of noise. Manifold Sculpting tends to produce less global distortion, while other algorithms\\ntend to produce less local distortion. Each point represents an image. This experiment was done\\nin each case with 8 neighbors. (LLE fails to yield results with these parameters, but [13] reports a\\nsimilar experiment in which LLE produces results. In that case, as with Isomap and HLLE as shown\\nhere, distortion is clearly visible near the edges.)\\n\\nFigure 3 and Figure 4). Perhaps more significantly, it also tends to keep the intrinsic variables in the\\ndataset more linearly separable. This is particularly important when the dimensionality reduction is\\nused as a pre-processing step for a supervised learning algorithm.\\nWe created four video sequences designed to show various types of manifold topologies and measured the accuracy of each manifold learning algorithm. These results (and sample frames from each\\nvideo) are shown in Figure 8. The first video shows a rotating stuffed animal. Since the background\\npixels remain nearly constant while the pixels on the rotating object change in value, the manifold\\ncorresponding to the vector encoding of this video will contain both smooth and changing areas.\\nThe second video was made by moving a camera down a hallway. This produces a manifold with a\\ncontinuous range of variability, since pixels near the center of the frame change slowly while pixels\\nnear the edges change rapidly. The third video pans across a scene. Unlike the video of the rotating\\nstuffed animal, there are no background pixels that remain constant. The last video shows another\\nrotating stuffed animal. Unlike the first video, however, the high-contrast texture of the object used\\nin this video results in a topology with much more variation. As the black spots shift across the\\npixels, a manifold is created that swings wildly in the respective dimensions. Due to the large hills\\nand valleys in the topology of this manifold, the nearest neighbors of a frame frequently create paths\\nthat cut across the manifold. In all four cases, Manifold Sculpting produced results competitive\\nwith Isomap, which does particularly well with manifolds that have an intrinsic dimensionality of\\n\\nFigure 8: Four video sequences were created with varying properties in the corresponding manfolds.\\nDimensionality was reduced to one with each of four manifold learning algorithms. The percentage\\nof frames that were correctly ordered by each algorithm is shown.\\n\\n7\\n\\n\\fone, but Manifold Sculpting is not limited by the intrinsic dimensionality as shown in the previous\\nexperiments.\\n\\n5\\n\\nDiscussion\\n\\nThe experiments tested in this paper show that Manifold Sculpting yields more accurate results\\nthan other well-known manifold learning algorithms. Manifold Sculpting is robust to holes in the\\nsampled area. Manifold Sculpting is more accurate than other algorithms when the manifold is\\nsparsely sampled, and the gap is even wider with higher sampling densities. Manifold Sculpting\\nhas difficulty when the selected number of neighbors is too small but consistently outperforms other\\nalgorithms when it is larger.\\nDue to the iterative nature of Manifold Sculpting, it?s difficult to produce a valid complexity analysis.\\nConsequently, we measured the scalability of Manifold Sculpting empirically and compared it with\\nthat of HLLE, L-Isomap, and LLE. Due to space constraints these results are not included here, but\\nthey indicate that Manifold Sculpting scales better than the other algorithms when when the number\\nof data points is much larger than the number of input dimensions.\\nManifold Sculpting benefits significantly when the data is pre-processed with the transformation step of PCA. The transformation step of any algorithm may be used in place of this step.\\nCurrent research seeks to identify which algorithms work best with Manifold Sculpting to efficiently produce high quality results. (An implementation of Manifold Sculpting is included at\\nhttp://waffles.sourceforge.net.)\\n\\nReferences\\n[1] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for\\nnonlinear dimensionality reduction. Science, 290:2319?2323, 2000.\\n[2] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear\\nembedding. Science, 290:2323?2326, 2000.\\n[3] Vin de Silva and Joshua B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. In NIPS, pages 705?712, 2002.\\n[4] Bernhard Sch?olkopf, Alexander J. Smola, and Klaus-Robert M?uller. Kernel principal component analysis. Advances in kernel methods: support vector learning, pages 327?352, 1999.\\n[5] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, 14, pages 585?\\n591, 2001.\\n[6] Matthew Brand. Charting a manifold. In Advances in Neural Information Processing Systems,\\n15, pages 961?968. MIT Press, Cambridge, MA, 2003.\\n[7] Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in Neural Information Processing Systems 15, pages 825?832. MIT Press, Cambridge, MA, 2003.\\n[8] D. Donoho and C. Grimes. Hessian eigenmaps: locally linear embedding techniques for high\\ndimensional data. Proc. of National Academy of Sciences, 100(10):5591?5596, 2003.\\n[9] Yoshua Bengio and Martin Monperrus. Non-local manifold tangent learning. In Advances\\nin Neural Information Processing Systems 17, pages 129?136. MIT Press, Cambridge, MA,\\n2005.\\n[10] Elizaveta Levina and Peter J. Bickel. Maximum likelihood estimation of intrinsic dimension.\\nIn NIPS, 2004.\\n[11] Zhenyue Zhang and Hongyuan Zha. A domain decomposition method for fast manifold learning. In Y. Weiss, B. Sch?olkopf, and J. Platt, editors, Advances in Neural Information Processing\\nSystems 18. MIT Press, Cambridge, MA, 2006.\\n[12] Sam Roweis. Em algorithms for PCA and SPCA. In Michael I. Jordan, Michael J. Kearns, and\\nSara A. Solla, editors, Advances in Neural Information Processing Systems, volume 10, 1998.\\n[13] Lawrence K. Saul and Sam T. Roweis. Think globally, fit locally: Unsupervised learning of\\nlow dimensional manifolds. Journal of Machine Learning Research, 4:119?155, 2003.\\n\\n8\\n\\n\\f\",\n          \"Diffeomorphic Dimensionality Reduction\\n\\nChristian Walder and Bernhard Sch?olkopf\\nMax Planck Institute for Biological Cybernetics\\n72076 T?ubingen, Germany\\nfirst.last@tuebingen.mpg.de\\n\\nAbstract\\nThis paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a\\nnatural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to\\na lower dimensional subspace and then projects onto that subspace. The problem\\nof solving for the mapping is transformed into one of solving for an Eulerian flow\\nfield which we compute using ideas from kernel methods. We demonstrate the\\nefficacy of our approach on various real world data sets.\\n\\n1\\n\\nIntroduction\\n\\nThe problem of visualizing high dimensional data often arises in the context of exploratory data\\nanalysis. For many real world data sets this is a challenging task, as the spaces in which the data\\nlie are often too high dimensional to be visualized directly. If the data themselves lie on a lower\\ndimensional subspace however, dimensionality reduction techniques may be employed, which aim\\nto meaningfully represent the data as elements of this lower dimensional subspace.\\nThe earliest approaches to dimensionality reduction are the linear methods known as principal components analysis (PCA) and factor analysis (Duda et al., 2000). More recently however, the majority of research has focussed on non-linear methods, in order to overcome the limitations of linear\\napproaches?for an overview and numerical comparison see e.g. (Venna, 2007; van der Maaten\\net al., 2008), respectively. In an effort to better understand the numerous methods which have been\\nproposed, various categorizations have been proposed. In the present case, it is pertinent to make\\nthe distinction between methods which focus on properties of the mapping to the lower dimensional\\nspace, and methods which focus on properties of the mapped data, in that space. A canonical example of the latter is multidimensional scaling (MDS), which in its basic form finds the minimizer\\nwith respect to y1 , y2 , . . . , ym of (Cox & Cox, 1994)\\nm\\nX\\n\\n2\\n\\n(kxi ? xj k ? kyi ? yj k) ,\\n\\n(1)\\n\\ni,j=1\\n\\nwhere here, as throughout the paper, the xi ? Ra are input or high dimensional points, and the\\nyi ? Rb are output or low dimensional points, so that b < a. Note that the above term is a\\nfunction only of the input points and the corresponding mapped points, and is designed to preserve\\nthe pairwise distances of the data set.\\nThe methods which focus on the mapping itself (from the higher to the lower dimensional space,\\nwhich we refer to as the downward mapping, or the upward mapping which is the converse) are less\\ncommon, and form a category into which the present work falls. Both auto-encoders (DeMers &\\nCottrell, 1993) and the Gaussian process latent variable model (GP-LVM) (Lawrence, 2004) also\\nfall into this category, but we focus on the latter as it provides an appropriate transition into the\\n1\\n\\n\\fmain part the paper. The GP-LVM places a Gaussian process (GP) prior over each high dimensional component of the upward mapping, and optimizes with respect to the set of low dimensional\\npoints?which can be thought of as hyper-parameters of the model?the likelihood of the high dimensional points. Hence the GP-LVM constructs a regular (in the sense of regularization, i.e. likely\\nunder the GP prior) upward mapping. By doing so, the model guarantees that nearby points in\\nthe low dimensional space should be mapped to nearby points in the high dimensional space?an\\nintuitive idea for dimensionality reduction which is also present in the MDS objective (1), above.\\nThe converse is not guaranteed in the original GP-LVM however, and this has lead to the more recent development of the so-called back-constrained GP-LVM (Lawrence & Candela, 2006), which\\nessentially places an additional GP prior over the downward mapping. By guaranteeing in this way\\nthat (the modes of the posterior distributions over) both the upward and downward mappings are\\nregular, the back constrained GP-LVM induces something reminiscent of a diffeomorphic mapping\\nbetween the two spaces. This leads us to the present work, in which we derive our new algorithm,\\nDiffeomap, by explicitly casting the dimensionality reduction problem as one of constructing a diffeomorphic mapping between the low dimensional space and the subspace of the high dimensional\\nspace on which the data lie.\\n\\n2\\n\\nDiffeomorphic Mappings and their Practical Construction\\n\\nIn this paper we use the following definition:\\nDefinition 2.1. Let U and V be open subsets of Ra and Rb , respectively. The mapping F : U ? V\\nis said to be a diffeomorphism if it is bijective (i.e. one to one), smooth (i.e. belonging to C ? ), and\\nhas a smooth inverse map F ?1 .\\nWe note in passing the connection between this definition, our discussion of the GP-LVM, and dimensionality reduction. The GP-LVM constructs a regular upward mapping (analogous to F ?1 )\\nwhich ensures that points nearby in Rb will be mapped to points nearby in Ra , a property referred\\nto as similarity preservation in (Lawrence & Candela, 2006). The back constrained GP-LVM simultaneously ensures that the downward mapping (analogous to F ) is regular, thereby additionally\\nimplementing what its authors refer to as dissimilarity preservation. Finally, the similarity between\\nsmoothness (required of F and F ?1 in Definition 2.1) and regularity (imposed on the downward and\\nupward mappings by the GP prior in the back constrained GP-LVM) complete the analogy. There is\\nalso an alternative, more direct motivation for diffeomorphic mappings in the context of dimensionality reduction, however. In particular, a diffeomorphic mapping has the property that it does not\\nlose any information. That is, given the mapping itself and the lower dimensional representation of\\nthe data set, it is always possible to reconstruct the original data.\\nThere has been significant interest from within the image processing community, in the construction\\nof diffeomorphic mappings for the purpose of image warping (Dupuis & Grenander, 1998; Joshi\\n& Miller, 2000; Karac?ali & Davatzikos, 2003). The reason for this can be understood as follows.\\nLet I : U ? R3 represent the RGB values of an image, where U ? R2 is the image plane. If we\\nnow define the warped version of I to be I ? W , then we can guarantee that the warp is topology\\npreserving, i.e. that it does not ?tear? the image, by ensuring the W be a diffeomorphism U ? U .\\nThe following two main approaches to constructing such diffeomorphisms have been taken by the\\nimage processing community, the first of which we mention for reference, while the second forms\\nthe basis of Diffeomap. It is a notable aside that there seem to be no image warping algorithms\\nanalogous to the back constrained GP-LVM, in which regular forward and inverse mappings are\\nsimultaneously constructed.\\n1. Enforcement of the constraint that |J(W )|, the determinant of the Jacobian of the mapping, be positive everywhere. This approach has been successfully applied to the problem\\nof warping 3D magnetic resonance images (Karac?ali & Davatzikos, 2003), for example,\\nbut a key ingredient of that success was the fact that the authors defined the mapping W\\nnumerically on a regular grid. For the high dimensional cases relevant to dimensionality\\nreduction however, such a numerical grid is highly computationally unattractive.\\n2. Recasting the problem of constructing W as an Eulerian flow problem (Dupuis & Grenander, 1998; Joshi & Miller, 2000). This approach is the focus of the next section.\\n2\\n\\n\\fR\\n\\nR\\n?(x, 1) = ?(x)\\n(s, ?(x, s))\\n(1, v(?(x, s), s))\\n\\nx\\nt\\n0\\n\\ns\\n\\n1\\n\\nFigure 1: The relationship between v(?, ?), ?(?, ?) and ?(?) for the one dimensional case ? : R ? R.\\n\\n2.1\\n\\nDiffeomorphisms via Flow Fields\\n\\nThe idea here is to indirectly define the mapping of interest, call it ? : Ra ? Ra , by way of a ?time?\\nindexed velocity field v : Ra ? R ? Ra . In particular we write ?(x) = ?(x, 1), where\\nZ t\\nv(?(x, s), s)ds.\\n(2)\\n?(x, t) = x +\\ns=0\\n\\nThis choice of ? satisfies the following Eulerian transport equation with boundary conditions:\\n??(x, s)\\n= v(?(x, s), s),\\n?s\\n\\n?(x, 0) = x.\\n\\n(3)\\n\\nThe role of v is to transport a given point x from its original location at time 0 to its mapped location\\n?(x, 1) by way of a trajectory whose position and tangent vector at time s are given by ?(x, s) and\\nv(?(x, s), s), respectively (see Figure 1). The point of this construction is that if v satisfies certain\\nregularity properties, then the mapping ? will be a diffeomorphism. This fact has been proven in a\\nnumber of places?one particularly accessible example is (Dupuis & Grenander, 1998), where the\\nnecessary conditions are provided for the three dimensional case along with a proof that the induced\\nmapping is a diffeomorphism. Generalizing the result to higher dimensions is straightforward?this\\nfact is stated in (Dupuis & Grenander, 1998) along with the basic idea of how to do so.\\nWe now offer an intuitive argument for the result. Consider Figure 1, and imagine adding a new\\nstarting point x? , along with its associated trajectory. It is clear that for the mapping ? to be a\\ndiffeomorphism, then for any such pair of points x and x? , the associated trajectories must not\\ncollide. This is because the two trajectories would be identical after the collision, x and x? would\\nmap to the same point, and hence the mapping would not be invertible. But if v is sufficiently regular\\nthen such collisions cannot occur.\\n\\n3\\n\\nDiffeomorphic Dimensionality Reduction\\n\\nThe framework of Eulerian flow fields which we have just introduced provides an elegant means\\nof constructing diffeomorphic mappings Ra ? Ra , but for dimensionality reduction we require\\nadditional ingredients, which we now introduce. The basic idea is to construct a diffeomorphic\\nmapping in such a way that it maps our data set near to a subspace of Ra , and then to project onto\\nthis subspace. The subspace we use, call it Sb , is the b-dimensional one spanned by the first b\\ncanonical basis vectors of Ra . Let P(a?b) : Ra ? Rb be the projection operator which extracts the\\nfirst b components of the vector it is applied to, i.e.\\nP(a?b) x = (I Z) x,\\n\\n(4)\\n\\nwhere I ? Ra?a is the identity matrix and Z ? Ra?b?a is a matrix of zeros. We can now write the\\nmapping ? : Ra ? Rb which we propose for dimensionality reduction as\\n?(x) = P(a?b) ?(x, 1),\\n3\\n\\n(5)\\n\\n\\fwhere ? is given by (2). We choose each component of v at each time to belong to a reproducing\\nkernel Hilbert Space (RKHS) H, so that v(?, t) ? Ha , t ? [0, 1]. If we define the norm1\\na \\n\\n2\\nX\\n\\n\\n2\\nkv(?, t)kHa ,\\n(6)\\n\\n[v(?, t)]j \\n ,\\nH\\n\\nj=1\\n\\n2\\n\\nthen kv(?, t)kHa < ?, ?t ? [0, 1] is a sufficient condition which guarantees that ? is a diffeomorphism, provided that some technical conditions are satisfied (Dupuis & Grenander, 1998; Joshi\\n& Miller, 2000). In particular v need not be regular in its second argument. For dimensionality\\nreduction we propose to construct v as the minimizer of\\nZ 1\\nm\\nX\\n2\\nkv(?, t)kHd dt +\\nL (?(xj )) ,\\n(7)\\nO=?\\nt=0\\n\\nj=1\\n\\n+\\n\\nwhere ? ? R is a regularization parameter. Here, L measures the squared distance to our b\\ndimensional linear subspace of interest Sb , i.e.\\nL(x) =\\n\\na\\nX\\n\\n2\\n\\n[x]d .\\n\\n(8)\\n\\nd=b+1\\n\\nNote that this places special importance on the first b dimensions of the input space of interest?\\naccordingly we make the natural and important preprocessing step of applying PCA such that as\\nmuch as possible of the variance of the data is captured in these first b dimensions.\\n3.1\\n\\nImplementation\\n\\nOne can show that the minimizer in v of (7) takes the form\\n[v(?, t)]d =\\n\\nm\\nX\\n\\n[?d (t)]j k(?(xj , t), ?),\\n\\nd = 1 . . . a,\\n\\n(9)\\n\\nj=1\\n\\nwhere k is the reproducing kernel of H and ?d is a function [0, 1] ? Rm . This was proven directly\\nfor a similar specific case (Joshi & Miller, 2000), but we note in passing that it follows immediately\\nfrom the celebrated representer theorem of RKHS?s (Sch?olkopf et al., 2001), by considering a fixed\\ntime t. Hence, we have simplified the problem of determining v to one of determining m trajectories\\n?(xj , ?). This is because not only does (9) hold, but we can use standard manipulations (in the\\ncontext of kernel ridge regression, for example) to determine that for a given set of such trajectories,\\n?d (t) = K(t)?1 ud (t),\\nm?m\\n\\nd = 1, 2, . . . , a,\\n\\n(10)\\n\\nm\\n\\nwhere t ? [0, 1], K(t) ? R\\n, ud (t) ? R and we have let [K(t)]j,k = k(?(xj , t), ?(xk , t))\\nalong with [ud (t)]j = ?t ?(xj , t). Note that the invertibility of K(t) is guaranteed for certain kernel\\nfunctions (including the Gaussian kernel which we employ in all our Experiments, see Section 4),\\nprovided that the set ?(xj , t) are distinct. Hence, one can verify using (9), (10) and the reproducing\\nproperty of k in H (i.e. the fact that hf, k(x, ?)iH = f (x), ?f ? H), that for the optimal v,\\n2\\n\\nkv(?, t)kHa =\\n\\na\\nX\\n\\nud (t)? K(t)?1 ud (t).\\n\\n(11)\\n\\nd=1\\n\\nThis allows us to write our objective (7) in terms of the m trajectories mentioned above:\\nZ 1 X\\na\\nm X\\na\\nX\\n2\\nO=?\\nud (t)? K(t)?1 ud (t) +\\n[?(xj , 1)]d .\\nt=0 d=1\\n\\n(12)\\n\\nj=1 d=b+1\\n\\nSo far no approximations have been made, and we have constructed an optimal finite dimensional\\nbasis for v(?, t). The second argument of v is not so easily dealt with however, so as an approximate\\nby discretizing the interval [0, 1]. In particular, we let tk = k?, k = 0, 1, . . . , p, where ? = 1/p,\\nand make the approximation ?t=tk ?(xj , t) = (?(xj , tk ) ? ?(xj , tk?1 )) /?. By making the further\\n1\\n\\nSquare brackets w/ subscripts denote matrix elements, and colons denote entire rows or columns.\\n\\n4\\n\\n\\f0.9\\n\\n0.8\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n(d)\\n\\n0.4\\n\\n(c)\\n(b)\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\n(d)\\n\\nFigure 2: Dimensionality reduction of motion capture data. (a) The data mapped from 102 to\\n2 dimensions using Diffeomap (the line shows the temporal order in which the input data were\\nrecorded). (b)-(d) Three rendered input points corresponding to the marked locations in (a).\\nR tk\\napproximation t=t\\nK(t)?1 dt = ?K(tk?1 )?1 , and substituting into (12) we obtain the first form\\nk?1\\nof our problem which is finite dimensional and hence readily optimized, i.e. the minimization of\\np\\na\\nb\\nX\\n? XX\\n?\\n(?k,d ? ?k?1,d ) K(tk )?1 (?k,d ? ?k?1,d ) +\\nk?p,d k2\\n?\\n\\n(13)\\n\\nd=a+1\\n\\nd=1 k=1\\n\\nwith respect to ?k,d ? Rm for k = 1, 2, . . . , p and d = 1, 2, . . . , a, where [?k,d ]j = [?(xj , tk )]d .\\n3.2\\n\\nA Practical Reduced Set Implementation\\n\\nA practical problem with (13) is the computationally expensive matrix inverse. In practice we reduce\\nthis burden by employing a reduced set expansion which replaces the sum over 1, 2, . . . , m in (9)\\nwith a sum over a randomly selected subset I, thereby using |I| = n basis functions to represent\\nv(?, t). In this case it is possible to show using the reproducing property of k(?, ?) that the resulting\\nobjective function is identical to (13), but with the matrix K(tk )?1 replaced by the expression\\nKm,n (Kn,m Km,n )\\n?\\nKn,m\\n\\n?1\\n\\n?1\\n\\nKn,n (Kn,m Km,n )\\n\\nKn,m ,\\n\\n(14)\\n\\nm?n\\n\\nwhere Km,n =\\n? R\\nis the sub-matrix of K(tk ) formed by taking all of the rows, but\\nonly those columns given by I. Similarly, Kn,n ? Rn?n is the square sub-matrix of K(tk ) formed\\nby taking a subset of both the rows and columns, namely those given by I. For optimization we\\nalso use the gradients of the above expression, the derivation of which we have omitted for brevity.\\nNote however that by factorizing appropriately, the computation of the objective function and its\\ngradients can be performed with an asymptotic time complexity of n2 (m + a).\\n\\n4\\n\\nExperiments\\n\\nIt is difficult to objectively compare dimensionality reduction algorithms, as there is no universally\\nagreed upon measure of performance. Algorithms which are generalizations or variations of older\\nones may be compared side by side with their predecessors, but this is not the case with our new\\nalgorithm, Diffeomap. Hence, in this section we attempt to convince the reader of the utility of our\\napproach by visually presenting our results on as many and as varied realistic problems as space\\npermits, while providing pointers to comparable results from other authors. For all experiments\\nwe fixed the parameters which trade off between computational speed and accuracy, i.e. we set the\\ntemporal resolution p = 20, and the number of\\n\\u0001 basis functions n = 300. We used a Gaussian kernel\\nfunction k(x, y) = exp ?kx ? yk2 /(2? 2 ) , and tuned the ? parameter manually along with the\\nregularization parameter ?. For optimization we used a conjugate gradient type method2 fixed to\\n1000 iterations and with starting point [?k,d ]j = [xj ]d , k = 1, 2, . . . p.\\n2\\n\\nCarl Rasmussen?s minimize.m, which is freely available from http://www.kyb.mpg.de/?carl.\\n\\n5\\n\\n\\fa\\n?\\n\\\"\\ne\\ni\\n1\\no\\n@\\nu\\n(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 3: Vowel data mapped from 24 to 2 dimensions using (a) PCA and (b)-(c) Diffeomap. Plots\\n(b) and (c) differ only in the parameter settings of Diffeomap, with (b) corresponding to minimal\\none nearest neighbor errors in the low dimensional space?see Section 4.2 for details.\\n4.1\\n\\nMotion Capture Data\\n\\nThe first data set we consider consists of the coordinates in R3 of a set of markers placed on a person\\nbreaking into a run, sampled at a constant frequency, resulting in m = 217 data points in a = 102\\ndimensions, which we mapped to b = 2 dimensions using Diffeomap (see Figure 2). This data set\\nis freely available from http://accad.osu.edu/research/mocap/mocap_data.htm\\nas Figure 1 Run, and was also considered in (Lawrence & Candela, 2006), where it was shown\\nthat while the original GP-LVM fails to correctly discover the periodic component of the sequence,\\nthe back constrained version maps poses in the same part of the subject?s step cycle nearby to\\neach other, while simultaneously capturing variations in the inclination of the subject. Diffeomap\\nalso succeeded in this sense, and produced results which are competitive with those of the back\\nconstrained GP-LVM.\\n4.2\\n\\nVowel Data\\n\\nIn this next example we consider a data set of a = 24 features (cepstral coefficients and delta\\ncepstral coefficients) of a single speaker performing nine different vowels 300 times per vowel,\\nacquired as training data for a vocal joystick system (Bilmes & et.al., 2006), and publicly available\\nin pre-processed form from http://www.dcs.shef.ac.uk/?neil/fgplvm/. Once again\\nwe used Diffeomap to map the data to b = 2 dimensions, as depicted in Figure 3. We also depict\\nthe poor result of linear PCA, in order to rule out the hypothesis that it is merely the PCA based\\ninitialization of Diffeomap (mentioned after equation (8) on page 4) which does most of the work.\\nThe results in Figure 3 are directly comparable to those provided in (Lawrence & Candela, 2006)\\nfor the GP-LVM, back constrained GP-LVM, and Isomap (Tenenbaum et al., 2000). Visually, the\\nDiffeomap result appears to be superior to those of the GP-LVM and Isomap, and comparable to the\\nback constrained GP-LVM. We also measured the performance of a one nearest neighbor classifier\\napplied to the mapped data in R2 . For the best choice of the parameters ? and ?, Diffeomap made\\n140 errors, which is favorable to the figures quoted for Isomap (458), the GP-LVM (226) and the\\nback constrained GP-LVM (155) in (Lawrence & Candela, 2006). We emphasize however that this\\nmeasure of performance is at best a rough one, since by manually varying our choice of the parameters ? and ?, we were able to obtain a result (Figure 3 (c)) which, although leads to a significantly\\nhigher number of such errors (418), is arguably superior from a qualitative perspective to the result\\nwith minimal errors (Figure 3 (b)).\\n4.3\\n\\nUSPS Handwritten Digits\\n\\nWe now consider the USPS database of handwritten digits (Hull, 1994). Following the methodology of the stochastic neighbor embedding (SNE) and GP-LVM papers (Hinton & Roweis, 2003;\\nLawrence, 2004), we take 600 images per class from the five classes corresponding to digits 0, 1, 2,\\n3, 4. Since the images are in gray scale and a resolution of 16 by 16 pixels, this results in a data set\\nof m = 3000 examples in a = 256 dimensions, which we again mapped to b = 2 dimensions as\\ndepicted in Figure 4. The figure shows the individual points color coded according to class, along\\n6\\n\\n\\f(a)\\n\\n(b)\\n\\nFigure 4: USPS handwritten digits 0-4 mapped to 2 dimensions using Diffeomap. (a) Mapped points\\ncolor coded by class label. (b) A composite image of the mapped data?see Section 4.3 for details.\\n\\nwith a composite image formed by sequentially drawing each digit in random order at its mapped\\nlocation, but only if it would not obscure a previously drawn digit. Diffeomap manages to arrange\\nthe data in a manner which reveals such image properties as digit angle and stroke thickness. At the\\nsame time the classes are reasonably well separated, with the exception of the ones which are split\\ninto two clusters depending on the angle. Although unfortunate, we believe that this splitting can\\nbe explained by the fact that (a) the left- and right-pointing ones are rather dissimilar in input space,\\nand (b) the number of fairly vertical ones which could help to connect the left- and right-pointing\\nones is rather small. Diffeomap seems to produce a result which is superior to that of the GP-LVM\\n(Lawrence, 2004), for example, but may be inferior to that of the SNE (Hinton & Roweis, 2003). We\\nbelieve this is due to the fact that the nearest neighbor graph used by SNE is highly appropriate to the\\nUSPS data set. This is indicated by the fact that a nearest neighbor classifier in the 256 dimensional\\ninput space is known to perform strongly, with numerous authors having reported error rates of less\\nthan 5% on the ten class classification problem.\\n4.4\\n\\nNIPS Text Data\\n\\nFinally, we present results on the text data of papers from the NIPS conference proceedings volumes\\n0-12, which can be obtained from http://www.cs.toronto.edu/?roweis/data.html.\\nThis experiment is intended to address the natural concern that by working in the input space rather\\nthan on a nearest neighbor graph, for example, Diffeomap may have difficulty with very high dimensional data. Following (Hinton & Roweis, 2003; Song et al., 2008) we represent the data as a word\\nfrequency vs. document matrix in which the author names are treated as words but weighted up by\\na factor 20 (i.e. an author name is worth 20 words). The result is a data set of m = 1740 papers\\nrepresented in a = 13649 words + 2037 authors = 15686 dimensions. Note however that the input\\ndimensionality is effectively reduced by the PCA preprocessing step to m ? 1 = 1739, that being\\nthe rank of the centered covariance matrix of the data.\\nAs this data set is difficult to visualize without taking up large amounts of space, we have included\\nthe results in the supplementary material which accompanies our NIPS submission. In particular,\\nwe provide a first figure which shows the data mapped to b = 2 dimensions, with certain authors (or\\ngroups of authors) color coded?the choice of authors and their corresponding color codes follows\\nprecisely those of (Song et al., 2008). A second figure shows a plain marker drawn at the mapped\\nlocations corresponding to each of the papers. This second figure also contains the paper title and\\nauthors of the corrsponding papers however, which are revealed when the user moves the mouse\\nover the marked locations. Hence, this second figure allows one to browse the NIPS collection con7\\n\\n\\ftextually. Since the mapping may be hard to judge, we note in passing that the correct classification\\nrate of a one nearest neighbor classifier applied to the result of Diffeomap was 48%, which compares\\nfavorably to the rate of 33% achieved by linear PCA (which we use for preprocessing). To compute\\nthis score we treated authors as classes, and considered only those authors who were color coded\\nboth in our supplementary figure and in (Song et al., 2008).\\n\\n5\\n\\nConclusion\\n\\nWe have presented an approach to dimensionality reduction which is based on the idea that the mapping between the lower and higher dimensional spaces should be diffeomorphic. We provided a\\njustification for this approach, by showing that the common intuition that dimensionality reduction\\nalgorithms should approximately preserve pairwise distances of a given data set is closely related to\\nthe idea that the mapping induced by the algorithm should be a diffeomorphism. This realization\\nallowed us to take advantage of established mathematical machinery in order to convert the dimensionality reduction problem into a so called Eulerian flow problem, the solution of which is guaranteed to generate a diffeomorphism. Requiring that the mapping and its inverse both be smooth is\\nreminiscent of the GP-LVM algorithm (Lawrence & Candela, 2006), but has the advantage in terms\\nof statistical strength that we need not separately estimate a mapping in each direction. We showed\\nresults of our algorithm, Diffeomap, on a relatively small motion capture data set, a larger vowel\\ndata set, the USPS image data set, and finally the rather high dimensional data set derived from the\\ntext corpus of NIPS papers, with successes in all cases. Since our new approach performs well in\\npractice while being significantly different to all previous approaches to dimensionality reduction, it\\nhas the potential to lead to a significant new direction in the field.\\n\\nReferences\\nBilmes, J., & et.al. (2006). The Vocal Joystick. Proc. IEEE Intl. Conf. on Acoustic, Speech and Signal Processing. Toulouse, France.\\nCox, T., & Cox, M. (1994). Multidimensional scaling. London, UK: Chapman & Hall.\\nDeMers, D., & Cottrell, G. (1993). Non-linear dimensionality reduction. NIPS 5 (pp. 580?587). Morgan\\nKaufmann, San Mateo, CA.\\nDuda, R. O., Hart, P. E., & Stork, D. G. (2000). Pattern classification. New York: Wiley. 2nd Edition.\\nDupuis, P., & Grenander, U. (1998). Variational problems on flows of diffeomorphisms for image matching.\\nQuarterly of Applied Mathematics, LVI, 587?600.\\nHinton, G., & Roweis, S. (2003). Stochastic neighbor embedding. In S. T. S. Becker and K. Obermayer (Eds.),\\nAdvances in neural information processing systems 15, 833?840. Cambridge, MA: MIT Press.\\nHull, J. J. (1994). A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach.\\nIntell., 16, 550?554.\\nJoshi, S. C., & Miller, M. I. (2000). Landmark matching via large deformation diffeomorphisms. IEEE Transactions on Image Processing, 9, 1357?1370.\\nKarac?ali, B., & Davatzikos, C. (2003). Topology preservation and regularity in estimated deformation fields.\\nInformation Processing in Medical Imaging (pp. 426?437).\\nLawrence, N. D. (2004). Gaussian process latent variable models for visualisation of high dimensional data. In\\nS. Thrun, L. Saul and B. Sch?olkopf (Eds.), Nips 16. Cambridge, MA: MIT Press.\\nLawrence, N. D., & Candela, J. Q. (2006). Local distance preservation in the GP-LVM through back constraints.\\nIn International conference on machine learning, 513?520. ACM.\\nSch?olkopf, B., Herbrich, R., & Smola, A. J. (2001). A generalized representer theorem. Proc. of the 14th\\nAnnual Conf. on Computational Learning Theory (pp. 416?426). London, UK: Springer-Verlag.\\nSong, L., Smola, A., Borgwardt, K., & Gretton, A. (2008). Colored maximum variance unfolding. In J. Platt,\\nD. Koller, Y. Singer and S. Roweis (Eds.), Nips 20, 1385?1392. Cambridge, MA: MIT Press.\\nTenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319?2323.\\nvan der Maaten, L. J. P., Postma, E., & van den Herik, H. (2008). Dimensionality reduction: A comparative\\nreview. In T. Ertl (Ed.), Submitted to neurocognition. Elsevier.\\nVenna, J. (2007). Dimensionality reduction for visual exploration of similarity structures. Doctoral dissertation,\\nHelsinki University of Technology.\\n\\n8\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import pickle\n","pickle.dump(cv,open('count_vectorizer.pkl','wb'))\n","pickle.dump(tfidf_transformer,open('tfidf.pkl','wb'))\n","pickle.dump(feature_names,open('feature_names.pkl','wb'))"],"metadata":{"id":"rHuTzdzqMGuU","executionInfo":{"status":"ok","timestamp":1738207990767,"user_tz":-330,"elapsed":422,"user":{"displayName":"Siddhi Shintre","userId":"09311790356752987921"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jgjYtjPlRNAa"},"execution_count":null,"outputs":[]}]}